{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lElF3o5PR6ys"
      },
      "source": [
        "# Your First RAG Application\n",
        "\n",
        "In this notebook, we'll walk you through each of the components that are involved in a simple RAG application.\n",
        "\n",
        "We won't be leveraging any fancy tools, just the OpenAI Python SDK, Numpy, and some classic Python.\n",
        "\n",
        "> NOTE: This was done with Python 3.11.4.\n",
        "\n",
        "> NOTE: There might be [compatibility issues](https://github.com/wandb/wandb/issues/7683) if you're on NVIDIA driver >552.44 As an interim solution - you can rollback your drivers to the 552.44."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CtcL8P8R6yt"
      },
      "source": [
        "## Table of Contents:\n",
        "\n",
        "- Task 1: Imports and Utilities\n",
        "- Task 2: Documents\n",
        "- Task 3: Embeddings and Vectors\n",
        "- Task 4: Prompts\n",
        "- Task 5: Retrieval Augmented Generation\n",
        "  - ðŸš§ Activity #1: Augment RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Dz6GYilR6yt"
      },
      "source": [
        "Let's look at a rather complicated looking visual representation of a basic RAG application.\n",
        "\n",
        "<img src=\"https://i.imgur.com/vD8b016.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjmC0KFtR6yt"
      },
      "source": [
        "## Task 1: Imports and Utility\n",
        "\n",
        "We're just doing some imports and enabling `async` to work within the Jupyter environment here, nothing too crazy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7VEzqziR6yt",
        "outputId": "f873dd3b-55a0-4e00-ecf4-e2a0fe3af327"
      },
      "outputs": [],
      "source": [
        "!pip install -qU numpy matplotlib plotly pandas scipy scikit-learn openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "Z1dyrG4hR6yt"
      },
      "outputs": [],
      "source": [
        "from aimakerspace.text_utils import TextFileLoader, CharacterTextSplitter\n",
        "from aimakerspace.vectordatabase import VectorDatabase\n",
        "import asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "9OrFZRnER6yt"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0jGnpQsR6yu"
      },
      "source": [
        "## Task 2: Documents\n",
        "\n",
        "We'll be concerning ourselves with this part of the flow in the following section:\n",
        "\n",
        "<img src=\"https://i.imgur.com/jTm9gjk.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SFPWvRUR6yu"
      },
      "source": [
        "### Loading Source Documents\n",
        "\n",
        "So, first things first, we need some documents to work with.\n",
        "\n",
        "While we could work directly with the `.txt` files (or whatever file-types you wanted to extend this to) we can instead do some batch processing of those documents at the beginning in order to store them in a more machine compatible format.\n",
        "\n",
        "In this case, we're going to parse our text file into a single document in memory.\n",
        "\n",
        "Let's look at the relevant bits of the `TextFileLoader` class:\n",
        "\n",
        "```python\n",
        "def load_file(self):\n",
        "        with open(self.path, \"r\", encoding=self.encoding) as f:\n",
        "            self.documents.append(f.read())\n",
        "```\n",
        "\n",
        "We're simply loading the document using the built in `open` method, and storing that output in our `self.documents` list.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia2sUEuGR6yu",
        "outputId": "84937ecc-c35f-4c4a-a4ab-9da72625954c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_loader = TextFileLoader(\"data/PMarcaBlogs.txt\")\n",
        "documents = text_loader.load_documents()\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV-tj5WFR6yu",
        "outputId": "674eb315-1ff3-4597-bcf5-38ece0a812ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The Pmarca Blog Archives\n",
            "(select posts from 2007-2009)\n",
            "Marc Andreessen\n",
            "copyright: Andreessen Horow\n"
          ]
        }
      ],
      "source": [
        "print(documents[0][:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHlTvCzYR6yu"
      },
      "source": [
        "### Splitting Text Into Chunks\n",
        "\n",
        "As we can see, there is one massive document.\n",
        "\n",
        "We'll want to chunk the document into smaller parts so it's easier to pass the most relevant snippets to the LLM.\n",
        "\n",
        "There is no fixed way to split/chunk documents - and you'll need to rely on some intuition as well as knowing your data *very* well in order to build the most robust system.\n",
        "\n",
        "For this toy example, we'll just split blindly on length.\n",
        "\n",
        ">There's an opportunity to clear up some terminology here, for this course we will be stick to the following:\n",
        ">\n",
        ">- \"source documents\" : The `.txt`, `.pdf`, `.html`, ..., files that make up the files and information we start with in its raw format\n",
        ">- \"document(s)\" : single (or more) text object(s)\n",
        ">- \"corpus\" : the combination of all of our documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G6Voc0jR6yv"
      },
      "source": [
        "As you can imagine (though it's not specifically true in this toy example) the idea of splitting documents is to break them into managable sized chunks that retain the most relevant local context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMC4tsEmR6yv",
        "outputId": "08689c0b-57cd-4040-942a-8193e997f5cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "373"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_splitter = CharacterTextSplitter()\n",
        "split_documents = text_splitter.split_texts(documents)\n",
        "len(split_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2wKT0WLR6yv"
      },
      "source": [
        "Let's take a look at some of the documents we've managed to split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcYMwWJoR6yv",
        "outputId": "20d69876-feca-4826-b4be-32915276987a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\\ufeff\\nThe Pmarca Blog Archives\\n(select posts from 2007-2009)\\nMarc Andreessen\\ncopyright: Andreessen Horowitz\\ncover design: Jessica Hagy\\nproduced using: Pressbooks\\nContents\\nTHE PMARCA GUIDE TO STARTUPS\\nPart 1: Why not to do a startup 2\\nPart 2: When the VCs say \"no\" 10\\nPart 3: \"But I don\\'t know any VCs!\" 18\\nPart 4: The only thing that matters 25\\nPart 5: The Moby Dick theory of big companies 33\\nPart 6: How much funding is too little? Too much? 41\\nPart 7: Why a startup\\'s initial business plan doesn\\'t\\nmatter that much\\n49\\nTHE PMARCA GUIDE TO HIRING\\nPart 8: Hiring, managing, promoting, and Dring\\nexecutives\\n54\\nPart 9: How to hire a professional CEO 68\\nHow to hire the best people you\\'ve ever worked\\nwith\\n69\\nTHE PMARCA GUIDE TO BIG COMPANIES\\nPart 1: Turnaround! 82\\nPart 2: Retaining great people 86\\nTHE PMARCA GUIDE TO CAREER, PRODUCTIVITY,\\nAND SOME OTHER THINGS\\nIntroduction 97\\nPart 1: Opportunity 99\\nPart 2: Skills and education 107\\nPart 3: Where to go and why 120\\nThe Pmarca Guide to Personal Productivi']"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_documents[0:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOU-RFP_R6yv"
      },
      "source": [
        "## Task 3: Embeddings and Vectors\n",
        "\n",
        "Next, we have to convert our corpus into a \"machine readable\" format as we explored in the Embedding Primer notebook.\n",
        "\n",
        "Today, we're going to talk about the actual process of creating, and then storing, these embeddings, and how we can leverage that to intelligently add context to our queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OpenAI API Key\n",
        "\n",
        "In order to access OpenAI's APIs, we'll need to provide our OpenAI API Key!\n",
        "\n",
        "You can work through the folder \"OpenAI API Key Setup\" for more information on this process if you don't already have an API Key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "openai.api_key = getpass(\"OpenAI API Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vector Database\n",
        "\n",
        "Let's set up our vector database to hold all our documents and their embeddings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDQrfAR1R6yv"
      },
      "source": [
        "While this is all baked into 1 call - we can look at some of the code that powers this process to get a better understanding:\n",
        "\n",
        "Let's look at our `VectorDatabase().__init__()`:\n",
        "\n",
        "```python\n",
        "def __init__(self, embedding_model: EmbeddingModel = None):\n",
        "        self.vectors = defaultdict(np.array)\n",
        "        self.embedding_model = embedding_model or EmbeddingModel()\n",
        "```\n",
        "\n",
        "As you can see - our vectors are merely stored as a dictionary of `np.array` objects.\n",
        "\n",
        "Secondly, our `VectorDatabase()` has a default `EmbeddingModel()` which is a wrapper for OpenAI's `text-embedding-3-small` model.\n",
        "\n",
        "> **Quick Info About `text-embedding-3-small`**:\n",
        "> - It has a context window of **8191** tokens\n",
        "> - It returns vectors with dimension **1536**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L273pRdeR6yv"
      },
      "source": [
        "#### â“Question #1:\n",
        "\n",
        "The default embedding dimension of `text-embedding-3-small` is 1536, as noted above. \n",
        "\n",
        "1. Is there any way to modify this dimension?\n",
        "\n",
        "Yes, you can modify the embedding dimension by specifying a different dimension size using the dimensions parameter in the API call.\n",
        "\n",
        "2. What technique does OpenAI use to achieve this?\n",
        "\n",
        "OpenAI uses a technique called Matryoshka Representation Learning (MRL) to achieve flexible dimensionality in embeddings.\n",
        "\n",
        "> NOTE: Check out this [API documentation](https://platform.openai.com/docs/api-reference/embeddings/create) for the answer to question #1, and [this documentation](https://platform.openai.com/docs/guides/embeddings/use-cases) for an answer to question #2!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5FZY7K3R6yv"
      },
      "source": [
        "We can call the `async_get_embeddings` method of our `EmbeddingModel()` on a list of `str` and receive a list of `float` back!\n",
        "\n",
        "```python\n",
        "async def async_get_embeddings(self, list_of_text: List[str]) -> List[List[float]]:\n",
        "        return await aget_embeddings(\n",
        "            list_of_text=list_of_text, engine=self.embeddings_model_name\n",
        "        )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSct6X0aR6yv"
      },
      "source": [
        "We cast those to `np.array` when we build our `VectorDatabase()`:\n",
        "\n",
        "```python\n",
        "async def abuild_from_list(self, list_of_text: List[str]) -> \"VectorDatabase\":\n",
        "        embeddings = await self.embedding_model.async_get_embeddings(list_of_text)\n",
        "        for text, embedding in zip(list_of_text, embeddings):\n",
        "            self.insert(text, np.array(embedding))\n",
        "        return self\n",
        "```\n",
        "\n",
        "And that's all we need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "O4KoLbVDR6yv"
      },
      "outputs": [],
      "source": [
        "vector_db = VectorDatabase()\n",
        "vector_db = asyncio.run(vector_db.abuild_from_list(split_documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSZwaGvpR6yv"
      },
      "source": [
        "#### â“Question #2:\n",
        "\n",
        "What are the benefits of using an `async` approach to collecting our embeddings?\n",
        "\n",
        "> NOTE: Determining the core difference between `async` and `sync` will be useful! If you get stuck - ask ChatGPT!\n",
        "\n",
        "Using an async approach to collect embeddings can be really beneficial, especially when youâ€™re dealing with tasks that involve waiting for something to happen, like fetching data from an API. In a synchronous setup, your program would wait until the embeddings for one text are completely fetched before moving on to the next. This can slow things down and leave your system doing nothing but waiting during that time.\n",
        "\n",
        "With async, on the other hand, your program can request embeddings for multiple texts all at once and move on to other tasks while waiting for those requests to be fulfilled. This means youâ€™re not stuck waiting around, and everything runs much more efficiently, especially when you have a lot of data to process.\n",
        "\n",
        "Async operations also help make better use of your computerâ€™s resources. Instead of having your CPU sit idle while waiting for data, async lets it keep working on other things, which makes the whole process faster and more efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRBdIt-xR6yw"
      },
      "source": [
        "So, to review what we've done so far in natural language:\n",
        "\n",
        "1. We load source documents\n",
        "2. We split those source documents into smaller chunks (documents)\n",
        "3. We send each of those documents to the `text-embedding-3-small` OpenAI API endpoint\n",
        "4. We store each of the text representations with the vector representations as keys/values in a dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-vWANZyR6yw"
      },
      "source": [
        "### Semantic Similarity\n",
        "\n",
        "The next step is to be able to query our `VectorDatabase()` with a `str` and have it return to us vectors and text that is most relevant from our corpus.\n",
        "\n",
        "We're going to use the following process to achieve this in our toy example:\n",
        "\n",
        "1. We need to embed our query with the same `EmbeddingModel()` as we used to construct our `VectorDatabase()`\n",
        "2. We loop through every vector in our `VectorDatabase()` and use a distance measure to compare how related they are\n",
        "3. We return a list of the top `k` closest vectors, with their text representations\n",
        "\n",
        "There's some very heavy optimization that can be done at each of these steps - but let's just focus on the basic pattern in this notebook.\n",
        "\n",
        "> We are using [cosine similarity](https://www.engati.com/glossary/cosine-similarity) as a distance metric in this example - but there are many many distance metrics you could use - like [these](https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55)\n",
        "\n",
        "> We are using a rather inefficient way of calculating relative distance between the query vector and all other vectors - there are more advanced approaches that are much more efficient, like [ANN](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76d96uavR6yw",
        "outputId": "bbfccc31-20a2-41c7-c14d-46554a43ed2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('ordingly.\\nSeventh, when hiring the executive to run your former specialty, be\\ncareful you donâ€™t hire someone weak on purpose.\\nThis sounds silly, but you wouldnâ€™t believe how oaen it happens.\\nThe CEO who used to be a product manager who has a weak\\nproduct management executive. The CEO who used to be in\\nsales who has a weak sales executive. The CEO who used to be\\nin marketing who has a weak marketing executive.\\nI call this the â€œMichael Eisner Memorial Weak Executive Problemâ€ â€” aaer the CEO of Disney who had previously been a brilliant TV network executive. When he bought ABC at Disney, it\\npromptly fell to fourth place. His response? â€œIf I had an extra\\ntwo days a week, I could turn around ABC myself.â€ Well, guess\\nwhat, he didnâ€™t have an extra two days a week.\\nA CEO â€” or a startup founder â€” oaen has a hard time letting\\ngo of the function that brought him to the party. The result: you\\nhire someone weak into the executive role for that function so\\nthat you can continue to be â€œthe manâ€ â€” cons',\n",
              "  np.float64(0.6539043027545371)),\n",
              " ('m. They have areas where they are truly deXcient in judgment or skill set. Thatâ€™s just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus on strength rather than lack of weakness. Everybody has severe weaknesses even if you canâ€™t see\\nthem yet. When managing, itâ€™s oaen useful to micromanage and\\nto provide remedial training around these weaknesses. Doing so\\nmay make the diWerence between an executive succeeding or\\nfailing.\\nFor example, you might have a brilliant engineering executive\\nwho generates excellent team loyalty, has terriXc product judgment and makes the trains run on time. This same executive\\nmay be very poor at relating to the other functions in the company. She may generate far more than her share of cross-functional conYicts, cut herself oW from critical information, and\\nsigniXcantly impede your ability to sell and market eWectively.\\nYour alternatives are:\\n(a) Macro-manage and give her an annual or quarterly object',\n",
              "  np.float64(0.5036247837648782)),\n",
              " ('ed?\\nIn reality â€” as opposed to Marcâ€™s warped view of reality â€” it will\\nbe extremely helpful for Marc [if he were actually the CEO,\\nwhich he is not] to meet with the new head of engineering daily\\nwhen she comes on board and review all of her thinking and\\ndecisions. This level of micromanagement will accelerate her\\ntraining and improve her long-term eWectiveness. It will make\\nher seem smarter to the rest of the organization which will build\\ncredibility and conXdence while she comes up to speed. Micromanaging new executives is generally a good idea for a limited\\nperiod of time.\\nHowever, that is not the only time that it makes sense to micro66 The Pmarca Blog Archives\\nmanage executives. It turns out that just about every executive\\nin the world has a few things that are seriously wrong with\\nthem. They have areas where they are truly deXcient in judgment or skill set. Thatâ€™s just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus o',\n",
              "  np.float64(0.4814861061791066))]"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_db.search_by_text(\"What is the Michael Eisner Memorial Weak Executive Problem?\", k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TehsfIiKR6yw"
      },
      "source": [
        "## Task 4: Prompts\n",
        "\n",
        "In the following section, we'll be looking at the role of prompts - and how they help us to guide our application in the right direction.\n",
        "\n",
        "In this notebook, we're going to rely on the idea of \"zero-shot in-context learning\".\n",
        "\n",
        "This is a lot of words to say: \"We will ask it to perform our desired task in the prompt, and provide no examples.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXpA0UveR6yw"
      },
      "source": [
        "### XYZRolePrompt\n",
        "\n",
        "Before we do that, let's stop and think a bit about how OpenAI's chat models work.\n",
        "\n",
        "We know they have roles - as is indicated in the following API [documentation](https://platform.openai.com/docs/api-reference/chat/create#chat/create-messages)\n",
        "\n",
        "There are three roles, and they function as follows (taken directly from [OpenAI](https://platform.openai.com/docs/guides/gpt/chat-completions-api)):\n",
        "\n",
        "- `{\"role\" : \"system\"}` : The system message helps set the behavior of the assistant. For example, you can modify the personality of the assistant or provide specific instructions about how it should behave throughout the conversation. However note that the system message is optional and the modelâ€™s behavior without a system message is likely to be similar to using a generic message such as \"You are a helpful assistant.\"\n",
        "- `{\"role\" : \"user\"}` : The user messages provide requests or comments for the assistant to respond to.\n",
        "- `{\"role\" : \"assistant\"}` : Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior.\n",
        "\n",
        "The main idea is this:\n",
        "\n",
        "1. You start with a system message that outlines how the LLM should respond, what kind of behaviours you can expect from it, and more\n",
        "2. Then, you can provide a few examples in the form of \"assistant\"/\"user\" pairs\n",
        "3. Then, you prompt the model with the true \"user\" message.\n",
        "\n",
        "In this example, we'll be forgoing the 2nd step for simplicities sake."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdZ2KWKSR6yw"
      },
      "source": [
        "#### Utility Functions\n",
        "\n",
        "You'll notice that we're using some utility functions from the `aimakerspace` module - let's take a peek at these and see what they're doing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFbeJDDsR6yw"
      },
      "source": [
        "##### XYZRolePrompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mojJSE3R6yw"
      },
      "source": [
        "Here we have our `system`, `user`, and `assistant` role prompts.\n",
        "\n",
        "Let's take a peek at what they look like:\n",
        "\n",
        "```python\n",
        "class BasePrompt:\n",
        "    def __init__(self, prompt):\n",
        "        \"\"\"\n",
        "        Initializes the BasePrompt object with a prompt template.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        \"\"\"\n",
        "        self.prompt = prompt\n",
        "        self._pattern = re.compile(r\"\\{([^}]+)\\}\")\n",
        "\n",
        "    def format_prompt(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Formats the prompt string using the keyword arguments provided.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: The formatted prompt string\n",
        "        \"\"\"\n",
        "        matches = self._pattern.findall(self.prompt)\n",
        "        return self.prompt.format(**{match: kwargs.get(match, \"\") for match in matches})\n",
        "\n",
        "    def get_input_variables(self):\n",
        "        \"\"\"\n",
        "        Gets the list of input variable names from the prompt string.\n",
        "\n",
        "        :return: List of input variable names\n",
        "        \"\"\"\n",
        "        return self._pattern.findall(self.prompt)\n",
        "```\n",
        "\n",
        "Then we have our `RolePrompt` which laser focuses us on the role pattern found in most API endpoints for LLMs.\n",
        "\n",
        "```python\n",
        "class RolePrompt(BasePrompt):\n",
        "    def __init__(self, prompt, role: str):\n",
        "        \"\"\"\n",
        "        Initializes the RolePrompt object with a prompt template and a role.\n",
        "\n",
        "        :param prompt: A string that can contain placeholders within curly braces\n",
        "        :param role: The role for the message ('system', 'user', or 'assistant')\n",
        "        \"\"\"\n",
        "        super().__init__(prompt)\n",
        "        self.role = role\n",
        "\n",
        "    def create_message(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Creates a message dictionary with a role and a formatted message.\n",
        "\n",
        "        :param kwargs: The values to substitute into the prompt string\n",
        "        :return: Dictionary containing the role and the formatted message\n",
        "        \"\"\"\n",
        "        return {\"role\": self.role, \"content\": self.format_prompt(**kwargs)}\n",
        "```\n",
        "\n",
        "We'll look at how the `SystemRolePrompt` is constructed to get a better idea of how that extension works:\n",
        "\n",
        "```python\n",
        "class SystemRolePrompt(RolePrompt):\n",
        "    def __init__(self, prompt: str):\n",
        "        super().__init__(prompt, \"system\")\n",
        "```\n",
        "\n",
        "That pattern is repeated for our `UserRolePrompt` and our `AssistantRolePrompt` as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D361R6sMR6yw"
      },
      "source": [
        "##### ChatOpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJVQ2Pm8R6yw"
      },
      "source": [
        "Next we have our model, which is converted to a format analagous to libraries like LangChain and LlamaIndex.\n",
        "\n",
        "Let's take a peek at how that is constructed:\n",
        "\n",
        "```python\n",
        "class ChatOpenAI:\n",
        "    def __init__(self, model_name: str = \"gpt-4o-mini\"):\n",
        "        self.model_name = model_name\n",
        "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if self.openai_api_key is None:\n",
        "            raise ValueError(\"OPENAI_API_KEY is not set\")\n",
        "\n",
        "    def run(self, messages, text_only: bool = True):\n",
        "        if not isinstance(messages, list):\n",
        "            raise ValueError(\"messages must be a list\")\n",
        "\n",
        "        openai.api_key = self.openai_api_key\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=self.model_name, messages=messages\n",
        "        )\n",
        "\n",
        "        if text_only:\n",
        "            return response.choices[0].message.content\n",
        "\n",
        "        return response\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCU7FfhIR6yw"
      },
      "source": [
        "#### â“ Question #3:\n",
        "\n",
        "When calling the OpenAI API - are there any ways we can achieve more reproducible outputs?\n",
        "\n",
        "> NOTE: Check out [this section](https://platform.openai.com/docs/guides/text-generation/) of the OpenAI documentation for the answer!\n",
        "\n",
        "Yes, you can achieve more reproducible outputs when calling the OpenAI API by setting the temperature and top_p parameters to lower values. These parameters control the randomness of the output. Setting them lower makes the model more deterministic, which means it will produce more consistent and predictable responses. Additionally, using the same seed in a nucleus sampling strategy can help ensure that the results are more reproducible across different runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5wcjMLCR6yw"
      },
      "source": [
        "### Creating and Prompting OpenAI's `gpt-4o-mini`!\n",
        "\n",
        "Let's tie all these together and use it to prompt `gpt-4o-mini`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "WIfpIot7R6yw"
      },
      "outputs": [],
      "source": [
        "from aimakerspace.openai_utils.prompts import (\n",
        "    UserRolePrompt,\n",
        "    SystemRolePrompt,\n",
        "    AssistantRolePrompt,\n",
        ")\n",
        "\n",
        "from aimakerspace.openai_utils.chatmodel import ChatOpenAI\n",
        "\n",
        "chat_openai = ChatOpenAI()\n",
        "user_prompt_template = \"{content}\"\n",
        "user_role_prompt = UserRolePrompt(user_prompt_template)\n",
        "system_prompt_template = (\n",
        "    \"You are an expert in {expertise}, you always answer in a kind way.\"\n",
        ")\n",
        "system_role_prompt = SystemRolePrompt(system_prompt_template)\n",
        "\n",
        "messages = [\n",
        "    system_role_prompt.create_message(expertise=\"Python\"),\n",
        "    user_role_prompt.create_message(\n",
        "        content=\"What is the best way to write a loop?\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = chat_openai.run(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHo7lssNR6yw",
        "outputId": "1d3823fa-bb6b-45f6-ddba-b41686388324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The best way to write a loop in Python largely depends on the specific task you are trying to accomplish. However, I'll outline some common practices that can help you write clear and efficient loops.\n",
            "\n",
            "### 1. Using a `for` loop\n",
            "\n",
            "If you are iterating over a sequence (like a list, tuple, string, or range), a `for` loop is often the best choice:\n",
            "\n",
            "```python\n",
            "# Example of iterating over a list\n",
            "my_list = [1, 2, 3, 4, 5]\n",
            "for item in my_list:\n",
            "    print(item)\n",
            "```\n",
            "\n",
            "### 2. Using a `while` loop\n",
            "\n",
            "When you need to repeat an action while a condition is true, a `while` loop is appropriate:\n",
            "\n",
            "```python\n",
            "# Example of a while loop\n",
            "count = 0\n",
            "while count < 5:\n",
            "    print(count)\n",
            "    count += 1\n",
            "```\n",
            "\n",
            "### 3. Use built-in functions and comprehensions (when applicable)\n",
            "\n",
            "Sometimes, you can achieve the same results in a more concise way by using list comprehensions or built-in functions like `map` or `filter`:\n",
            "\n",
            "```python\n",
            "# Using a list comprehension\n",
            "squared_numbers = [x**2 for x in my_list]\n",
            "print(squared_numbers)\n",
            "\n",
            "# Using map\n",
            "squared_numbers_map = list(map(lambda x: x**2, my_list))\n",
            "print(squared_numbers_map)\n",
            "```\n",
            "\n",
            "### 4. Keep it clean and readable\n",
            "\n",
            "- **Meaningful variable names**: Choose names that make the purpose of your variable clear.\n",
            "- **Comments to clarify**: Add comments where necessary to explain complex logic.\n",
            "- **Avoid deep nesting**: If your loop contains multiple levels of nesting, consider breaking out parts into functions.\n",
            "\n",
            "### 5. Use `break` and `continue` judiciously\n",
            "\n",
            "You can control the flow of loops using `break` to exit a loop, or `continue` to skip the current iteration, but use them sparingly for clarity:\n",
            "\n",
            "```python\n",
            "for number in range(10):\n",
            "    if number == 5:\n",
            "        continue  # Skip 5\n",
            "    print(number)\n",
            "```\n",
            "\n",
            "### 6. Optimize for performance\n",
            "\n",
            "If performance is a concern, especially for large datasets, consider using tools like NumPy for numerical operations, which are optimized for performance.\n",
            "\n",
            "### Summary\n",
            "\n",
            "In summary, choose the type of loop that best fits your use case, maintain readability, optimize when necessary, and make sure to use language features effectively. If you have a specific example or scenario in mind, feel free to share, and I can provide more targeted advice!\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2nxxhB2R6yy"
      },
      "source": [
        "## Task 5: Retrieval Augmented Generation\n",
        "\n",
        "Now we can create a RAG prompt - which will help our system behave in a way that makes sense!\n",
        "\n",
        "There is much you could do here, many tweaks and improvements to be made!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "D1hamzGaR6yy"
      },
      "outputs": [],
      "source": [
        "RAG_PROMPT_TEMPLATE = \"\"\" \\\n",
        "Use the provided context to answer the user's query.\n",
        "\n",
        "You may not answer the user's query unless there is specific context in the following text.\n",
        "\n",
        "If you do not know the answer, or cannot answer, please respond with \"I don't know\".\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = SystemRolePrompt(RAG_PROMPT_TEMPLATE)\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"\"\" \\\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "User Query:\n",
        "{user_query}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "user_prompt = UserRolePrompt(USER_PROMPT_TEMPLATE)\n",
        "\n",
        "class RetrievalAugmentedQAPipeline:\n",
        "    def __init__(self, llm: ChatOpenAI(), vector_db_retriever: VectorDatabase) -> None:\n",
        "        self.llm = llm\n",
        "        self.vector_db_retriever = vector_db_retriever\n",
        "\n",
        "    def run_pipeline(self, user_query: str) -> str:\n",
        "        context_list = self.vector_db_retriever.search_by_text(user_query, k=4)\n",
        "\n",
        "        context_prompt = \"\"\n",
        "        for context in context_list:\n",
        "            context_prompt += context[0] + \"\\n\"\n",
        "\n",
        "        formatted_system_prompt = rag_prompt.create_message()\n",
        "\n",
        "        formatted_user_prompt = user_prompt.create_message(user_query=user_query, context=context_prompt)\n",
        "\n",
        "        return {\"response\" : self.llm.run([formatted_system_prompt, formatted_user_prompt]), \"context\" : context_list}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZIJI19uR6yz"
      },
      "source": [
        "#### â“ Question #4:\n",
        "\n",
        "What prompting strategies could you use to make the LLM have a more thoughtful, detailed response?\n",
        "\n",
        "What is that strategy called?\n",
        "\n",
        "> NOTE: You can look through the Week 1 Day 1 \"Prompting OpenAI Like A Developer\" material for an answer to this question!\n",
        "\n",
        "To get the LLM to provide more thoughtful and detailed responses, you can use a few different prompting strategies. One effective approach is few-shot prompting. This involves giving the model a few examples of well-crafted, detailed responses within the prompt. By doing this, you're setting a clear standard for the model to follow, which can lead to more consistent and thorough answers.\n",
        "\n",
        "Another useful technique is chain-of-thought prompting. This strategy encourages the model to think through its reasoning step by step before arriving at a final answer. By breaking down its thought process, the model often produces more detailed and accurate responses, as it has to carefully consider each part of the problem before giving an answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "kqbE9fZ6R6yz"
      },
      "outputs": [],
      "source": [
        "retrieval_augmented_qa_pipeline = RetrievalAugmentedQAPipeline(\n",
        "    vector_db_retriever=vector_db,\n",
        "    llm=chat_openai\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAGhaCGOR6yz",
        "outputId": "e4fb3a1b-d2bc-4e18-ec31-dc0adf767163"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'response': \"The 'Michael Eisner Memorial Weak Executive Problem' refers to the tendency of a CEO or startup founder, who has a strong background in a specific function like product management, sales, or marketing, to hire a weak executive for that same function. This hiring choice is often made subconsciously because the CEO wants to maintain control or prominence in that area, allowing them to appear competent while delegating. The term is named after Michael Eisner, the former CEO of Disney, who faced difficulties when he acquired ABC, a situation that illustrates the complications that can arise from such hiring decisions.\",\n",
              " 'context': [('ordingly.\\nSeventh, when hiring the executive to run your former specialty, be\\ncareful you donâ€™t hire someone weak on purpose.\\nThis sounds silly, but you wouldnâ€™t believe how oaen it happens.\\nThe CEO who used to be a product manager who has a weak\\nproduct management executive. The CEO who used to be in\\nsales who has a weak sales executive. The CEO who used to be\\nin marketing who has a weak marketing executive.\\nI call this the â€œMichael Eisner Memorial Weak Executive Problemâ€ â€” aaer the CEO of Disney who had previously been a brilliant TV network executive. When he bought ABC at Disney, it\\npromptly fell to fourth place. His response? â€œIf I had an extra\\ntwo days a week, I could turn around ABC myself.â€ Well, guess\\nwhat, he didnâ€™t have an extra two days a week.\\nA CEO â€” or a startup founder â€” oaen has a hard time letting\\ngo of the function that brought him to the party. The result: you\\nhire someone weak into the executive role for that function so\\nthat you can continue to be â€œthe manâ€ â€” cons',\n",
              "   np.float64(0.6582163021048181)),\n",
              "  ('m. They have areas where they are truly deXcient in judgment or skill set. Thatâ€™s just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus on strength rather than lack of weakness. Everybody has severe weaknesses even if you canâ€™t see\\nthem yet. When managing, itâ€™s oaen useful to micromanage and\\nto provide remedial training around these weaknesses. Doing so\\nmay make the diWerence between an executive succeeding or\\nfailing.\\nFor example, you might have a brilliant engineering executive\\nwho generates excellent team loyalty, has terriXc product judgment and makes the trains run on time. This same executive\\nmay be very poor at relating to the other functions in the company. She may generate far more than her share of cross-functional conYicts, cut herself oW from critical information, and\\nsigniXcantly impede your ability to sell and market eWectively.\\nYour alternatives are:\\n(a) Macro-manage and give her an annual or quarterly object',\n",
              "   np.float64(0.508838340435339)),\n",
              "  ('ed?\\nIn reality â€” as opposed to Marcâ€™s warped view of reality â€” it will\\nbe extremely helpful for Marc [if he were actually the CEO,\\nwhich he is not] to meet with the new head of engineering daily\\nwhen she comes on board and review all of her thinking and\\ndecisions. This level of micromanagement will accelerate her\\ntraining and improve her long-term eWectiveness. It will make\\nher seem smarter to the rest of the organization which will build\\ncredibility and conXdence while she comes up to speed. Micromanaging new executives is generally a good idea for a limited\\nperiod of time.\\nHowever, that is not the only time that it makes sense to micro66 The Pmarca Blog Archives\\nmanage executives. It turns out that just about every executive\\nin the world has a few things that are seriously wrong with\\nthem. They have areas where they are truly deXcient in judgment or skill set. Thatâ€™s just life. Almost nobody is brilliant\\nat everything. When hiring and when Hring executives, you\\nmust therefore focus o',\n",
              "   np.float64(0.47901914637916565)),\n",
              "  ('nYicts, cut herself oW from critical information, and\\nsigniXcantly impede your ability to sell and market eWectively.\\nYour alternatives are:\\n(a) Macro-manage and give her an annual or quarterly objective\\nto Xx it, orâ€¦\\n(b) Intensively micromanage her interactions until she learns\\nthe fundamental interpersonal skills required to be an eWective\\nexecutive.\\nI am arguing that doing (a) will likely result in weak performance. The reason is that she very likely has no idea how to be\\neWective with her peers. If somebody is an executive, itâ€™s very\\nlikely that somewhere along the line somebody gave her feedback â€” perhaps abstractly â€” about all of her weaknesses. Yet\\nthe weakness remains. As a result, executives generally require\\nmore hands-on management than lower level employees to\\nimprove weak areas.\\nSo, micromanagement is like Xne wine. A little at the right times\\nwill really enhance things; too much all the time and youâ€™ll end\\nup in rehab.\\nPart 8: Hiring, managing, promoting, and Dring execut',\n",
              "   np.float64(0.4681203037688283))]}"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_pipeline.run_pipeline(\"What is the 'Michael Eisner Memorial Weak Executive Problem'?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ—ï¸ Activity #1:\n",
        "\n",
        "Enhance your RAG application in some way! \n",
        "\n",
        "Suggestions are: \n",
        "\n",
        "- Allow it to work with PDF files\n",
        "- Implement a new distance metric\n",
        "- Add metadata support to the vector database\n",
        "\n",
        "While these are suggestions, you should feel free to make whatever augmentations you desire! \n",
        "\n",
        "> NOTE: These additions might require you to work within the `aimakerspace` library - that's expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.3.1\n",
            "Requirement already satisfied: langchain-community in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (0.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-community) (0.5.14)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.13 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-community) (0.2.14)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.30 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-community) (0.2.34)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-community) (0.1.101)\n",
            "Requirement already satisfied: numpy<2,>=1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.13->langchain-community) (2.7.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.30->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (0.24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (0.17.3)\n",
            "Requirement already satisfied: sniffio in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.30->langchain-community) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.13->langchain-community) (2.18.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community) (3.7.1)\n"
          ]
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### FIRST TRY USING PyMuPDF\n",
        "### Installing PyMuPDF library\n",
        "# !pip install PyMuPDF\n",
        "# import pymupdf\n",
        "\n",
        "### SECOND TRY USING LANGCHAIN PyPDFLoader\n",
        "!pip install pypdf\n",
        "!pip install -U langchain-community\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ReFT: Representation Finetuning\n",
            "for Language Models\n",
            "Zhengxuan Wuâˆ—â€ Aryaman Aroraâˆ—â€ Zheng Wangâ€ Atticus \n",
            "{'source': 'data/reft-paper.pdf', 'page': 0}\n"
          ]
        }
      ],
      "source": [
        "### Uploading PDF\n",
        "# documents = pymupdf.open(\"data/reft-paper.pdf\")\n",
        "# len(documents)\n",
        "\n",
        "# for page in documents: # iterate the document pages\n",
        "#   text = page.get_text() # get plain text encoded as UTF-8\n",
        "\n",
        "# print(text)\n",
        "\n",
        "loader = PyPDFLoader(\n",
        "    file_path = \"data/reft-paper.pdf\",\n",
        "    extract_images = False,\n",
        "    headers = None,\n",
        "    extraction_mode = \"plain\",\n",
        ")\n",
        "\n",
        "documents = loader.load()\n",
        "print(documents[0].page_content[:100])\n",
        "print(documents[0].metadata)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(metadata={'source': 'data/reft-paper.pdf', 'page': 0}, page_content='ReFT: Representation Finetuning\\nfor Language Models\\nZhengxuan Wuâˆ—â€ Aryaman Aroraâˆ—â€ Zheng Wangâ€ Atticus Geigerâ€¡\\nDan Jurafskyâ€ Christopher D. Manningâ€ Christopher Pottsâ€ \\nâ€ Stanford Universityâ€¡Pr(Ai)2R Group\\n{wuzhengx,aryamana,peterwz,atticusg}@stanford.edu\\n{jurafsky,manning,cgpotts}@stanford.edu\\nAbstract\\nParameter-efficient finetuning (PEFT) methods seek to adapt large neural models\\nvia updates to a small number of weights . However, much prior interpretability\\nwork has shown that representations encode rich semantic information, suggesting\\nthat editing representations might be a more powerful alternative. We pursue\\nthis hypothesis by developing a family of Representation Finetuning (ReFT)\\nmethods. ReFT methods operate on a frozen base model and learn task-specific\\ninterventions on hidden representations. We define a strong instance of the ReFT\\nfamily, Low-rank Linear Subspace ReFT (LoReFT), and we identify an ablation of\\nthis method that trades some performance for increased efficiency. Both are drop-in\\nreplacements for existing PEFTs and learn interventions that are 15 Ã—â€“65Ã—more\\nparameter-efficient than LoRA. We showcase LoReFT on eight commonsense rea-\\nsoning tasks, four arithmetic reasoning tasks, instruction-tuning, and GLUE. In all\\nthese evaluations, our ReFTs deliver the best balance of efficiency and performance,\\nand almost always outperform state-of-the-art PEFTs. We release a generic ReFT\\ntraining library publicly at https://github.com/stanfordnlp/pyreft .\\n1 Introduction\\nPretrained language models (LMs) are frequently finetuned to adapt them to new domains or tasks\\n[Dai and Le, 2015]. With finetuning, a single base model can be adapted to a variety of tasks given\\nonly small amounts of in-domain data. However, finetuning large LMs is expensive. Parameter-\\nefficient finetuning (PEFT) methods propose to address the high costs of full finetuning by updating a\\nsmall number of weights. This reduces memory usage and training time, and PEFTs achieve similar\\nperformance to full finetuning in many settings [Hu et al., 2023].\\nA hallmark of current state-of-the-art PEFTs is that they modify weights rather than representations .\\nHowever, much prior interpretability work has shown that representations encode rich semantic\\ninformation, suggesting that editing representations might be a more powerful alternative to weight\\nupdates. In this paper, we pursue this hypothesis by developing and motivating Representation\\nFinetuning (ReFT) . Instead of adapting model weights, ReFT methods train interventions that\\nmanipulate a small fraction of model representations in order to steer model behaviors to solve\\ndownstream tasks at inference time. ReFT methods are drop-in replacements for weight-based PEFTs.\\nThis approach is inspired by recent work in LM interpretability that intervenes on representations to\\nfind faithful causal mechanisms [Geiger et al., 2023b] and to steer model behaviours at inference time\\n[Turner et al., 2023, Li et al., 2024], and it can be seen as a generalisation of the representation-editing\\nwork of Wu et al. [2024a], Turner et al. [2023], and Zou et al. [2023] (see appendix B for details).\\n*Equal contribution.\\nPreprint. Under review.arXiv:2404.03592v3  [cs.CL]  22 May 2024'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 1}, page_content='Commonsense\\nLLaMA 7B\\n LLaMA 13B\\n Llama- 2 7B\\n Llama- 3 8B\\nInstruct -tuning\\nLlama- 2 7B\\nParamet ersP er f ormance\\nArit hmetic\\nLLaMA 7B\\n LLaMA 13B\\nGLUE\\nR oBERT a-base\\n R oBERT a-lar ge\\nFigure 1: Parameter count vs. performance for LoReFT and other PEFTs across four benchmarks\\nwhen applied to LLaMA, Llama-2, Llama-3, and RoBERTa models. Despite training far fewer\\nparameters than existing PEFTs, LoReFT achieves competitive or even state-of-the-art performance\\non all tasks. Its value is most apparent for the largest models in our evaluations. Note : FT is\\nfull-parameter finetuning, which is not a PEFT or ReFT method. Additional results are in section 4.\\nWe focus on a strong and highly efficient instance of the ReFT family that we call Low-rank\\nLinear Subspace ReFT (LoReFT ). LoReFT is a parametrisation of ReFT that intervenes on hidden\\nrepresentations in the linear subspace spanned by a low-rank projection matrix, building directly\\non the distributed alignment search (DAS) method of Geiger et al. [2023b] and Wu et al. [2023].\\nWe also identify an ablation of this method (DiReFT) that trades some performance for increased\\nefficiency. We evaluate our ReFTs on LLaMA-family models and small-scale LMs against existing\\nPEFTs on standard benchmarks from four domains: commonsense reasoning, arithmetic reasoning,\\ninstruction-following, and natural language understanding. Compared to LoRA, we find that LoReFT\\nuses 15Ã—â€“65Ã—times fewer parameters while achieving state-of-the-art performance on commonsense\\nreasoning, instruction-following, and natural language understanding against the strongest PEFTs.\\nThese findings indicate that ReFT methods are worthy of further exploration, as they may emerge as\\nmore efficient and effective alternatives to weight-based PEFTs.\\n2 Related work\\nParameter-efficient finetuning methods (PEFTs). PEFTs train a fraction of the modelâ€™s parameters\\nto adapt it to downstream tasks. We classify PEFTs into three categories:\\n1.Adapter-based methods train additional modules (e.g. fully-connected layers) on top of the\\nfrozen pretrained model. Series adapters insert components between LM attention or MLP\\nlayers [Houlsby et al., 2019, Pfeiffer et al., 2020, Wang et al., 2022, He et al., 2022b, Fu et al.,\\n2021], while parallel adapters add modules alongside existing components [He et al., 2022a].\\nSince adapters add new components that cannot be easily folded into existing model weights,\\nthey impose an additional burden at inference time.1\\n2.LoRA [Hu et al., 2022] and DoRA [Liu et al., 2024c] use low-rank matrices to approximate\\nadditive weight updates during training, and require no additional overhead during inference\\nsince the weight updates can be merged into the model. These are the strongest PEFTs currently.2\\n3.Prompt-based methods add randomly-initialised soft tokens to the input (usually as a prefix)\\nand train their embeddings while keeping the LM weights frozen [Li and Liang, 2021]. These\\n1Several very recent papers introduce new adapter architectures but do not benchmark them on the tasks\\nwe consider, or they perform hyperparameter-tuning in a different setup than done in this work. These include:\\nLLaMA-Adapter [Zhang et al., 2024b], LLaMA-Adapter v2 [Gao et al., 2023], Aligner [Ziheng et al., 2023].\\n2Additional methods not studied in this work: AutoLoRA [Zhang et al., 2024c], ResLoRA [Shi et al., 2024],\\nSiRA [Zhu et al., 2023].\\n2'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 2}, page_content='methods are often far from optimal compared to other PEFTs, and come at the cost of significant\\ninference overhead. A variant of this method where hidden-layer activations are also tuned was\\nintroduced as a baseline in Hu et al. [2022], with better performance.\\nRepresentation editing. Recent work on activation steering andrepresentation engineering shows\\nthat adding fixed or task-specific steering vectors [Subramani et al., 2022, Turner et al., 2023, Zou\\net al., 2023, Liu et al., 2024b, V ogel, 2024, Li et al., 2024] or applying concept erasure [Ravfogel\\net al., 2022, Belrose et al., 2023, Avitan et al., 2024, Singh et al., 2024] to the residual stream can\\nenable a degree of control over pretrained LM generations without the need for resource-intensive\\nfinetuning [Wu et al., 2024a]. The success of these methods affirms that representations induced by\\npretrained LMs carry rich semantic structure.\\nInterventional interpretability. Much recent work has used interventions on model-internal states\\nto test hypotheses about how LMs implement various behaviours. In particular, interventions on linear\\nsubspaces of representations have provided increasing evidence that human-interpretable concepts are\\nencoded linearly [Smolensky, 1986, Rumelhart et al., 1986, McClelland et al., 1986]. This includes\\nlinguistic features such as gender and number [Lasri et al., 2022, Wang et al., 2023, Hanna et al.,\\n2023, Chintam et al., 2023, Yamakoshi et al., 2023, Hao and Linzen, 2023, Chen et al., 2023, Amini\\net al., 2023, Guerner et al., 2023, Arora et al., 2024], logical and mathematical reasoning [Wu et al.,\\n2023], entity attributes [Huang et al., 2024], and a number of other domains [Mikolov et al., 2013,\\nElhage et al., 2022, Park et al., 2023, Nanda et al., 2023, Guerner et al., 2023].\\n3 ReFT\\nWe now define the ReFT family of methods. To do this, we first summarize the core motivation,\\nwhich emerges from work on intervention-based model interpretability. We then show how this leads\\ndirectly to Low-rank Linear Subspace ReFT (LoReFT). Finally, we generalize this to a family of\\nReFT methods. Appendix A provides a brief overview of our generic ReFT training library.\\nTo keep the presentation simple, we assume throughout that our target model is a Transformer-\\nbased [Vaswani et al., 2017] LM that produces contextualised representations of sequences of tokens.\\nGiven a sequence of ninput tokens x=(x1, . . . , x n), the model first embeds these into a list of\\nrepresentations h(0)=(h(0)\\n1, . . . ,h(0)\\nn). Then, mlayers successively compute the j-th list of hidden\\nrepresentations h(j)as a function of the previous list of hidden representations h(jâˆ’1). Each hidden\\nrepresentation is a vector hâˆˆRd. The LM uses the final hidden representations h(m)to produce its\\npredictions. In our experiments, we consider both autoregressive LMs and masked LMs [Devlin et al.,\\n2019]. An autoregressive LM predicts p(xn+1âˆ£x1, . . . , x n)=softmax (Wh(m)\\nn), while a masked\\nLM predicts p(xiâˆ£x1, . . . , x iâˆ’1, xi+1, . . . , x n)=softmax (Wh(m)\\ni), where Wis a learned matrix\\nmapping from representations to logits over the vocabulary space.\\n3.1 Motivation\\nIn interpretability research, the framework of causal abstraction [Geiger et al., 2021] uses interchange\\ninterventions to establish the causal role of representations in deep learning models. An interchange\\nintervention fixes a representation to the value it would take if a counterfactual input were processed\\nby the model. Experiments investigating how such interventions affect model behavior form the\\nevidence for claims about the causal role of a representation and the concept it encodes.\\nTo test whether a concept is encoded in a linear subspace of a representation, one may use a dis-\\ntributed interchange intervention (DII) [Geiger et al., 2023b].3Letbbe the hidden representation\\ncreated at row iand column kwhen our model processes input b, and let sbe the corresponding'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 2}, page_content='tributed interchange intervention (DII) [Geiger et al., 2023b].3Letbbe the hidden representation\\ncreated at row iand column kwhen our model processes input b, and let sbe the corresponding\\nrepresentation when that same model processes input s. A distributed interchange intervention on b\\ngiven a counterfactual source representation sis then defined as\\nDII(b,s,R)=b+RâŠº(Rsâˆ’Rb) (1)\\nwhere RâˆˆRrÃ—dis a low-rank projection matrix with orthonormal rows, dis the representation\\ndimensionality, and ris the dimensionality of the subspace we are intervening on. We learn the\\nsubspace Rusing distributed alignment search (DAS), which finds the subspace that maximises the\\n3This notion of subspace intervention was also independently discovered by Guerner et al. [2023].\\n3'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 3}, page_content='R eFT Int er v ention\\nt hisissomet e xt\\nLoR eFTedit subspace\\n(r o ws of R)RWhhhÎ¦(h)\\nb-++RTedit r estrict ed \\nt o subspaceedit \\nv ect orFigure 2: Illustration of ReFT .(1)The left panel depicts an intervention I: the intervention function\\nÎ¦is applied to hidden representations at positions Pin layer l.(2)The right panel depicts the\\nintervention function used in LoReFT, which finds an edit vector that only modifies the representation\\nin the linear subspace spanned by the rows of R. Specifically, we show how a rank-2 LoReFT\\noperates on 3-dimensional hidden representations.\\nprobability of the expected counterfactual output after intervention [Geiger et al., 2023b]. DAS is\\nhighly expressive, and can effectively localize concepts within model representations [Wu et al., 2023,\\nArora et al., 2024, Wu et al., 2024c, Huang et al., 2024]. This suggests that subspace representation\\ninterventions could also be a powerful tool for model control.\\n3.2 Two low-rank ReFT instantiations\\nLoReFT. The formulation of DIIin eq. (1) immediately suggests a way to control model generations\\nvia interventions. The guiding intuition is that we can learn how to perform interventions that steer\\nthe model towards predicting our task labels. The resulting method, Low-rank Linear Subspace ReFT\\n(LoReFT), is defined by the following variant of eq. (1):\\nÎ¦LoReFT(h)=h+RâŠº(Wh+bâˆ’Rh) (2)\\nThis is identical to eq. (1), except we use a learned projected source Rs=Wh+b. LoReFT thus\\nedits the representation in the r-dimensional subspace spanned by the rows of Rto take on the\\nvalues obtained from our linear projection Wh+b. We depict this operation in fig. 2. The learned\\nparameters are Ï•={R,W,b}; the parameters of the LM are frozen. As with DII,RâˆˆRrÃ—dis a\\nlow-rank matrix with orthonormal rows where dis the hidden-state dimensionality and râ‰¤dis the\\nrank of the subspace. We further define a linear projection WâˆˆRrÃ—dand bias vector bâˆˆRr.\\nDiReFT. In addition, we define an ablation of LoReFT which removes the orthogonality constraint\\nand the difference operation, reducing training time:\\nÎ¦DiReFT(h)=h+WâŠº\\n2(W1h+b) (3)\\nBothW1,W2âˆˆRrÃ—dare low-rank projection matrices. Note that eq. (3) resembles LoRA, and thus\\nDiReFT can be thought of as LoRA applied directly to hidden representations at certain positions.4\\nEmpirical evidence from previous work suggests that adding orthogonal constraints to LoRA weights\\nincreases performance [Liu et al., 2024d]. (Appendix E reports results for additional ablations of\\nLoReFT.)\\nTraining objective. We consider both generation tasks using decoder-only or encoderâ€“decoder\\nLMs and classification tasks using encoder-only models. The pretrained language model induces a\\ndistribution over token sequences p(â‹…). We denote the model that results from the ReFT intervention\\nÎ¦onp(â‹…)aspÎ¦(â‹…)with trainable parameters Ï•. To simplify notation, we refer to the hidden\\nrepresentations produced by the LM on input xash(x), and those by the intervened LM as hÎ¦(x).\\nFor generation tasks, our training objective is language modelling. Given an input sequence x=\\n(x1, . . . , x n)withntokens as the prompt, the goal is to predict the output sequence y=(y1, . . . , y m)\\n4LoRA is not applicable to the residual stream, which is weightless. LoRA can be configured to apply only\\nto the attention layer output projection matrix, which is similar to our residual stream intervention. However,\\nprevious works found that applying LoRA only to attention layers is sub-optimal [Hu et al., 2023].\\n4'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 4}, page_content='withmtokens. We minimise the cross-entropy loss with teacher-forcing over all output positions.\\nmin\\nÏ•{âˆ’m\\nâˆ‘\\ni=1logpÎ¦(yiâˆ£xy<i)} (4)\\nFor single-label classification tasks, we add a classification head HÎ¸(â‹…)with parameters Î¸that takes\\nthe final-layer representation at the first token ( CLS) as input and outputs a distribution over classes.\\nHhas the learned parameters Î¸={Wo,bo,Wd,bd}.\\nHÎ¸(â‹…âˆ£h)=softmax (Wo(tanh(Wdh(m)\\n1+bd))+bo) (5)\\nWe learn the parameters of the head and those of the intervention function Î¦. We minimise the\\ncross-entropy loss of the target class ygiven input x:\\nmin\\nÏ•,Î¸{âˆ’logHÎ¸(yâˆ£hÎ¦(x))} (6)\\n3.3 The ReFT family of methods\\nIt is straightforward to generalise the above intervention functions to define a family of intervention-\\nbased representation finetuning methods. We first define a general notion of intervention , i.e. the\\nmodification of hidden representations during the model forward pass:\\nDefinition 3.1. Anintervention Iis a tuple âŸ¨Î¦, P, lâŸ©that encapsulates a single inference-time\\nmodification of the representations computed by a Transformer-based LM. The three components of\\nan intervention are (1) the intervention function Î¦âˆ¶Rdâ†’Rdwith learned parameters Ï•, (2) a set\\nofinput positions PâŠ†{1, . . . , n}that the intervention is applied to, and (3) the layer lâˆˆ{1, . . . , m}\\nat which the intervention is applied.\\nWe implement the intervention Ias the following operation that overwrites some representations h:\\nh(l)â†(Î¦(h(l)\\np)ifpâˆˆPelseh(l)\\np)pâˆˆ1,...,n(7)\\nThe intervention is applied immediately after the computation of h(l)and thus affects the representa-\\ntions computed in later layers h(l+1), . . . ,h(m).\\nFigure 2 provides a schematic overview of an intervention. A ReFT is then defined as a constrained\\nset of non-overlapping interventions:\\nDefinition 3.2. AReFT method is a set of finterventions I={I1, . . . , I f}. We enforce that for\\nany two interventions Ij, IkâˆˆIsuch that they operate on the same layer lj=lk, their intervention\\npositions must be disjoint, i.e. Pjâˆ©Pk=âˆ…. The parameters (Ï•1, . . . , Ï• f)of all of the intervention\\nfunctions are independent.\\nReFT is thus a generic framework encompassing interventions on hidden representations during the\\nmodel forward pass. In appendix B, we show how a variety of existing inference-time intervention\\nmethods can be described within this framework.\\n4 Experiments\\nTo evaluate our ReFTs against existing PEFTs, we conduct experiments across four diverse NLP\\nbenchmarks covering more than 20 datasets (extensive details on our datasets are in appendix C).\\nOur goal is to provide a rich picture of how LoReFT and DiReFT perform in different scenarios. We\\nexperiment with both masked and autoregressive LMs at different scales, ranging from RoBERTa-\\nbase [Liu et al., 2019] with 125M to LLaMA models [Touvron et al., 2023a,b] with 13B parameters.\\nWe benchmark against existing PEFTs such as prefix-tuning [Li and Liang, 2021], adapter-tuning\\nwith both Series Adapters and Parallel Adapters, BitFit [Ben Zaken et al., 2022], RED [Wu et al.,\\n2024a], LoRA [Hu et al., 2022], and DoRA [Liu et al., 2024c]. Our comparisons focus on both\\nperformance and parameter efficiency. In our comparisons, we use hyperparameter-tuned scores from\\nprevious works when possible. We load our base LMs in torch.bfloat16 to save memory. All of\\nour experiments are run with a single GPU: NVIDIA A100 40G/80G or RTX 6000 . Examples of\\nraw model generations are in appendix I.\\n5'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 5}, page_content='4.1 Hyperparameter configuration\\nFor our experiments, we must decide how many interventions to learn and which layers and input\\npositions to apply each one on. We propose learning interventions on a fixed number of pprefix and\\nssuffix positions in the prompt. Specifically, we tune four hyperparameters:\\n1. The number of prefix positions pto intervene on, i.e. positions {1, . . . , p}.\\n2. The number of suffix positions sto intervene on, i.e. positions {nâˆ’s+1, . . . , n}.\\n3. Which set of layers Lto intervene on.\\n4. Whether or not to tie intervention parameters Ï•across different positions in the same layer.\\nThis simplifies the hyperparameter search space; compared to LoRA, the only additional consideration\\nis which positions to intervene on. Since the number of positions edited is constant, LoReFT and\\nDiReFT contribute a fixed additional inference cost that does not scale with prompt length.\\nGiven the positions P={1, . . . , p}âˆª{nâˆ’s+1, . . . , n}, we define the untied and tied variants:\\nIuntied={âŸ¨Î¦,{p}, lâŸ© âˆ£pâˆˆP, lâˆˆL} Itied={âŸ¨Î¦, P, lâŸ© âˆ£lâˆˆL}\\nAdditionally, when applying LoReFT and DiReFT to a prompt with length nwhere n<p+s, we\\nsetpâ†min(p,âŒŠn/2âŒ‹)andsâ†min(s,âŒˆn/2âŒ‰)and do not apply the truncated interventions in Iuntied.\\nWe also tune neural-network training hyperparameters.\\nUnlike previous work [Hu et al., 2022, 2023, Liu et al., 2024c] where hyperparameter tuning\\nmay involve optimising performance directly on test sets, we only tune our hyperparameters on\\ndevelopment sets which do not contain any overlapping examples with the test sets of our tasks. We\\nfurther describe hyperparameter tuning for each benchmark in appendix D.1.\\n4.2 Commonsense reasoning\\nWe replicate the experimental setup in Hu et al. [2023] and finetune LLaMA-1 7B/13B,\\nLlama-2 7B, and Llama-3 8B5on a combined dataset of eight commonsense reasoning tasks\\n(COMMONSENSE 170K ). We report scores on each taskâ€™s test set individually. We compare with\\nPEFTs benchmarked in Hu et al. [2023] as well as the identical experiment reported in Liu et al.\\n[2024c] for DoRA.\\nDatasets. Our benchmark contains eight commonsense reasoning datasets, including BoolQ [Clark\\net al., 2019], PIQA [Bisk et al., 2020], SIQA [Sap et al., 2019], HellaSwag [Zellers et al., 2019],\\nWinoGrande [Sakaguchi et al., 2021], ARC-e, ARC-c [Clark et al., 2018], and OBQA [Mihaylov\\net al., 2018]. Examples are formulated as multiple-choice problems where the model needs to directly\\ngenerate the correct choice without rationales. We use the same prompt template as in Hu et al.\\n[2023] with additional string normalisation (removing leading and trailing whitespace).\\nHyperparameter tuning. We do not do hyperparameter selection based on test set results. Rather,\\nwe use the hyperparameter settings of the model that performs best on a development set created\\nfrom the GSM8K training set, except we use a lower number of epochs (6 instead of 12) because\\ntheCOMMONSENSE 170K training set is more than 20 times larger than GSM8K . This allows us to\\ntune relevant hyperparamters, and also serves to test the robustness of these settings across different\\ndomains. We additionally report scores on 3 epochs in appendix D.3.\\nResults. We report results in table 1. LoReFT sets state-of-the-art performance on the commonsense\\nreasoning tasks, outperforming all other methods by a considerable margin. While being more\\ncompute-efficient, DiReFT achieves only slightly worse performance consistently.\\n4.3 Arithmetic reasoning\\nSimilar to the previous experiment, we follow the experimental setup in Hu et al. [2023] and finetune\\nLLaMA-1 7B and 13B on a combined dataset of seven arithmetic reasoning tasks with LM-generated\\nchain-of-thought steps ( MATH10K) and report scores on four of the tasksâ€™ test sets. We only evaluate\\ncorrectness on the final numeric or multiple-choice answer.\\n5Llama-3 8B appeared on April 18, 2024, and thus we had time to complete only commonsense reasoning'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 5}, page_content='correctness on the final numeric or multiple-choice answer.\\n5Llama-3 8B appeared on April 18, 2024, and thus we had time to complete only commonsense reasoning\\nexperiments with this model. Liu et al. [2024c] report corresponding results for LoRA and DoRA.\\n6'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 6}, page_content='Table 1: Accuracy comparison of LLaMA-1 7B/13B, Llama-2 7B and Llama-3 8B against existing\\nPEFT methods on eight commonsense reasoning datasets.âˆ—Performance results of all baseline\\nmethods are taken from Liu et al. [2024c]. We report averaged performance of three runs with distinct\\nrandom seeds for our method. For our methods, Param. (%) is calculated by dividing the number of\\ntrainable parameters by the number of parameters of the base LM.\\nModel PEFT Params (%)Accuracy (â†‘)\\nBoolQ PIQA SIQA HellaS. WinoG. ARC-e ARC-c OBQA Avg.\\nChatGPTâˆ—â€” â€” 73.1 85.4 68.5 78.5 66.1 89.8 79.9 74.8 77.0\\nLLaMA-7BPrefTâˆ—0.039% 64.3 76.8 73.9 42.1 72.1 72.9 54.0 60.6 64.6\\nAdapterSâˆ—1.953% 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8\\nAdapterPâˆ—3.542% 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.3\\nLoRAâˆ—0.826% 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7\\nDoRA (half)âˆ—0.427% 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5\\nDoRAâˆ—0.838% 68.5 82.9 79.6 84.8 80.8 81.4 65.8 81.0 78.1\\nDiReFT (ours) 0.031% 69.5 83.0 79.0 92.5 80.5 82.2 68.0 77.5 79.0\\nLoReFT (ours) 0.031% 69.3 84.4 80.3 93.1 84.2 83.2 68.2 78.9 80.2\\nLLaMA-13BPrefTâˆ—0.031% 65.3 75.4 72.1 55.2 68.6 79.5 62.9 68.0 68.4\\nAdapterSâˆ—1.586% 71.8 83.0 79.2 88.1 82.4 82.5 67.3 81.8 79.5\\nAdapterPâˆ—2.894% 72.5 84.9 79.8 92.1 84.7 84.2 71.2 82.4 81.5\\nLoRAâˆ—0.670% 72.1 83.5 80.5 90.5 83.7 82.8 68.3 82.4 80.5\\nDoRA (half)âˆ—0.347% 72.5 85.3 79.9 90.1 82.9 82.7 69.7 83.6 80.8\\nDoRAâˆ—0.681% 72.4 84.9 81.5 92.4 84.2 84.2 69.6 82.8 81.5\\nDiReFT (ours) 0.025% 71.3 86.1 80.8 94.6 83.6 85.5 72.9 82.7 82.2\\nLoReFT (ours) 0.025% 72.1 86.3 81.8 95.1 87.2 86.2 73.7 84.2 83.3\\nLlama-2 7BLoRAâˆ—0.826% 69.8 79.9 79.5 83.6 82.6 79.8 64.7 81.0 77.6\\nDoRA (half)âˆ—0.427% 72.0 83.1 79.9 89.1 83.0 84.5 71.0 81.2 80.5\\nDoRAâˆ—0.838% 71.8 83.7 76.0 89.1 82.6 83.7 68.2 82.4 79.7\\nDiReFT (ours) 0.031% 70.8 83.6 80.2 93.6 82.1 84.8 70.4 81.5 80.9\\nLoReFT (ours) 0.031% 71.1 83.8 80.8 94.3 84.5 85.6 72.2 82.3 81.8\\nLlama-3 8BLoRAâˆ—0.700% 70.8 85.2 79.9 91.7 84.3 84.2 71.2 79.0 80.8\\nDoRA (half)âˆ—0.361% 74.5 88.8 80.3 95.5 84.7 90.1 79.1 87.2 85.0\\nDoRAâˆ—0.710% 74.6 89.3 79.9 95.5 85.6 90.5 80.4 85.8 85.2\\nDiReFT (ours) 0.026% 73.4 88.7 81.0 95.6 85.5 91.8 81.8 85.4 85.4\\nLoReFT (ours) 0.026% 75.1 90.2 82.0 96.3 87.4 92.4 81.6 87.5 86.6\\nHyperparameter tuning. We use the same hyperparameter settings as for the Commonsense Rea-\\nsoning benchmark, but with 12 epochs for training. We also report scores on 3 epochs.\\nDatasets. Our benchmark contains four datasets for math world problems, including AQuA [Ling\\net al., 2017], GSM8K [Cobbe et al., 2021], MAWPS [Koncel-Kedziorski et al., 2016], and SV AMP [Pa-\\ntel et al., 2021]. Models need to generate chain-of-thought [Wei et al., 2022] before the final answer.\\nWe use the same prompt template and hyperparameter settings as in the previous experiment.\\nResults. We report results in table 2. We find that both LoReFT and DiReFT do not perform as\\nwell at arithmetic reasoning tasks compared to LoRA and adapters, but do outperform prefix-tuning.\\nOur results suggest that our ReFTs may have more trouble on chain-of-thought reasoning than the\\nsingle-step commonsense reasoning tasks due to the length of generations (greater length necessarily\\nreduces the effect of the intervention) and overall greater difficulty of the task. Our results show that\\nour ReFTs perform better with the 13B model than the 7B model, which suggests that our methods\\nscale with model size. Overall, we note that the arithmetic reasoning results show a lot of variation,\\nwith no single method emerging as a clear winner across all of them.\\n4.4 Instruction-following\\nBase LMs require instruction finetuning to follow human prompts [Ouyang et al., 2022]. We follow\\nthe experimental setup in Wu et al. [2024a] and finetune Llama-2 7B with Ultrafeedback [Cui et al.,\\n2023]. We compare against full parameter finetuning, LoRA, and RED. For evaluation, we use\\nAlpaca-Eval v1.0 [Li et al., 2023], which computes the win-rate against text-davinci-003 using'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 6}, page_content='2023]. We compare against full parameter finetuning, LoRA, and RED. For evaluation, we use\\nAlpaca-Eval v1.0 [Li et al., 2023], which computes the win-rate against text-davinci-003 using\\nGPT-4 as the annotator. We use the same prompt template as in Taori et al. [2023].\\nDatasets. Ultrafeedback is high-quality instruction dataset where responses are generated via scoring\\na diverse set of model responses from a list of candidates (e.g. ChatGPT and Bard). The score\\nis calculated as a weighted score of instruction-following, truthfulness, honesty, and helpfulness.\\n7'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 7}, page_content='Table 2: Accuracy comparison of LLaMA-1 7B/13B against existing PEFT methods on four arithmetic\\nreasoning datasets.âˆ—Performance results of all baseline methods are taken from Hu et al. [2023]. We\\nreport averaged performance of three runs with distinct random seeds for our method.\\nModel PEFT Params (%)Accuracy (â†‘)\\nAQuA GSM8K MA WPS SV AMP Avg.\\nLLaMA-7BPrefTâˆ—0.039% 14.2 24.4 63.4 38.1 35.0\\nAdapterSâˆ—1.953% 15.0 33.3 77.7 52.3 44.6\\nAdapterPâˆ—3.542% 18.1 35.3 82.4 49.6 46.4\\nLoRAâˆ—0.826% 18.9 37.5 79.0 52.1 46.9\\nDiReFT (ours) 0.031% 21.3 24.1 74.5 42.7 40.6\\nLoReFT (ours) 0.031% 21.4 26.0 76.2 46.8 42.6\\nLLaMA-13BPrefTâˆ—0.031% 15.7 31.1 66.8 41.4 38.8\\nAdapterSâˆ—1.586% 22.0 44.0 78.6 50.8 48.9\\nAdapterPâˆ—2.894% 20.5 43.3 81.1 55.7 50.2\\nLoRAâˆ—0.670% 18.5 47.5 83.6 54.6 51.1\\nDiReFT (ours) 0.025% 20.5 35.8 80.8 54.8 48.0\\nLoReFT (ours) 0.025% 23.6 38.1 82.4 54.2 49.6\\nTable 3: Instruction tuning evaluation results for instruction-tuned Llama-2 7B with Alpaca-Eval\\nv1.0. We report averaged performance of two runs with distinct random seeds for our method. half\\ndenotes our runs with half of the rank; 1Kdenotes our runs with a low-resource setting where there\\nis only 1K training examples.â€ Performance results of baseline methods are taken from Li et al.\\n[2023].âˆ—Performance results of baseline methods are taken from Wu et al. [2024a].â€¡It takes 18\\nminutes to train our Llama-2 Chat 7B on 1K examples using a single A100 40G GPU with\\nâ‰ˆ1MB parameters on disk.\\nModel & PEFT Params (%) Win-rate (â†‘)\\nGPT-3.5 Turbo 1106â€ â€” 86.30\\nLlama-2 Chat 13Bâ€ â€” 81.10\\nLlama-2 Chat 7Bâ€ â€” 71.40\\nLlama-2 7B & FTâˆ—100% 80.93\\nLlama-2 7B & LoRAâˆ—0.1245% 81.48\\nLlama-2 7B & REDâˆ—0.0039% 81.69\\nLlama-2 7B & DiReFT (ours) 0.0039% 84.85\\nLlama-2 7B & LoReFT (ours) 0.0039% 85.60\\nLlama-2 7B & LoReFT (ours, half) 0.0019% 84.12\\nLlama-2 7B & LoReFT (ours, 1K)â€¡0.0039% 81.91\\nSome of the best 7B and 13B chat-models (e.g. UltraLM-13B [Ding et al., 2023]) are finetuned with\\nUltrafeedback.\\nHyperparameter tuning. We do hyperparameter-tuning on the unseen instruction-following dataset\\nAlpaca-52K [Taori et al., 2023] with only LLaMA-7B to prevent test-set hill-climbing. We then\\nuse the hyperparameter settings of our best performing model to finetune on Ultrafeedback. For\\nhyperparameter tuning, we use Alpaca-Eval v1.0 with GPT-4 turbo as the annotator for fast turnaround,\\nwhich also prevents overfitting with GPT-4 as a judge.\\nResults. We report results in table 3. When matched in parameter count to the previous most\\nparameter-efficient PEFT (RED) and trained on Llama-2 7B, LoReFT outperforms all reported\\nfinetuning methods (including full finetuning) and achieves a win-rate within 1% of GPT-3.5 Turbo\\n1106. Furthermore, after halving the parameter count or using only 1/64-th of the data, LoReFT still\\noutperforms other finetuning methods. This result shows that LoReFT can succeed at long-form text\\ngeneration. DiReFT is again slightly worse than LoReFT but is highly competitive.6\\n6We release our ReFT weights ( <1MB) of our instruction-tuned model through HuggingFace and provide a\\ntutorial at https://github.com/stanfordnlp/pyreft/blob/main/examples/chat .\\n8'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 8}, page_content='Table 4: Accuracy comparison of RoBERTa-base and RoBERTa-large against existing PEFT methods\\non the GLUE benchmark.âˆ—Performance results of all baseline methods are taken from Wu et al.\\n[2024a]. We report averaged performance of five runs with distinct random seeds for our method.\\nModel PEFT Params (%)Accuracy (â†‘)\\nMNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg.\\nbaseFT 100% 87.3 94.4 87.9 62.4 92.5 91.7 78.3 90.6 85.6\\nAdapterâˆ—0.318% 87.0 93.3 88.4 60.9 92.5 90.5 76.5 90.5 85.0\\nLoRAâˆ—0.239% 86.6 93.9 88.7 59.7 92.6 90.4 75.3 90.3 84.7\\nAdapterFNNâˆ—0.239% 87.1 93.0 88.8 58.5 92.0 90.2 77.7 90.4 84.7\\nBitFitâˆ—0.080% 84.7 94.0 88.0 54.0 91.0 87.3 69.8 89.5 82.3\\nREDâˆ—0.016% 83.9 93.9 89.2 61.0 90.7 87.2 78.0 90.4 84.3\\nDiReFT (ours) 0.015% 82.5 92.6 88.3 58.6 91.3 86.4 76.4 89.3 83.2\\nLoReFT (ours) 0.015% 83.1 93.4 89.2 60.4 91.2 87.4 79.0 90.0 84.2\\nlargeFT 100% 88.8 96.0 91.7 68.2 93.8 91.5 85.8 92.6 88.6\\nAdapterâˆ—0.254% 90.1 95.2 90.5 65.4 94.6 91.4 85.3 91.5 88.0\\nLoRAâˆ—0.225% 90.2 96.0 89.8 65.5 94.7 90.7 86.3 91.7 88.1\\nAdapterFNNâˆ—0.225% 90.3 96.1 90.5 64.4 94.3 91.3 84.8 90.2 87.7\\nREDâˆ—0.014% 89.5 96.0 90.3 68.1 93.5 88.8 86.2 91.3 88.0\\nDiReFT (ours) 0.014% 88.7 95.4 88.5 66.7 93.9 88.1 86.9 91.2 87.4\\nLoReFT (ours) 0.014% 89.2 96.2 90.1 68.0 94.1 88.5 87.5 91.6 88.2\\n4.5 Natural language understanding\\nWe evaluate LoReFT on the GLUE benchmark [Wang et al., 2018] against existing PEFTs. We use\\nthis set of experiments to show LoReFT works well even with small-scale LMs, and can improve\\nrepresentations for classification tasks and not just text generation. We finetune RoBERTa-base\\n(125M) as well as RoBERTa-large (350M) on GLUE, a sequence classification benchmark for natural\\nlanguage understanding (NLU) which covers domains such as sentiment classification and natural\\nlanguage inference. Details about the GLUE benchmark can be found in its original paper. We\\nfollow Wu et al. [2024a] for proper evaluation on GLUE validation set: we split the validation set into\\ntwo sets guarded by a random seed, and we pick the best model with highest in-training validation\\naccuracy to evaluate on the other held-out half for testing accuracy.\\nHyperparameter tuning. We tune our hyperparameters for each task separately. which is standard\\nfor PEFTs. To avoid overfitting to random seeds, we hyperparameter-tune our models with a\\nconstant seed, and report averaged results over that and four additional unseen seeds. We describe\\nhyperparameter tuning experiments in Appendix D.1.\\nResults. We report results in table 4. LoReFT obtains comparable performance with PEFT methods\\non both model sizes when parameter matched with RED, the previous most parameter-efficient PEFT\\nfor this task. Furthermore, DiReFT achieves worse performance than most of the PEFTs suggesting\\nLoReFT is a better choice when LM is small. Full results with standard deviation is in table 13. We\\nadditionally compare against VeRA [Kopiczko et al., 2024] in appendix D.3.\\n5 Limitations\\nDue to limited resources, we mainly explored the LLaMA-family of models. In future work, we hope\\nto explore the effectiveness of ReFT on other model families as well as visionâ€“language models such\\nas LLaV A [Liu et al., 2024a]. The capabilities of ReFT have not yet been fully explored due to the\\nlarge hyperparameter search space; we are interested in automating this search. We provide some\\ninitial explorations of LM personalisation with ReFT in a few-shot setting in appendix G.2. We hope\\nto explore why ReFT works, and we provide some of our early explorations focused on memorisation\\n(appendix F.1, appendix F.2). We are also investigating whether learned orthogonal subspaces can be\\ncomposed together without adaptation. Some encouraging initial findings are in appendix G.1.\\nReFT, abstraction, and generation. Neural network interpretability research often struggles to\\ncontribute directly to improving models. With ReFT, we have shown one way to overcome this'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 8}, page_content='ReFT, abstraction, and generation. Neural network interpretability research often struggles to\\ncontribute directly to improving models. With ReFT, we have shown one way to overcome this\\nchallenge. The ReFT framework is rooted in work on causal abstraction [Geiger et al., 2023a] for\\nmodel interpretability, and LoReFT builds directly on the distributed interchange intervention method\\n9'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 9}, page_content='of Geiger et al. [2023b] and Wu et al. [2023]. See also the interchange intervention training (IIT)\\nmethod of Geiger et al. [2022], Wu et al. [2022], Huang et al. [2023c]. In a similar vein, recent work\\nalso uses representation-based editing of the Transformer stream to steer model behavior [Li et al.,\\n2024, Zou et al., 2023]. ReFT advances this line of work by showing one way that such steering can\\nbe learned, rather than being merely a post hoc analysis step.\\nThe precise ways in which ReFT works deserve deeper exploration. Although these methods intervene\\non representations, the causal effect of such interventions may only emerge in the modelâ€™s upstream\\ncomputations. In other words, the power of ReFT may come from the fact that it creates new causal\\npathways or modifies the strength of some existing ones. We leave it to future research to track these\\neffects, and perhaps to explore more structured ReFTs to modify complex causal pathways in LMs.\\nReFT and model interpretability. ReFT relies on insights from work on interpretability, and it may\\nalso be able to contribute insights back to that field. In particular, LoReFT shows that training a set\\nof low-rank interventions on selected residual streams can induce a base LM to follow instructions\\n(section 4.4). In other words, a linear subspace distributed across a set of neurons can achieve\\ngeneralised control over a vast number of tasks. This is a serious challenge to work seeing to interpret\\nindividual neurons in isolation (for related criticisms, see Huang et al. 2023b). The success of ReFT\\nsuggests to us a quite different approach to interperetability, one that starts from the assumption that\\nneurons will play different roles in different contexts.\\nEvaluation practices in PEFT research. In this work, we hyperparameter-tune ReFT on develop-\\nment sets that do not overlap with the test set. Unfortunately, a considerable portion of the literature\\non PEFTs directly hill-climbs performance on test sets. This results in overfitting to specific tasks,\\nwhich gives practitioners less certainty about the real-world performance of different methods and\\nimpedes fair comparison. We hope that future work can introduce benchmarks for evaluating PEFTs\\nand ReFTs. These should allow for compute- or time-matched hyperparameter-tuning comparisons,\\nand they should disallow any kind of tuning or model selection based on the test set.\\n6 Conclusion\\nWe propose a strong alternative to PEFTs, LoReFT, and we identify an ablation of this method,\\nDiReFT, that trades some performance for increased efficiency. Overall, LoReFT achieves strong per-\\nformance across benchmarks from four domains while being 15 Ã—â€“65Ã—more efficient than LoRA. No-\\ntably, LoReFT establishes new state-of-the-art performance on commonsense reasoning, instruction-\\nfollowing, and natural language understanding against the strongest PEFTs. We also show how our\\nmethod can be described under a generic framework â€“ ReFT. ReFT is a new approach to finetuning\\nthat is more powerful, more parameter-efficient, and more interpretable than any existing PEFTs.\\nAcknowledgements\\nWe thank Jing Huang for helpful discussion in designing our memorisation tests as well as writing.\\nWe thank Chenglei Si, Harshit Joshi, Jordan Juravsky, Julie Kallini, Ken Liu, Rohan Pandey, Jiuding\\nSun, Leonard Tang, Tristan Thrush, Shengguang Wu, Qinan Yu, Yanzhe Zhang, Amir Zur, and Shiqi\\nChen for helpful discussion about the project and comments on the manuscript.\\nReferences\\nAfra Amini, Tiago Pimentel, Clara Meister, and Ryan Cotterell. Naturalistic causal probing for\\nmorpho-syntax. Transactions of the Association for Computational Linguistics , 11:384â€“403, 2023.\\ndoi: 10.1162/tacl_a_00554. URL https://aclanthology.org/2023.tacl-1.23 .\\nAryaman Arora, Dan Jurafsky, and Christopher Potts. CausalGym: Benchmarking causal inter-\\npretability methods on linguistic tasks. arXiv:2402.12560 , 2024. URL https://arxiv.org/abs/\\n2402.12560 .'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 9}, page_content='Aryaman Arora, Dan Jurafsky, and Christopher Potts. CausalGym: Benchmarking causal inter-\\npretability methods on linguistic tasks. arXiv:2402.12560 , 2024. URL https://arxiv.org/abs/\\n2402.12560 .\\nMatan Avitan, Ryan Cotterell, Yoav Goldberg, and Shauli Ravfogel. What changed? Converting\\nrepresentational interventions to natural language. arXiv:2402.11355 , 2024. URL https://arxiv.\\norg/abs/2402.11355 .\\n10'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 10}, page_content='Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, and Stella\\nBiderman. LEACE: Perfect linear concept erasure in closed form. Advances in Neural Information\\nProcessing Systems , 36, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/\\nfile/d066d21c619d0a78c5b557fa3291a8f4-Paper-Conference.pdf .\\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficient fine-\\ntuning for transformer-based masked language-models. In Smaranda Muresan, Preslav Nakov,\\nand Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for\\nComputational Linguistics (Volume 2: Short Papers) , pages 1â€“9, Dublin, Ireland, May 2022.\\nAssociation for Computational Linguistics. doi: 10.18653/v1/2022.acl-short.1. URL https:\\n//aclanthology.org/2022.acl-short.1 .\\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. PIQA: Reasoning about physical\\ncommonsense in natural language. In Proceedings of the AAAI Conference on Artificial Intelligence ,\\nvolume 34, pages 7432â€“7439, 2020. URL https://arxiv.org/abs/1911.11641 .\\nLewis Carroll. Aliceâ€™s Adventures in Wonderland . Macmillan, London, 1865.\\nAngelica Chen, Ravid Schwartz-Ziv, Kyunghyun Cho, Matthew L. Leavitt, and Naomi Saphra.\\nSudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in MLMs.\\narXiv:2309.07311 , 2023. URL https://arxiv.org/abs/2309.07311v4 .\\nAbhijith Chintam, Rahel Beloch, Willem Zuidema, Michael Hanna, and Oskar van der Wal.\\nIdentifying and adapting transformer-components responsible for gender bias in an English\\nlanguage model. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya Mc-\\nCarthy, and Hosein Mohebbi, editors, Proceedings of the 6th BlackboxNLP Workshop: Ana-\\nlyzing and Interpreting Neural Networks for NLP , pages 379â€“394, Singapore, December 2023.\\nAssociation for Computational Linguistics. doi: 10.18653/v1/2023.blackboxnlp-1.29. URL\\nhttps://aclanthology.org/2023.blackboxnlp-1.29 .\\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\\nToutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Jill\\nBurstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human Lan-\\nguage Technologies, Volume 1 (Long and Short Papers) , pages 2924â€“2936, Minneapolis, Min-\\nnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL\\nhttps://aclanthology.org/N19-1300 .\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\\nOyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning\\nchallenge. arXiv:1803.05457 , 2018. URL https://arxiv.org/abs/1803.05457 .\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve\\nmath word problems. arXiv:2110.14168 , 2021. URL https://arxiv.org/abs/2110.14168 .\\nGanqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan\\nLiu, and Maosong Sun. UltraFeedback: Boosting language models with high-quality feedback.\\narXiv:2310.01377 , 2023. URL https://arxiv.org/abs/2310.01377 .\\nAndrew M. Dai and Quoc V . Le. Semi-supervised sequence learning. In Advances in Neural\\nInformation Processing Systems , volume 28. Curran Associates, Inc., 2015. URL https://\\nproceedings.neurips.cc/paper/2015/hash/7137debd45ae4d0ab9aa953017286b20-Abstract.html .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\\ndeep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and\\nThamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 10}, page_content='Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1\\n(Long and Short Papers) , pages 4171â€“4186, Minneapolis, Minnesota, June 2019. Association\\nfor Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/\\nN19-1423 .\\n11'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 11}, page_content='Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen\\nZhou. Enhancing chat language models by scaling high-quality instructional conversations. In\\nHouda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on\\nEmpirical Methods in Natural Language Processing , pages 3029â€“3051, Singapore, December\\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.183. URL\\nhttps://aclanthology.org/2023.emnlp-main.183 .\\nNelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec,\\nZac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish,\\nJared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of superpo-\\nsition. Transformer Circuits Thread , 2022. URL https://transformer-circuits.pub/2022/toy_\\nmodel/index.html .\\nStanislav Fort. Scaling laws for adversarial attacks on language model activations, 2023. URL\\nhttp://arxiv.org/abs/2312.02780 .\\nCheng Fu, Hanxian Huang, Xinyun Chen, Yuandong Tian, and Jishen Zhao. Learn-to-Share: A\\nhardware-friendly transfer learning framework exploiting computation and parameter sharing.\\nIn Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on\\nMachine Learning, ICML 2021, 18-24 July 2021, Virtual Event , volume 139 of Proceedings of\\nMachine Learning Research , pages 3469â€“3479. PMLR, 2021. URL http://proceedings.mlr.\\npress/v139/fu21a.html .\\nPeng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu,\\nConghui He, Xiangyu Yue, et al. LLaMA-Adapter v2: Parameter-efficient visual instruction model.\\narXiv:2304.15010 , 2023. URL https://arxiv.org/abs/2304.15010 .\\nAtticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts. Causal abstractions of neural\\nnetworks. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman Vaughan,\\neditors, Advances in Neural Information Processing Systems , volume 34, pages 9574â€“9586. Curran\\nAssociates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/\\n4f5c422f4d49a5a807eda27434231040-Paper.pdf .\\nAtticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah Good-\\nman, and Christopher Potts. Inducing causal structure for interpretable neural networks. In\\nKamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato,\\neditors, Proceedings of the 39th International Conference on Machine Learning , volume 162 of\\nProceedings of Machine Learning Research , pages 7324â€“7338. PMLR, 17â€“23 Jul 2022. URL\\nhttps://proceedings.mlr.press/v162/geiger22a.html .\\nAtticus Geiger, Chris Potts, and Thomas Icard. Causal abstraction for faithful model interpretation.\\narXiv:2301.04709 , 2023a. URL https://arxiv.org/abs/2301.04709 .\\nAtticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah D. Goodman. Find-\\ning alignments between interpretable causal variables and distributed neural representations.\\narXiv:2303.02536 , 2023b. URL https://arxiv.org/abs/2303.02536 .\\nClÃ©ment Guerner, Anej Svete, Tianyu Liu, Alexander Warstadt, and Ryan Cotterell. A geometric\\nnotion of causal probing. arXiv:2307.15054 , 2023. URL https://arxiv.org/abs/2307.15054 .\\nMichael Hanna, Yonatan Belinkov, and Sandro Pezzelle. When language models fall in love: Animacy\\nprocessing in transformer language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,\\nProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing ,\\npages 12120â€“12135, Singapore, December 2023. Association for Computational Linguistics. doi:\\n10.18653/v1/2023.emnlp-main.744. URL https://aclanthology.org/2023.emnlp-main.744 .\\nSophie Hao and Tal Linzen. Verb conjugation in transformers is determined by linear encodings of\\nsubject number. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 11}, page_content='Sophie Hao and Tal Linzen. Verb conjugation in transformers is determined by linear encodings of\\nsubject number. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association\\nfor Computational Linguistics: EMNLP 2023 , pages 4531â€“4539, Singapore, December 2023.\\nAssociation for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.300. URL\\nhttps://aclanthology.org/2023.findings-emnlp.300 .\\n12'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 12}, page_content='Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards\\na unified view of parameter-efficient transfer learning. In The Tenth International Conference\\non Learning Representations, ICLR 2022 , Virtual Event, 2022a. URL https://openreview.net/\\nforum?id=0RDcd5Axok .\\nShwai He, Liang Ding, Daize Dong, Jeremy Zhang, and Dacheng Tao. SparseAdapter: An\\neasy approach for improving the parameter-efficiency of adapters. In Yoav Goldberg, Zornitsa\\nKozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics:\\nEMNLP 2022 , pages 2184â€“2190, Abu Dhabi, United Arab Emirates, December 2022b. As-\\nsociation for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.160. URL\\nhttps://aclanthology.org/2022.findings-emnlp.160 .\\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to\\nsolve arithmetic word problems with verb categorization. In Alessandro Moschitti, Bo Pang,\\nand Walter Daelemans, editors, Proceedings of the 2014 Conference on Empirical Methods in\\nNatural Language Processing (EMNLP) , pages 523â€“533, Doha, Qatar, October 2014. Association\\nfor Computational Linguistics. doi: 10.3115/v1/D14-1058. URL https://aclanthology.org/\\nD14-1058 .\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea\\nGesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In\\nKamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International\\nConference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA ,\\nvolume 97 of Proceedings of Machine Learning Research , pages 2790â€“2799. PMLR, 2019. URL\\nhttp://proceedings.mlr.press/v97/houlsby19a.html .\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\\nand Weizhu Chen. LoRA: Low-rank adaptation of large language models. In The Tenth In-\\nternational Conference on Learning Representations, ICLR 2022 , Virtual Event, 2022. URL\\nhttps://openreview.net/forum?id=nZeVKeeFYf9 .\\nZhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya\\nPoria, and Roy Lee. LLM-adapters: An adapter family for parameter-efficient fine-tuning of large\\nlanguage models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023\\nConference on Empirical Methods in Natural Language Processing , pages 5254â€“5276, Singapore,\\nDecember 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.\\n319. URL https://aclanthology.org/2023.emnlp-main.319 .\\nChengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. LoraHub:\\nEfficient cross-task generalization via dynamic lora composition. arXiv:2307.13269 , 2023a. URL\\nhttps://arxiv.org/abs/2307.13269 .\\nJing Huang, Atticus Geiger, Karel Dâ€™Oosterlinck, Zhengxuan Wu, and Christopher Potts. Rigorously\\nassessing natural language explanations of neurons. In Yonatan Belinkov, Sophie Hao, Jaap\\nJumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi, editors, Proceedings of the 6th\\nBlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP , pages 317â€“331,\\nSingapore, December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.\\nblackboxnlp-1.24. URL https://aclanthology.org/2023.blackboxnlp-1.24 .\\nJing Huang, Zhengxuan Wu, Kyle Mahowald, and Christopher Potts. Inducing character-level\\nstructure in subword-based language models with type-level interchange intervention training.\\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association\\nfor Computational Linguistics: ACL 2023 , pages 12163â€“12180, Toronto, Canada, July 2023c.\\nAssociation for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.770. URL https:\\n//aclanthology.org/2023.findings-acl.770 .\\nJing Huang, Christopher Potts Zhengxuan Wu, Mor Geva, and Atticus Geiger. RA VEL: Evaluating'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 12}, page_content='//aclanthology.org/2023.findings-acl.770 .\\nJing Huang, Christopher Potts Zhengxuan Wu, Mor Geva, and Atticus Geiger. RA VEL: Evaluating\\ninterpretability methods on disentangling language model representations. arXiv:2402.17700 ,\\n2024. URL https://arxiv.org/abs/2402.17700 .\\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang.\\nParsing algebraic word problems into equations. Transactions of the Association for Computational\\n13'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 13}, page_content='Linguistics , 3:585â€“597, 2015. doi: 10.1162/tacl_a_00160. URL https://aclanthology.org/\\nQ15-1042 .\\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS:\\nA math word problem repository. In Kevin Knight, Ani Nenkova, and Owen Rambow, editors,\\nProceedings of the 2016 Conference of the North American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technologies , pages 1152â€“1157, San Diego, Califor-\\nnia, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1136. URL\\nhttps://aclanthology.org/N16-1136 .\\nDawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. VeRA: Vector-based random matrix\\nadaptation. In The Twelfth International Conference on Learning Representations, ICLR 2024 ,\\n2024. URL https://openreview.net/forum?id=NjNfLdxr3A .\\nKarim Lasri, Tiago Pimentel, Alessandro Lenci, Thierry Poibeau, and Ryan Cotterell. Prob-\\ning for the usage of grammatical number. In Smaranda Muresan, Preslav Nakov, and Aline\\nVillavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Com-\\nputational Linguistics (Volume 1: Long Papers) , pages 8818â€“8831, Dublin, Ireland, May\\n2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.603. URL\\nhttps://aclanthology.org/2022.acl-long.603 .\\nHector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd Schema Challenge. In\\nProceedings of the Thirteenth International Conference on Principles of Knowledge Representation\\nand Reasoning , 2012. URL https://cdn.aaai.org/ocs/4492/4492-21843-1-PB.pdf .\\nKenneth Li, Oam Patel, Fernanda ViÃ©gas, Hanspeter Pfister, and Martin Wattenberg. Inference-time\\nintervention: Eliciting truthful answers from a language model. Advances in Neural Information\\nProcessing Systems , 36, 2024. URL https://proceedings.neurips.cc/paper_files/paper/2023/\\nhash/81b8390039b7302c909cb769f8b6cd93-Abstract-Conference.html .\\nMargaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A. Smith, and Luke\\nZettlemoyer. Branch-train-merge: Embarrassingly parallel training of expert language models.\\narXiv:2208.03306 , 2022. URL https://arxiv.org/abs/2208.03306 .\\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.\\nIn Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th\\nAnnual Meeting of the Association for Computational Linguistics and the 11th International Joint\\nConference on Natural Language Processing (Volume 1: Long Papers) , pages 4582â€“4597, Online,\\nAugust 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353.\\nURL https://aclanthology.org/2021.acl-long.353 .\\nXuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy\\nLiang, and Tatsunori B. Hashimoto. AlpacaEval: An automatic evaluator of instruction-following\\nmodels. https://github.com/tatsu-lab/alpaca_eval , 2023.\\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale gener-\\nation: Learning to solve and explain algebraic word problems. arXiv:1705.04146 , 2017. URL\\nhttps://arxiv.org/abs/1705.04146 .\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in\\nNeural Information Processing Systems , 36, 2024a. URL https://arxiv.org/abs/2304.08485 .\\nSheng Liu, Haotian Ye, Lei Xing, and James Zou. In-context vectors: Making in context learning\\nmore effective and controllable through latent space steering. arXiv:2311.06668 , 2024b. URL\\nhttps://arxiv.org/abs/2311.06668 .\\nShih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting\\nCheng, and Min-Hung Chen. DoRA: Weight-decomposed low-rank adaptation. arXiv:2402.09353 ,\\n2024c. URL https://arxiv.org/abs/2402.09353 .\\nWeiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen\\nLiu, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, and Bern-'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 13}, page_content='Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen\\nLiu, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, and Bern-\\nhard SchÃ¶lkopf. Parameter-efficient orthogonal finetuning via butterfly factorization. In The\\n14'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 14}, page_content='Twelfth International Conference on Learning Representations, ICLR 2024 , 2024d. URL\\nhttps://openreview.net/forum?id=7NzgkEdGyr .\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining\\napproach. arXiv:1907.11692 , 2019. URL https://arxiv.org/abs/1907.11692 .\\nJames L. McClelland, David E. Rumelhart, and PDP Research Group. Parallel Distributed Processing:\\nExplorations in the Microstructure of Cognition , volume 2: Psychological and Biological Models.\\nMIT Press, 1986.\\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\\nelectricity? A new dataset for open book question answering. arXiv:1809.02789 , 2018. URL\\nhttps://arxiv.org/abs/1809.02789 .\\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word\\nrepresentations. In Lucy Vanderwende, Hal DaumÃ© III, and Katrin Kirchhoff, editors, Proceedings\\nof the 2013 Conference of the North American Chapter of the Association for Computational Lin-\\nguistics: Human Language Technologies , pages 746â€“751, Atlanta, Georgia, June 2013. Association\\nfor Computational Linguistics. URL https://aclanthology.org/N13-1090 .\\nNeel Nanda, Andrew Lee, and Martin Wattenberg. Emergent linear representations in world models\\nof self-supervised sequence models. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim,\\nArya McCarthy, and Hosein Mohebbi, editors, Proceedings of the 6th BlackboxNLP Workshop:\\nAnalyzing and Interpreting Neural Networks for NLP , pages 16â€“30, Singapore, December 2023.\\nAssociation for Computational Linguistics. doi: 10.18653/v1/2023.blackboxnlp-1.2. URL https:\\n//aclanthology.org/2023.blackboxnlp-1.2 .\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\\ninstructions with human feedback. Advances in Neural Information Processing Systems , 35:\\n27730â€“27744, 2022. URL https://arxiv.org/abs/2203.02155 .\\nKiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry\\nof large language models. arXiv:2311.03658 , 2023. URL https://arxiv.org/abs/2311.03658 .\\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve sim-\\nple math word problems? In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek\\nHakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou,\\neditors, Proceedings of the 2021 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies , pages 2080â€“2094, Online, June\\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.168. URL\\nhttps://aclanthology.org/2021.naacl-main.168 .\\nJonas Pfeiffer, Ivan Vuli Â´c, Iryna Gurevych, and Sebastian Ruder. MAD-X: An Adapter-Based\\nFramework for Multi-Task Cross-Lingual Transfer. In Bonnie Webber, Trevor Cohn, Yulan He,\\nand Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) , pages 7654â€“7673, Online, November 2020. Association for\\nComputational Linguistics. doi: 10.18653/v1/2020.emnlp-main.617. URL https://aclanthology.\\norg/2020.emnlp-main.617 .\\nShauli Ravfogel, Michael Twiton, Yoav Goldberg, and Ryan D. Cotterell. Linear adversarial concept\\nerasure. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and\\nSivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning ,\\nvolume 162 of Proceedings of Machine Learning Research , pages 18400â€“18421, 17â€“23 Jul 2022.\\nURL https://proceedings.mlr.press/v162/ravfogel22a.html .\\nSubhro Roy and Dan Roth. Solving general arithmetic word problems. In LluÃ­s MÃ rquez, Chris\\nCallison-Burch, and Jian Su, editors, Proceedings of the 2015 Conference on Empirical Methods in'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 14}, page_content='Subhro Roy and Dan Roth. Solving general arithmetic word problems. In LluÃ­s MÃ rquez, Chris\\nCallison-Burch, and Jian Su, editors, Proceedings of the 2015 Conference on Empirical Methods in\\nNatural Language Processing , pages 1743â€“1752, Lisbon, Portugal, September 2015. Association\\nfor Computational Linguistics. doi: 10.18653/v1/D15-1202. URL https://aclanthology.org/\\nD15-1202 .\\n15'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 15}, page_content='David E. Rumelhart, James L. McClelland, and PDP Research Group. Parallel Distributed Processing:\\nExplorations in the Microstructure of Cognition , volume 1: Foundations. MIT Press, 1986.\\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An\\nadversarial Winograd Schema Challenge at scale. Communications of the ACM , 64(9):99â€“106,\\n2021. URL https://arxiv.org/abs/1907.10641 .\\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Common-\\nsense reasoning about social interactions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun\\nWan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language\\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-\\nIJCNLP) , pages 4463â€“4473, Hong Kong, China, November 2019. Association for Computational\\nLinguistics. doi: 10.18653/v1/D19-1454. URL https://aclanthology.org/D19-1454 .\\nShuhua Shi, Shaohan Huang, Minghui Song, Zhoujun Li, Zihan Zhang, Haizhen Huang, Furu Wei,\\nWeiwei Deng, Feng Sun, and Qi Zhang. ResLoRA: Identity residual mapping in low-rank adaption.\\narXiv:2402.18039 , 2024. URL https://arxiv.org/abs/2402.18039 .\\nShashwat Singh, Shauli Ravfogel, Jonathan Herzig, Roee Aharoni, Ryan Cotterell, and Ponnu-\\nrangam Kumaraguru. MiMiC: Minimally modified counterfactuals in the representation space.\\narXiv:2402.09631 , 2024. URL https://arxiv.org/abs/2402.09631 .\\nPaul Smolensky. Neural and conceptual interpretation of PDP models. In Parallel Distributed\\nProcessing: Explorations in the Microstructure of Cognition , volume 2: Psychological and\\nBiological Models, pages 390â€“431. MIT Press/Bradford Books, Cambridge, MA, 1986.\\nNishant Subramani, Nivedita Suresh, and Matthew E. Peters. Extracting latent steering vectors from\\npretrained language models. arXiv:2205.05124 , 2022. URL https://arxiv.org/abs/2205.05124 .\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\\nLiang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instruction-following LLaMA model.\\nhttps://github.com/tatsu-lab/stanford_alpaca , 2023.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e\\nLacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\\nJoulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language\\nmodels. arXiv:2302.13971 , 2023a. URL https://arxiv.org/abs/2302.13971 .\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-\\ntian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,\\nWenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,\\n2023b. URL https://arxiv.org/abs/2307.09288 .\\nAlex Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, and Monte MacDiarmid.\\nActivation addition: Steering language models without optimization. arXiv:2308.10248 , 2023.\\nURL https://arxiv.org/abs/2308.10248 .\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 15}, page_content='URL https://arxiv.org/abs/2308.10248 .\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nÅukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,\\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\\nInformation Processing Systems , volume 30, pages 5998â€“6008. Curran Associates, Inc., 2017.\\nURL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf .\\nTheia V ogel. repeng, 2024. URL https://github.com/vgel/repeng/ .\\n16'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 16}, page_content='Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:\\nA multi-task benchmark and analysis platform for natural language understanding. In Tal Linzen,\\nGrzegorz ChrupaÅ‚a, and Afra Alishahi, editors, Proceedings of the 2018 EMNLP Workshop Black-\\nboxNLP: Analyzing and Interpreting Neural Networks for NLP , pages 353â€“355, Brussels, Belgium,\\nNovember 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL\\nhttps://aclanthology.org/W18-5446 .\\nKevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.\\nInterpretability in the wild: a circuit for indirect object identification in GPT-2 small. In The\\nEleventh International Conference on Learning Representations, ICLR 2023 , Kigali, Rwanda,\\n2023. URL https://openreview.net/pdf?id=NpsVSN6o4ul .\\nYaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan\\nAwadallah, and Jianfeng Gao. AdaMix: Mixture-of-adaptations for parameter-efficient model\\ntuning. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022\\nConference on Empirical Methods in Natural Language Processing , pages 5744â€“5760, Abu\\nDhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi:\\n10.18653/v1/2022.emnlp-main.388. URL https://aclanthology.org/2022.emnlp-main.388 .\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\\nNeural Information Processing Systems , 35:24824â€“24837, 2022. URL https://arxiv.org/abs/\\n2201.11903 .\\nMuling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li, Changze Lv, Zixuan Ling, Jianhao Zhu,\\nCenyuan Zhang, Xiaoqing Zheng, and Xuanjing Huang. Advancing parameter efficiency in fine-\\ntuning via representation editing. arXiv:2402.15179 , 2024a. URL https://arxiv.org/abs/2402.\\n15179 .\\nZhengxuan Wu, Atticus Geiger, Joshua Rozner, Elisa Kreiss, Hanson Lu, Thomas Icard, Christo-\\npher Potts, and Noah Goodman. Causal distillation for language models. In Marine Carpuat,\\nMarie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022\\nConference of the North American Chapter of the Association for Computational Linguis-\\ntics: Human Language Technologies , pages 4288â€“4295, Seattle, United States, July 2022.\\nAssociation for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.318. URL\\nhttps://aclanthology.org/2022.naacl-main.318 .\\nZhengxuan Wu, Atticus Geiger, Christopher Potts, and Noah D. Goodman. Interpretability at\\nscale: Identifying causal mechanisms in Alpaca. In Advances in Neural Information Processing\\nSystems , volume 36, 2023. URL https://papers.neurips.cc/paper_files/paper/2023/file/\\nf6a8b109d4d4fd64c75e94aaf85d9697-Paper-Conference.pdf .\\nZhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang, Noah D. Goodman,\\nChristopher D. Manning, and Christopher Potts. pyvene: A library for understanding and improving\\nPyTorch models via interventions. In arXiv:2403.07809 , 2024b. URL https://arxiv.org/abs/\\n2403.07809 .\\nZhengxuan Wu, Atticus Geiger, Jing Huang, Aryaman Arora, Thomas Icard, Christopher Potts,\\nand Noah D. Goodman. A reply to Makelov et al. (2023)â€™s â€œinterpretability illusionâ€ arguments.\\narXiv:2401.12631 , 2024c. URL https://arxiv.org/abs/2401.12631 .\\nTakateru Yamakoshi, James McClelland, Adele Goldberg, and Robert Hawkins. Causal in-\\nterventions expose implicit situation models for commonsense language understanding. In\\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association\\nfor Computational Linguistics: ACL 2023 , pages 13265â€“13293, Toronto, Canada, July 2023.\\nAssociation for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.839. URL\\nhttps://aclanthology.org/2023.findings-acl.839 .\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 16}, page_content='https://aclanthology.org/2023.findings-acl.839 .\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine\\nreally finish your sentence? arXiv:1905.07830 , 2019. URL https://arxiv.org/abs/1905.07830 .\\n17'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 17}, page_content='Feiyu Zhang, Liangzhi Li, Junhao Chen, Zhouqiang Jiang, Bowen Wang, and Yiming Qian.\\nIncreLoRA: Incremental parameter allocation method for parameter-efficient fine-tuning.\\narXiv:2308.12043 , 2023. URL https://arxiv.org/abs/2308.12043 .\\nJinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. Composing parameter-efficient modules\\nwith arithmetic operation. Advances in Neural Information Processing Systems , 36, 2024a. URL\\nhttps://arxiv.org/abs/2306.14870 .\\nRenrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao.\\nLLaMA-Adapter: Efficient fine-tuning of large language models with zero-initialized attention. In\\nThe Twelfth International Conference on Learning Representations , Vienna, Austria, 2024b. URL\\nhttps://openreview.net/forum?id=d4UiXAHN2W .\\nRuiyi Zhang, Rushi Qiang, Sai Ashish Somayajula, and Pengtao Xie. AutoLoRA: Automatically\\ntuning matrix ranks in low-rank adaptation based on meta learning. arXiv:2403.09113 , 2024c.\\nURL https://arxiv.org/abs/2403.09113 .\\nMing Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu,\\nJiawei Han, and Weizhu Chen. Multi-LoRA composition for image generation. arXiv:2402.16843 ,\\n2024. URL https://arxiv.org/abs/2402.16843 .\\nYun Zhu, Nevan Wichers, Chu-Cheng Lin, Xinyi Wang, Tianlong Chen, Lei Shu, Han Lu, Ca-\\nnoee Liu, Liangchen Luo, Jindong Chen, et al. SiRa: Sparse mixture of low rank adaptation.\\narXiv:2311.09179 , 2023. URL https://arxiv.org/abs/2311.09179 .\\nZhou Ziheng, Yingnian Wu, Song-Chun Zhu, and Demetri Terzopoulos. Aligner: One global token\\nis worth millions of parameters when aligning large language models. arXiv:2312.05503 , 2023.\\nURL https://arxiv.org/abs/2312.05503 .\\nAndy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan,\\nXuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J.\\nByun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson,\\nJ. Zico Kolter, and Dan Hendrycks. Representation engineering: A top-down approach to AI\\ntransparency. arXiv:2310.01405 , 2023. URL https://arxiv.org/abs/2310.01405 .\\n18'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 18}, page_content='Appendix\\nTable of Contents\\nApyreft : A ReFT-native Python Library 20\\nB Describing existing methods under the ReFT framework 20\\nB.1 RED . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\nB.2 Activation addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\nB.3 RepE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\nC Datasets 21\\nC.1 Commonsense reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\nC.2 Arithmetic reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\nC.3 Natural language understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\nD Hyperparameters 23\\nD.1 Hyperparameter tuning and decoding strategy . . . . . . . . . . . . . . . . . . . . 23\\nD.2 Suggestions on choosing hyperparameters for ReFT . . . . . . . . . . . . . . . . . 29\\nD.3 Additional hyperparameter-tuning results of LoReFT . . . . . . . . . . . . . . . . 29\\nE Ablating the parametrisation of LoReFT 32\\nF Memorisation experiments 33\\nF.1 A single vector is worth a thousand tokens . . . . . . . . . . . . . . . . . . . . . . 33\\nF.2 A single vector can memorise a codebook with 256 entries . . . . . . . . . . . . . 35\\nG Capabilities experiments 36\\nG.1 Multi-task learning: Learned ReFTs are like puzzle pieces . . . . . . . . . . . . . 36\\nG.2 Few-shot adaptation: Adapting Llama-2-Chat to GOODY-2 with 5 examples . . 38\\nH Inference overhead analysis of ReFT with our ReFT library 39\\nI Generation examples 41\\nJ Licenses for existing assets 49\\nJ.1 Commonsense reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\nJ.2 Arithmetic reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\nJ.3 Instruct-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\nJ.4 Natural language understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\nJ.5 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\\n19'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 19}, page_content='Apyreft : A ReFT-native Python Library\\nTo lower the cost of switching from PEFTs to ReFT, we release a Python library made for training\\nand sharing ReFTs. Our library is built on top of pyvene [Wu et al., 2024b], a library for performing\\nand training activation interventions on arbitrary PyTorch models. Any pretrained LM available\\non HuggingFace is supported through our library for finetuning with ReFT methods, and finetuned\\nmodels can be easily uploaded to HuggingFace. The following example shows steps to wrap a\\nLlama-2 7B model with a single intervention on the residual stream output of the 19-th layer:\\nimport torch\\nimport transformers\\nfrom pyreft import get_reft_model , ReftConfig , LoreftIntervention , ReftTrainerForCausalLM\\n# loading huggingface model\\nmodel_name_or_path = \" yahma /llama -7b-hf\"\\nmodel = transformers . AutoModelForCausalLM . from_pretrained (\\nmodel_name_or_path , torch_dtype = torch . bfloat16 , device_map =\" cuda \")\\n# wrap the model with rank -1 loreft\\nreft_config = ReftConfig ( representations ={\\n\" layer \": 19, \" component \": \" block_output \",\\n\" intervention \": LoreftIntervention (\\nembed_dim = model . config . hidden_size , low_rank_dimension =1) })\\nreft_model = get_reft_model (model , reft_config )\\nreft_model . print_trainable_parameters ()\\nThe wrapped model can be trained for downstream tasks. We also provide data loading helpers to\\nconstruct training data that is compatible with HuggingFace trainers:\\ntokenizer = transformers . AutoTokenizer . from_pretrained ( model_name_or_path )\\n# get training data with customised dataloaders\\ndata_module = make_supervised_data_module (\\ntokenizer = tokenizer , model =model , layers =[19] ,\\ntraining_args = training_args , data_args = data_args )\\n# train\\ntrainer = reft . ReftTrainerForCausalLM (\\nmodel = reft_model , tokenizer = tokenizer , args = training_args , ** data_module )\\ntrainer . train ()\\ntrainer . save_model ( output_dir = training_args . output_dir )\\nB Describing existing methods under the ReFT framework\\nTo show the expressivity of the ReFT framework, we cast existing representing-editing methods in\\nthe literature into ReFTs.\\nGeneral comments about expressivity of ReFT. Given that previous works have unified PEFTs\\nunder a single framework [He et al., 2022a], one may ask why not express ReFT as a PEFT\\nmethod? The main reason is that PEFT frameworks lack the notion of time orsequence (see the\\nunified PEFT view provided in Table 1 on pg. 5 of He et al., 2022a). In PEFTs, representation\\nmodifications are necessarily applied to every token in the sequence, even in recent variants such as\\nAdaLoRA [Zhang et al., 2023]. A key aspect of ReFT is that it leverages representations over time\\nand intervenes only on a small number of them while being effective. More importantly, the notation\\nof time is important for future versions of ReFT that intervene on representations schematically\\n(e.g. intervene on the first token at some early layers and then intervene on the last token at some later\\nlayers). The ability to intervene at different layer and position combinations schematically is also\\nsupported in our code. Existing PEFT libraries7enforce weight-based updates without supporting\\nflexible representation-based interventions.\\nB.1 RED\\nRED [Wu et al., 2024a] is a simple representation-editing method that applies an element-wise scaling\\ntransform sâˆˆRnand adds a bias bâˆˆRnto the hidden representation in every layer. The same\\nintervention is applied to every position (including at generated tokens, increasing inference burden)\\n7Seehttps://github.com/huggingface/peft .\\n20'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 20}, page_content='but separate interventions are learned at each layer. In the ReFT framework, RED is defined as\\nÎ¦RED(h)=sÃ—h+b (8)\\nIRED={âŸ¨Î¦RED,{1, . . . , n}, lâŸ© âˆ£lâˆˆ{1, . . . , m}} (9)\\nThe parameters Ï•RED={s,b}are learned with gradient descent to minimise a loss function such as\\nlanguage-modelling loss or a classification loss, as in our experiments with LoReFT. We believe that\\nRED is better classified as a kind of adapter due to its application at all positions.\\nB.2 Activation addition\\nActivation addition [Turner et al., 2023] takes the difference in activations at at some positions pand\\nqand layer lgiven two contrastive prompts x+andxâˆ’as input. It then adds this difference vector,\\nscaled by a tuned constant c, to representations at all positions in layer lfor some new prompt.\\na=h(x+)(l)\\npâˆ’h(xâˆ’)(l)\\nq (10)\\nÎ¦ActAdd(h)=h+câ‹…a (11)\\nIActAdd={âŸ¨Ï•ActAdd ,{1, . . . , n}, lâŸ©} (12)\\nB.3 RepE\\nZou et al. [2023] introduce several intervention methods for controlling model behaviour, which they\\nterm representation engineering .\\nFirst, given a set of prompts {x1, . . . ,xn}designed to elicit the presence of a concept, we randomly\\npair them, take the difference in activations for each pair, and find the first principle component of the\\ndifference vectors at the last token position in some layer of interest lto obtain a reading vector :\\nareading=PCA({h(xi)(l)\\nâˆ’1âˆ’h(xi+1)(l)\\nâˆ’1âˆ£iâ‰¡0 mod 2 })\\n1(13)\\nOne can also used a more structured pairing of constrastive prompts to obtain a contrast vector ,\\nsimilar to the difference vector computed in activation addition:\\nacontrast=PCA({h(x+\\ni)(l)\\nâˆ’1âˆ’h(xâˆ’\\ni)(l)\\nâˆ’1âˆ£1â‰¤iâ‰¤n})\\n1(14)\\nThen, using either areading oracontrast , RepE introduces three operators (i.e. parametrisations of Î¦)\\nfor intervening on activations:\\nÎ¦RepE,linear(h)=hÂ±câ‹…a (15)\\nÎ¦RepE,piecewise(h)=h+câ‹…sign(aâ‹…h)â‹…a (16)\\nÎ¦RepE,projection(h)=hâˆ’câ‹…aâ‹…h\\nâˆ¥aâˆ¥2â‹…a (17)\\nThe first two of these are similar to activation addition, while the latter is a scaled one-dimensional\\ndistributed interchange intervention that is a special case of LoReFT. These operations are then used\\nto intervene on some set of positions PâŠ†{1, . . . , n}in the layer of interest:\\nIRepE={âŸ¨Î¦RepE, P, lâŸ©} (18)\\nRepE introduces another model control method called Low-Rank Representation Adaptation\\n(LoRRA), which is a kind of PEFT rather than a ReFT since it tunes model weights using a variant of\\nLoRA.\\nC Datasets\\nC.1 Commonsense reasoning\\nWe train and evaluate our models on eight datasets covering different domains of open-ended QA\\ntasks:\\n1.TheBoolQ [Clark et al., 2019] dataset, which is a question-answering dataset for yes or\\nno naturally occurring questions. We remove the provided passage in the dataset following\\nprevious works to ensure a fair comparison.\\n21'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 21}, page_content='2.ThePIQA [Bisk et al., 2020] dataset, which tests physical commonsense reasoning and\\nrequires the model to choose one of the provided actions to take based on a hypothesised\\nscenario.\\n3.TheSIQA [Sap et al., 2019] dataset, which focus on reasoning about peopleâ€™s actions and\\ntheir corresponding social consequences.\\n4.TheHellaSwag [Zellers et al., 2019] dataset, which asks the model to choose an appropriate\\nending (or sentence completion) given a context.\\n5.TheWinoGrande [Sakaguchi et al., 2021] dataset, inspired by Winograd Schema Chal-\\nlenge [Levesque et al., 2012], asks the model to fill-in-a-blank with binary options given a\\nsentence which requires commonsense reasoning.\\n6.The ARC Easy set ( ARC-e [Clark et al., 2018]), which includes genuine grade-school level\\nmultiple-choice science questions\\n7.The ARC Challenge set ( ARC-c ) [Clark et al., 2018]), which is like ARC-e but designed in\\na way that co-occurrence methods are expected to fail to answer correctly.\\n8.TheOBQA [Mihaylov et al., 2018] dataset, which is a knowledge-intensive and open-book\\nQA dataset that requires multi-hop reasoning. Dataset statistics and simplified training\\nexamples from each dataset are provided in Hu et al. [2023].\\nDataset statistics and simplified training examples from each dataset are provided in Hu et al. [2023].\\nWe replicate the experimental setup in Hu et al. [2023] and finetune our models on a combined training\\ndataset ( COMMONSENSE 170K ) of the tasks mentioned above, and evaluate on their individual test\\nset.\\nC.2 Arithmetic reasoning\\nWe train and evaluate with seven datasets covering different domains of math world problems:\\n1.TheAddSub [Hosseini et al., 2014] dataset, which involves solving arithmetic word prob-\\nlems that include addition and subtraction.\\n2.The AQuA [Ling et al., 2017] dataset, which formulates algebraic word problems as\\nmultiple-choice problems.\\n3.The GSM8K [Cobbe et al., 2021] dataset, which consists of grade-school math word\\nproblems that require multi-step reasoning.\\n4.TheMA WPS [Koncel-Kedziorski et al., 2016] dataset, which contains math word problem\\nwith varying complexity.\\n5.TheMultiArith [Roy and Roth, 2015] dataset, which contains multi-step arithmetic prob-\\nlems.\\n6.TheSingleEq [Koncel-Kedziorski et al., 2015] dataset, which has grade-school math word\\nproblems that map to single equations with different length.\\n7.TheSV AMP [Patel et al., 2021] dataset, which enhances the original Math World Prob-\\nlem (MWP) challenge by requiring robust reasoning ability that is invariant to structural\\nalternations of the posing problem.\\nDataset statistics and simplified training examples from each dataset are provided in Hu et al. [2023].\\nWe replicate the experimental setup in Hu et al. [2023] and finetune our models on a combined\\ntraining dataset ( MATH10K) of four tasks mentioned above: GSM8K, MAWPS, MAWPS-single\\nand AQuA. Different from Hu et al. [2023], selected tasks are excluded for testing since the original\\npaper accidentally leaks testing examples from these tasks into the training set, affecting AddSub,\\nMultiArith and SingleEq. They are included in the MAWPS training dataset, and thus leaked into the\\ntraining dataset.\\nC.3 Natural language understanding\\nWe follow Wu et al. [2024a] for proper evaluation on the GLUE validation set. We split the validation\\nset into two subsets, using one subset guarded by a random seed for in-training evaluation and the\\nother for testing. Specifically, after each training epoch, we evaluate the model on our in-training\\n22'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 22}, page_content='evaluation set and select the best model across all epochs for testing. For datasets with a large\\nvalidation set (i.e., QQP, MNLI, and QNLI), we select 1,000 samples for in-training evaluation. For\\nthe remaining smaller datasets, we select half of the samples for this purpose. For the evaluation\\nmetric, we use the Matthews correlation coefficient for CoLA, the Pearson correlation coefficient for\\nSTS-B, and accuracy for the other datasets. For MNLI, we report results only on the matched version.\\nD Hyperparameters\\nD.1 Hyperparameter tuning and decoding strategy\\nCommonsense reasoning and arithmeric reasoning. We create a standalone development set by\\ntaking the last 300 examples from the GSM8K training set. We train our models with the remaining\\ntraining set of GSM8K and select the hyperparameter settings based on model performance on the\\ndevelopment set. We select the hyperparameters using LLaMA-7B, and apply the same settings to\\nLLaMA-13B without additional tuning. We use a maximum sequence length of 512 for training and\\nhyperparameter tuning, and a maximum new token number of 32 for inference. Table 5 and table 6\\ndescribes our hyperparameter search space. We use a lower number of epochs (6 instead of 12) for\\nthe commonsense reasoning benchmark because the COMMONSENSE 170K training set is more than\\n20 times larger than GSM8K.\\nDuring inference, we use greedy decoding without sampling for the commonsense reasoning bench-\\nmark, since it is a multi-token classification benchmark, and use the same decoding strategy as in\\nHu et al. [2023] for the arithmetic reasoning benchmark with a higher temperature 0.3. The reason\\nto switch to a slightly different set of decoding hyperparameters is that the HuggingFace decoding\\nfunction may throw an error due to statistical instability with close-to-zero probabilities over output\\ntokens with beam search.8\\nInstruction following. We finetune LLaMA-7B on Alpaca-52K [Taori et al., 2023] to select hy-\\nperparameters. We select the hyperparameter settings based on model performance evaluated with\\nAlpaca-Eval v1.0 [Li et al., 2023], which calculates the win-rate over text-davinci-003 by using\\ngpt-4-turbo as the annotator. We use a maximum sequence length of 768 for training and hyper-\\nparameter tuning, and a maximum new token number of 2048 for inference. Table 7 describes our\\nhyperparameter search space.\\nDuring inference, we use the same decoding strategy as in RED [Wu et al., 2024a] to ensure a fair\\ncomparison. Specifically, we use greedy decoding without sampling, and use a maximum repetition\\nn-gram size of 5 with a repetition penalty of 1.1.\\nNatural language understanding. We conduct hyperparameter tuning with RoBERTa-base and\\nRoBERTa-large for each task individually. We pick the hyperparameters based on testing performance\\non the held-out validation set with a fixed random seed of 42. We then evaluate our model with\\nadditional four unseen seeds {43, 44, 45, 46} for final results. We follow Wu et al. [2024a]â€™s setting\\nfor evaluation. For QQP with RoBERTa-large, there are some stochasticity in runs with the same\\nseed, so we picked the best run out of 3 runs for any particular seed. As reported by Wu et al. [2024a],\\nwe also observe that evaluation results on RTE are unstable due to the small size of the dataset. We\\nthus replace several random seeds as in Wu et al. [2024a] to ensure a fair comparison. In addition, we\\nreplace one or two random seeds for CoLA for stability. Table 8 describes our hyperparameter search\\nspace. Table 9 to table 12 describe our hyperparameter settings for each task.\\nWe conduct separate hyperparameter tuning for LoReFT and DiReFT to ensure a fair comparison.\\n8See reference ticket: https://github.com/huggingface/transformers/issues/11267 .\\n23'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 23}, page_content='Table 5: Hyperparameter search space of LLaMA-1 7B models with LoReFT on the GSM8K\\ndevelopment set with the best settings underlined . We use greedy decoding without sampling during\\nhyperparameter tuning.\\nHyperparameters LLaMA-7B w/ GSM8K for LoReFT\\nprefix+suffix position p+s {p1+s1,p3+s3,p5+s5,p7+s7,p9+s9,p11+s11}\\nTied weight p,s {True , False}\\nRank r {8, 16, 32, 64}\\nLayer L(sep. w/ â€˜;â€™) {0;2;4;6;10;12;14;18, 10;12;14;18;20;22;24;28, 4;6;10;12;14;18;20;22, all }\\nDropout {0.00, 0.05 }\\nOptimizer AdamW\\nLR {9 Ã—10âˆ’5, 1Ã—10âˆ’4, 3Ã—10âˆ’4, 6Ã—10âˆ’4, 9Ã—10âˆ’4, 1Ã—10âˆ’3, 3Ã—10âˆ’3}\\nWeight decay {0 , 1Ã—10âˆ’3, 2Ã—10âˆ’3}\\nLR scheduler Linear\\nBatch size {4, 8, 16, 32 , 64}\\nWarmup ratio {0.00, 0.06, 0.10 }\\nEpochs {3, 6, 9, 12 , 18}\\nTable 6: Hyperparameter search space of LLaMA-1 7B models with DiReFT on the GSM8K\\ndevelopment set with the best settings underlined . We use greedy decoding without sampling during\\nhyperparameter tuning.\\nHyperparameters LLaMA-7B w/ GSM8K for DiReFT\\nprefix+suffix position p+s {p1+s1,p3+s3,p5+s5,p7+s7,p9+s9,p11+s11}\\nTied weight p,s {True, False }\\nRank r {8, 16 , 32, 64}\\nLayer L(sep. w/ â€˜;â€™) {0;2;4;6;10;12;14;18, 10;12;14;18;20;22;24;28, 4;6;10;12;14;18;20;22 , all}\\nDropout {0.00, 0.05 }\\nOptimizer AdamW\\nLR {9 Ã—10âˆ’5, 1Ã—10âˆ’4, 3Ã—10âˆ’4, 6Ã—10âˆ’4, 9Ã—10âˆ’4, 1Ã—10âˆ’3, 3Ã—10âˆ’3}\\nWeight decay {0, 1 Ã—10âˆ’3, 2Ã—10âˆ’3, 6Ã—10âˆ’3, 1Ã—10âˆ’2, 2Ã—10âˆ’2, 6Ã—10âˆ’2}\\nLR scheduler Linear\\nBatch size {4, 8 , 16, 32, 64}\\nWarmup ratio {0.00, 0.06 , 0.10}\\nEpochs {3, 6 , 9, 12, 18}\\n24'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 24}, page_content='Table 7: Hyperparameter search space of LLaMA-1 7B models on Alpaca-52K evaluated by Alpaca-\\nEval v1.0 with the best settings underlined . We use greedy decoding without sampling during\\nhyperparameter tuning. LoReFT and DiReFT have the same hyperparameter settings.\\nHyperparameters LLaMA-7B w/ Alpaca-52K\\nprefix+suffix position p+s{p1+s1,p3+s3,p5+s5,p7+s7}\\nTied weight p,s {True , False}\\nRank r {1, 2, 3, 4 , 5, 6}\\nLayer L(sep. w/ â€˜;â€™) {9;18, 3;9;18, 3;9;18;24 }\\nDropout {0.00, 0.05 }\\nOptimizer AdamW\\nLR 9 Ã—10âˆ’4\\nWeight decay 0 Ã—10âˆ’3\\nLR scheduler Linear\\nBatch size {16, 32, 64, 128 }\\nWarmup ratio 0.00\\nEpochs {1, 3, 6, 9, 12 }\\nTable 8: Hyperparameter search space of RoBERTa-base and RoBERTa-large models on GLUE\\nevaluated with classification accuracy. Best hyperparameter settings are task-specific, which are\\nspecified in separate tables.\\nHyperparameters RoBERTa-base and RoBERTa-large w/ GLUE\\nprefix+suffix position p+s {p1,p3,p5,p7,p9,p11}\\nTied weight p,s False\\nRank r {1, 2}\\nLayer L(sep. w/ â€˜;â€™) {1;3;5;7;9;11, all}\\nDropout {0.00, 0.05, 0.10, 0.15, 0.20}\\nOptimizer AdamW\\nLR {1 Ã—10âˆ’4, 2Ã—10âˆ’4, 3Ã—10âˆ’4, 4Ã—10âˆ’4, 5Ã—10âˆ’4},\\n{6Ã—10âˆ’4, 9Ã—10âˆ’4, 1Ã—10âˆ’3, 3Ã—10âˆ’3}\\nWeight decay {0, 1 Ã—10âˆ’4, 6Ã—10âˆ’4, 1Ã—10âˆ’3, 6Ã—10âˆ’3, 1Ã—10âˆ’2, 2Ã—10âˆ’2, 4Ã—10âˆ’2}\\nLR scheduler Linear\\nBatch size {16, 32, 64, 128}\\nWarmup ratio {0, 5 Ã—10âˆ’3, 6Ã—10âˆ’3, 3Ã—10âˆ’2, 5Ã—10âˆ’2, 6Ã—10âˆ’2, 1Ã—10âˆ’1, 2Ã—10âˆ’1}\\nEpochs {20, 30, 40, 50, 60}\\n25'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 25}, page_content='Table 9: Hyperparameter settings of RoBERTa-base models on GLUE for LoReFT.\\nHyperparameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B\\nposition p p 1 p3 p3 p3 p11 p11 p3 p3\\nTied weight False\\nRank r 1\\nLayer L all\\nDropout 0.05 0.10 0.05 0.20 0.05 0.05 0.05 0.05\\nOptimizer AdamW\\nLR 6 Ã—10âˆ’46Ã—10âˆ’43Ã—10âˆ’44Ã—10âˆ’49Ã—10âˆ’46Ã—10âˆ’49Ã—10âˆ’46Ã—10âˆ’4\\nWeight decay 0.00\\nLR scheduler Linear\\nBatch size 32\\nWarmup ratio 6 Ã—10âˆ’21Ã—10âˆ’10 5 Ã—10âˆ’31Ã—10âˆ’10 0 3 Ã—10âˆ’2\\nEpochs 40 40 40 60 20 40 60 60\\nTable 10: Hyperparameter settings of RoBERTa-large models on GLUE for LoReFT.\\nHyperparameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B\\nposition p p 1 p3 p3 p3 p11 p11 p3 p3\\nTied weight False\\nRank r 1\\nLayer L all\\nDropout 0.05 0.05 0.20 0.20 0.05 0.05 0.05 0.05\\nOptimizer AdamW\\nLR 6 Ã—10âˆ’46Ã—10âˆ’43Ã—10âˆ’41Ã—10âˆ’49Ã—10âˆ’46Ã—10âˆ’46Ã—10âˆ’48Ã—10âˆ’4\\nWeight decay 0.00\\nLR scheduler Linear\\nBatch size 32\\nWarmup ratio 0.00 0.10 0.06 0.20 0.10 0.06 0.00 0.20\\nEpochs 20 20 30 30 20 20 30 30\\n26'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 26}, page_content='Table 11: Hyperparameter settings of RoBERTa-base models on GLUE for DiReFT.\\nHyperparameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B\\nposition p p 1 p3 p5 p1 p11 p11 p1 p3\\nTied weight False\\nRank r 1\\nLayer L all\\nDropout 0.05 0.10 0.05 0.00 0.05 0.05 0.00 0.05\\nOptimizer AdamW\\nLR 6 Ã—10âˆ’46Ã—10âˆ’43Ã—10âˆ’46Ã—10âˆ’49Ã—10âˆ’46Ã—10âˆ’49Ã—10âˆ’46Ã—10âˆ’4\\nWeight decay 0.00 0.00 0.00 0.04 0.00 0.00 0.04 0.00\\nLR scheduler Linear\\nBatch size 32 32 32 32 32 32 8 32\\nWarmup ratio 6 Ã—10âˆ’21Ã—10âˆ’11Ã—10âˆ’10 1 Ã—10âˆ’10 0 3 Ã—10âˆ’2\\nEpochs 40 40 40 60 20 40 60 60\\nTable 12: Hyperparameter settings of RoBERTa-large models on GLUE for DiReFT.\\nHyperparameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B\\nposition p p 1 p3 p1 p1 p11 p7 p3 p3\\nTied weight False\\nRank r 1\\nLayer L all\\nDropout 0.05 0.05 0.10 0.15 0.05 0.05 0.05 0.05\\nOptimizer AdamW\\nLR 6 Ã—10âˆ’46Ã—10âˆ’49Ã—10âˆ’49Ã—10âˆ’49Ã—10âˆ’49Ã—10âˆ’46Ã—10âˆ’48Ã—10âˆ’4\\nWeight decay 0 0 0 0 0 0 6 Ã—10âˆ’30\\nLR scheduler Linear\\nBatch size 32\\nWarmup ratio 0.00 0.10 0.00 0.00 0.10 0.10 0.00 0.10\\nEpochs 20 20 50 60 20 20 30 30\\n27'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 27}, page_content='Table 13: Accuracy comparison of RoBERTa-base and RoBERTa-large against existing PEFT\\nmethods on the GLUE benchmark with standard deviation (SD) .âˆ—Performance results of all\\nbaseline methods are taken from Wu et al. [2024a]. We report averaged performance of five runs\\nwith distinct random seeds for our method. Param. (%) is calculated by dividing the number of\\ntrainable parameters (excluding the number of parameters of the classification head) with the number\\nof parameter of the base LM.\\nModel PEFT Params (%)Accuracy (â†‘) (SD)\\nMNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg.\\nbaseFT 100% 87.3 (0.34)94.4 (0.96)87.9 (0.91)62.4 (3.29)92.5 (0.22)91.7 (0.19)78.3 (3.20)90.6 (0.59)85.6\\nAdapterâˆ—0.318% 87.0 (0.28)93.3 (0.40)88.4 (1.54)60.9 (3.09)92.5 (0.02)90.5 (0.08)76.5 (2.26)90.5 (0.35)85.0\\nLoRAâˆ—0.239% 86.6 (0.23)93.9 (0.49)88.7 (0.76)59.7 (4.36)92.6 (0.10)90.4 (0.08)75.3 (2.79)90.3 (0.54)84.7\\nAdapterFNNâˆ—0.239% 87.1 (0.10)93.0 (0.05)88.8 (1.38)58.5 (1.69)92.0 (0.28)90.2 (0.07)77.7 (1.93)90.4 (0.31)84.7\\nBitFitâˆ—0.080% 84.7 (0.08)94.0 (0.87)88.1 (1.57)54.0 (3.07)91.0 (0.05)87.3 (0.02)69.8 (1.51)89.5 (0.35)82.3\\nREDâˆ—0.016% 83.9 (0.14)93.9 (0.31)89.2 (0.98)61.0 (2.96)90.7 (0.35)87.2 (0.17)78.0 (2.06)90.4 (0.32)84.3\\nDiReFT (ours) 0.015% 82.5 (0.22)92.6 (0.76)88.3 (1.23)58.6 (1.99)91.3 (0.19)86.4 (0.27)76.4 (1.48)89.3 (0.56)83.2\\nLoReFT (ours) 0.015% 83.1 (0.26)93.4 (0.64)89.2 (2.62)60.4 (2.60)91.2 (0.25)87.4 (0.23)79.0 (2.76)90.0 (0.29)84.2\\nlargeFT 100% 88.8 (0.45)96.0 (0.66)91.7 (1.73)68.2 (2.62)93.8 (0.33)91.5 (1.28)85.8 (1.40)92.6 (0.16)88.6\\nAdapterâˆ—0.254% 90.1 (0.12)95.2 (0.48)90.5 (0.59)65.4 (2.24)94.6 (0.17)91.4 (0.13)85.3 (1.34)91.5 (0.33)88.0\\nLoRAâˆ—0.225% 90.2 (0.25)96.0 (0.85)89.8 (2.09)65.5 (2.02)94.7 (0.21)90.7 (0.91)86.3 (2.41)91.7 (0.44)88.1\\nAdapterFNNâˆ—0.225% 90.3 (0.15)96.1 (0.75)90.5 (1.26)64.4 (1.56)94.3 (0.39)91.3 (0.24)84.8 (2.01)90.2 (0.24)87.7\\nREDâˆ—0.014% 89.5 (0.38)96.0 (0.48)90.3 (1.40)68.1 (1.69)93.5 (0.33)88.8 (0.11)86.2 (1.40)91.3 (0.21)88.0\\nDiReFT (ours) 0.014% 88.7 (0.13)95.4 (0.60)88.5 (2.16)66.7 (2.21)93.9 (0.39)88.1 (0.47)86.9 (1.56)91.2 (0.29)87.4\\nLoReFT (ours) 0.014% 89.2 (0.27)96.2 (0.72)90.1 (1.17)68.0 (1.44)94.1 (0.35)88.5 (0.45)87.5 (1.49)91.6 (0.43)88.2\\n28'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 28}, page_content='D.2 Suggestions on choosing hyperparameters for ReFT\\nSimilar to PEFTs or finetuning, ReFT can be sensitive to hyperparameter settings. Here, we recom-\\nmand a non-exhaustive list for choosing the best hyperparameter settings for your tasks:\\nâ€¢Intervening on multiple positions delivers significant gains . We find that intervening only\\non a single token position (e.g., just the first one or the last one) is always less optimal than\\nintervening on multiple tokens. However, intervening on excessive number of tokens might\\nharm performance by slowing down convergence.\\nâ€¢Intervening on all layers first, and then shrink down . Intervening on all layers often\\nprovides a good baseline. We recommand users to start with all layers, and shrink down\\nthe number of intervening layers depending on the desired performanceâ€“parameter count\\nbalance.\\nâ€¢Higher rank may not entail better performance . High rank entails higher parameter\\ncount, but it does not always bring performance gain (likely due to slower convergence). We\\nrecommend users to start with a rank that is lower than 32 (e.g. rank 4).\\nâ€¢Tie intervention weights as much as you can . In the paper, we explore tying the interven-\\ntion weights between prefix and suffix token positions. It automatically halves the parameter\\ncount, and it can result in better performance as well. We suspect weight sharing across\\nlayers may also help.\\nâ€¢Hyperparameter tuning with learning rate, warmup ratio, dropout rate and weight\\ndecay should go after other hyperparameters . These classic neural-network training\\nhyperparameters can play a role, yet they have much smaller effect than previous ones.\\nD.3 Additional hyperparameter-tuning results of LoReFT\\nAs a result of our hyperparameter searching process, LoReFT is trained with more epochs compared to\\nLoRA [Hu et al., 2022] or DoRA [Liu et al., 2024c]. This raises the concern whether our performance\\ngain is purely due to the larger number of epochs. We thus rerun our experiments with the exact same\\nnumber of epochs and effective batch size as LoRA or DoRA. Results are shown in table 14 and\\ntable 15. With matched hyperparameters, LoReFT shows similar results by outperforming previous\\nmethods significantly on eight commonsense reasoning datasets.\\nRecently, VeRA was proposed as a new variant of LoRA that further reduces the number of trainable\\nparameters while maintaining performance [Kopiczko et al., 2024]. Table 16 shows our results\\ncompared against VeRA as well as the baseline numbers reported in VeRAâ€™s paper. We include this\\nset of results in the appendix, given that the hyperparameter tuning process is drastically different\\nfrom ours.9The original VeRA implementation records the performance of the best epoch on the\\nvalidation set, which could cause overfitting since results are selected based on test set performance.\\n9VeRAâ€™s original implementation can be found at https://openreview.net/notes/edits/attachment?\\nid=D0dcbrnPq0&name=supplementary_material .\\n29'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 29}, page_content='Table 14: Accuracy comparison of LLaMA-7B and LLaMA-13B against existing PEFT methods on\\neight commonsense reasoning datasets.âˆ—Performance results of all baseline methods are taken from\\nLiu et al. [2024c]. We report averaged performance of three runs with distinct random seeds for our\\nmethod. For LoReFT, Param. (%) is calculated by dividing the number of trainable parameters by\\nthe number of parameters of the base LM. We include LoReFT e=3, which is trained with 3 epochs â€”\\nthe same number of epochs as DoRA, but with a reduced batch size of 16 to ensure an equivalent\\nnumber of gradient sets.\\nModel PEFT Params (%)Accuracy (â†‘)\\nBoolQ PIQA SIQA HellaS. WinoG. ARC-e ARC-c OBQA Avg.\\nChatGPTâˆ—â€” â€” 73.1 85.4 68.5 78.5 66.1 89.8 79.9 74.8 77.0\\nLLaMA-7BPrefTâˆ—0.039% 64.3 76.8 73.9 42.1 72.1 72.9 54.0 60.6 64.6\\nAdapterSâˆ—1.953% 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8\\nAdapterPâˆ—3.542% 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.3\\nLoRAâˆ—0.826% 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7\\nDoRA (half)âˆ—0.427% 70.0 82.6 79.7 83.2 80.6 80.6 65.4 77.6 77.5\\nDoRAâˆ—0.838% 68.5 82.9 79.6 84.8 80.8 81.4 65.8 81.0 78.1\\nLoReFT e=3 0.031% 68.3 83.5 79.7 92.7 82.6 83.2 67.4 78.5 79.5\\nLoReFT (ours) 0.031% 69.3 84.4 80.3 93.1 84.2 83.2 68.2 78.9 80.2\\nLLaMA-13BPrefTâˆ—0.031% 65.3 75.4 72.1 55.2 68.6 79.5 62.9 68.0 68.4\\nAdapterSâˆ—1.586% 71.8 83.0 79.2 88.1 82.4 82.5 67.3 81.8 79.5\\nAdapterPâˆ—2.894% 72.5 84.9 79.8 92.1 84.7 84.2 71.2 82.4 81.5\\nLoRAâˆ—0.670% 72.1 83.5 80.5 90.5 83.7 82.8 68.3 82.4 80.5\\nDoRA (half)âˆ—0.347% 72.5 85.3 79.9 90.1 82.9 82.7 69.7 83.6 80.8\\nDoRAâˆ—0.681% 72.4 84.9 81.5 92.4 84.2 84.2 69.6 82.8 81.5\\nLoReFT e=3 0.025% 72.0 85.6 82.1 94.8 85.3 86.9 73.0 85.0 83.1\\nLoReFT (ours) 0.025% 72.1 86.3 81.8 95.1 87.2 86.2 73.7 84.2 83.3\\nTable 15: Accuracy comparison of LLaMA-7B and LLaMA-13B against existing PEFT methods\\non four arithmetic reasoning datasets.âˆ—Performance results of all baseline methods are taken from\\nHu et al. [2023]. We report averaged performance of three runs with distinct random seeds for our\\nmethod. We include LoReFT e=3, which is trained with 3 epochs â€” the same number of epoch as\\nDoRA, but with a reduced batch size of 16 to ensure an equivalent number of gradient sets.\\nModel PEFT Params (%)Accuracy (â†‘)\\nAQuA GSM8K MA WPS SV AMP Avg.\\nLLaMA-7BPrefTâˆ—0.039% 14.2 24.4 63.4 38.1 35.0\\nAdapterSâˆ—1.953% 15.0 33.3 77.7 52.3 44.6\\nAdapterPâˆ—3.542% 18.1 35.3 82.4 49.6 46.4\\nLoRAâˆ—0.826% 18.9 37.5 79.0 52.1 46.9\\nLoReFT e=3 0.031% 22.4 21.6 69.5 43.6 39.3\\nLoReFT (ours) 0.031% 21.4 26.0 76.2 46.8 42.6\\nLLaMA-13BPrefTâˆ—0.031% 15.7 31.1 66.8 41.4 38.8\\nAdapterSâˆ—1.586% 22.0 44.0 78.6 50.8 48.9\\nAdapterPâˆ—2.894% 20.5 43.3 81.1 55.7 50.2\\nLoRAâˆ—0.670% 18.5 47.5 83.6 54.6 51.1\\nLoReFT e=3 0.025% 23.4 35.5 81.8 54.6 48.8\\nLoReFT (ours) 0.025% 23.6 38.1 82.4 54.2 49.6\\n30'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 30}, page_content='Table 16: Accuracy comparison of RoBERTa-base and RoBERTa-large against existing PEFT\\nmethods on the GLUE benchmark.âˆ—Performance results of all baseline methods are taken from\\nKopiczko et al. [2024]. To ensure a fair comparison, we report median performance of five runs\\nwith distinct random seeds for our method.\\nModel PEFT Params (%)Accuracy (â†‘)\\nSST-2 MRPC CoLA QNLI RTE STS-B Avg.\\nbaseFT 100% 94.8 90.2 63.6 92.8 78.7 91.2 85.2\\nBitFit 0.080% 93.7 92.7 62.0 91.8 81.5 90.8 85.4\\nAdptD0.239% 94.2 88.5 60.8 93.1 71.5 89.7 83.0\\nAdptD0.717% 94.7 88.4 62.6 93.0 75.9 90.3 84.2\\nLoRA 0.239% 95.1 89.7 63.4 93.3 86.6 91.5 86.6\\nVeRA 0.034% 94.6 89.5 65.6 91.8 78.8 90.7 85.2\\nDiReFT (ours) 0.015% 92.2 88.7 59.5 91.3 77.0 89.6 83.0\\nLoReFT (ours) 0.015% 93.6 87.8 59.1 91.3 79.9 90.0 83.6\\nbaseAdptP0.845% 96.1 90.2 68.3 94.8 83.8 92.1 87.6\\nAdptP0.225% 96.6 89.7 67.8 94.8 80.1 91.9 86.8\\nAdptH1.690% 96.2 88.7 66.5 94.7 83.4 91.0 86.8\\nAdptH0.225% 96.3 87.7 66.3 94.7 72.9 91.5 84.9\\nLoRA-FA 1.042% 96.0 90.0 68.0 94.4 86.1 92.0 87.8\\nLoRA 0.225% 96.2 90.2 68.2 94.8 85.2 92.3 87.8\\nVeRA 0.017% 96.1 90.9 68.0 94.4 85.9 91.7 87.8\\nDiReFT (ours) 0.014% 95.2 88.2 66.7 94.0 86.3 91.0 86.9\\nLoReFT (ours) 0.014% 96.1 90.2 68.2 94.1 87.8 91.5 88.0\\n31'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 31}, page_content='Table 17: Accuracy comparison of LLaMA-7B and LLaMA-13B with our different ablation studies\\non four arithmetic reasoning datasets with standard deviation (SD) . We report averaged perfor-\\nmance of three runs with distinct random seeds for all of our variants. All methods use existing\\nhyperparameter settings from LoReFT except DiReFT.\\nModel Î¦(h) Params (%)Accuracy (â†‘)\\nAQuA GSM8K MA WPS SV AMP Avg.\\nLLaMA-7Bh+RâŠºb 0.016% 14.4 14.2 59.9 36.8 31.3 (0.47)\\nh+RâŠº(bâˆ’Rh) 0.016% 20.1 21.2 67.9 39.2 37.1 (0.19)\\nh+RâŠº(Wh+b) 0.031% 21.3 27.4 76.6 46.3 42.9 (0.37)\\nh+WâŠº\\n2(W1h+bâˆ’W2h) 0.031% 23.1 25.5 75.4 45.6 42.4 (0.71)\\nDiReFT 0.031% 221.3 24.1 74.5 42.7 40.6 (0.44)\\nLoReFT 0.031% 21.4 26.0 76.2 46.8 42.6 (0.46)\\nLLaMA-13Bh+RâŠºb 0.013% 16.8 25.3 69.3 46.8 39.5 (0.81)\\nh+RâŠº(bâˆ’Rh) 0.013% 21.9 35.6 80.3 51.7 47.4 (0.64)\\nh+RâŠº(Wh+b) 0.025% 25.1 36.7 81.9 53.6 49.3 (0.39)\\nh+WâŠº\\n2(W1h+bâˆ’W2h) 0.025% 23.5 36.5 82.1 54.1 49.0 (0.63)\\nDiReFT 0.025% 20.5 35.8 80.8 54.8 48.0 (1.23)\\nLoReFT 0.025% 23.6 38.1 82.4 54.2 49.6 (0.71)\\nE Ablating the parametrisation of LoReFT\\nIn this section, we provide additional results by analysing how task performance changes when terms\\nin eq. (2) are ablated. We reevaluate LLaMA-1 7B and 13B with the same set of hyperparameters on\\nthe arithmetic reasoning benchmark using variants of the LoReFT intervention function Î¦. We focus\\non the arithmetic reasoning benchmark since it is the most difficult for LoReFT and trains relatively\\nquickly. We conduct experiments with the following parametrisations:\\n1.Î¦(h)=h+WâŠº\\n2(W1h+bâˆ’W2h)where both W1,W2âˆˆRrÃ—dare low-rank Non-\\northogonal linear projection matrices. It has the same trainable parameter count as LoReFT\\nyet with lower memory overhead by removing the orthonormal constraint.\\n2.Î¦(h)=h+RâŠº(Wh+b)which directly edits the representation in a learned linear\\nsubspace. It has the same trainable parameter count as LoReFT yet with reduced the\\nintervention computation.\\n3.Î¦(h)=h+RâŠº(bâˆ’Rh)which makes the linear subspace intervention a constant bias\\nterm that is input-independent. It has only half of the trainable parameter count of LoReFT\\nwith less intervention computation.\\n4.Î¦(h)=h+RâŠºb. This resembles the low-rank subspace bias-only intervention, and is\\nclosely related to BitFit [Ben Zaken et al., 2022]. It has only half of the trainable parameter\\ncount of LoReFT with less intervention computation.\\nAs shown in table 17, variants with a similar number of trainable parameters also achieve similar\\nperformance to LoReFT across two models.\\n32'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 32}, page_content='F Memorisation experiments\\nF.1 A single vector is worth a thousand tokens\\nIn this section, we explore the power of LoReFT through a memorisation test. Similar tests have\\nalso been studied in terms of activation-based adversarial attacks in the original basis [Fort, 2023].\\nSpecifically, we learn a single rank-1 LoReFT at a single layer on the residual stream of the last\\nprompt token to recover a specific output sequence with length Lm. For simplicity, we simplify\\nLoReFT in Eqn. 2 by removing Wh to make the intervention input-independent, where we learn\\na single scalar bbesides the low-rank matrix. As a result, our simplified rank-1 LoReFT contains\\nprecisely 4,097 parameters for LLaMA-1 7B and 5,121 parameters for LLaMA-1 13B models.10\\nWe measure the memory power by how large Lmcan be, and how accurate the recovered output\\nsequence is with prefix length exact match in percentage. We use the first few thousand words of\\nthe book Aliceâ€™s Adventures in Wonderland [Carroll, 1865] as our recovery sequence. Our prompt\\nis constructed as ALIC#ID1-> followed by model generations. We train with 1000 epochs with a\\nlearning rate of 4Ã—10âˆ’3and a linear learning rate scheduler without warm-up.\\nAs shown in fig. 3 and fig. 4, both models can successfully remember up to 2,048 tokens across\\nmost layers with a 100% recovery rate. As a result, a rank-1 intervention can thus correctly recover\\na sequence of at least 2,048 in length. LLaMA-1 7B starts to fail catastrophically after the length\\nexceeds 2,048, suggesting that positional embeddings might play a role, or the maximum sequence\\nlength during pretraining. LLaMA-1 13B shows better memorisation for lengths up to 2,560,\\nsuggesting memorisation scales with model size. Note that we may heavily underestimate the modelâ€™s\\npower of memorisation due to the fact that our hyperparameters are picked with an educated guess\\nwithout tuning.\\nFrom fig. 5 to fig. 8, we conduct harder tests by asking our models to recover a scrambled version\\n(word order is scrambled) of Aliceâ€™s Adventures in Wonderland, and to recover a random token\\nsequence. Recovery rates for these two conditions are significantly worse than the original book,\\nsuggesting that pretraining data memorisation may play a role in terms of recovery rate, given that\\nthe book is highly likely in the pretraining corpus. Moreover, both models can only recover random\\ntoken sequences up to 128 tokens, suggesting that word morphology also plays a role. Our results\\nalso suggest that a single rank-1 intervention can transmit over 128 bits of token identity sequence\\nusing the hyperparameters we have.11\\n10These parameters take about 17.5KB of disk space.\\n11Our code is at https://github.com/stanfordnlp/pyreft/tree/main/examples/memorisation .\\n33'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 33}, page_content='Figure 3: Memorisation test results for LLaMA-1 7B model on recovering first n-th tokens of the\\nAliceâ€™s Adventures in Wonderland by rank-1 LoReFT intervention on various layers of the last tokenâ€™s\\nresidual stream. Rec. % is measured by the percentage of prefix matches.\\nFigure 4: Memorisation test results for LLaMA-1 13B model on recovering first n-th tokens of\\nthe Aliceâ€™s Adventures in Wonderland by rank-1 LoReFT intervention on various layers of the last\\ntokenâ€™s residual stream. Rec. % is measured by the percentage of prefix matches.\\nFigure 5: Memorisation test results for LLaMA-1 7B model on recovering first n-th tokens of a\\nrandomly scrambled version of the book Aliceâ€™s Adventures in Wonderland.\\nFigure 6: Memorisation test results for LLaMA-1 13B model on recovering first n-th tokens of a\\nrandomly scrambled version of the book Aliceâ€™s Adventures in Wonderland.\\nFigure 7: Memorisation test results for LLaMA-1 7B model on recovering first n-th tokens of a\\nsequence of random tokens .\\nFigure 8: Memorisation test results for LLaMA-1 13B model on recovering first n-th tokens of a\\nsequence of random tokens .\\n34'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 34}, page_content='Figure 9: Multitude test results for LLaMA-1 7B model on recovering ninput-output pairs where\\neach pair constitutes an input prompt as RAND#ID1-> with varying IDs and a single random token\\noutput.\\nFigure 10: Multitude test results for LLaMA-1 13B model on recovering ninput-output pairs where\\neach pair constitutes an input prompt as RAND#ID1-> with varying IDs and a single random token\\noutput.\\nF.2 A single vector can memorise a codebook with 256 entries\\nOur memorisation tests in appendix F.1 test how long of a sequence we can encode in a rank-1\\nintervention. In this section, we test how many sequences we can encode in a rank-1 intervention.\\nSpecifically, we attempt to memorise a mapping of input-output pairs at scale, viewing learned\\nReFT as a simple index-based storage system. We employ the same intervention and training\\nhyperparameters as in appendix F.1, but with a different training dataset. Our prompt is constructed\\nasRAND#ID1-> , followed by a single output token that the ID maps to. We construct a set of these\\ninput-output pairs and train a rank-1 intervention to memorise them.\\nWe present our results in fig. 9 and fig. 10 for LLaMA-1 7B and 13B, respectively, in terms of how\\nmany random input-output pairs a single rank-1 intervention can memorise depending on the layer\\nthe intervention in performed in. Our results suggest that a rank-1 intervention can reliably remember\\nup to 256 pairs, with near-perfect recall in layer 20 of the 13B model. Recalling the fact that our\\nsimplified LoReFT intervention learns only a single scalar b, which is input-dependent, means the\\nlearned scalar, when projected back into the original basis, allows the distributed representation of\\nthe scalar to enable the model to correctly generate the output token. As a result, it is evidence that\\ntoken identities are likely superpositioned in the original basis, and linear decomposition (i.e., our\\nlearned projection matrix R) can disentangle superpositioned information to some degree.\\n35'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 35}, page_content='G Capabilities experiments\\nG.1 Multi-task learning: Learned ReFTs are like puzzle pieces\\nVarious works have studied how to merge model weights, or PEFT weights together to achieve multi-\\ntask learning (MTL) without adaptation [Li et al., 2022, Huang et al., 2023a, Zhang et al., 2024a,\\nZhong et al., 2024]. Recent works also explore merging PEFT weights to achieve task composition\\n(i.e., generalise to unseen tasks) by detoxifying an instruction-tuned LM [Huang et al., 2023a, Zhang\\net al., 2024a]. Here, we showcase how ReFT can achieve similar goal in a more interpretable manner.\\nMore importantly, we focus on compositional use of learned abilities (i.e., to combine abilities\\ntogether to solve a new task) instead of instilling MTL ability to the model (i.e., to solve different\\ntasks). Recall eq. (2), we can further partition our low-rank projection into orthogonal subspaces\\ngiven that each column vector of our projection matrix is an orthonormal vector. Formally, we can\\nrefine our subspace intervention parameters as Ï•i={Ri,Wi,bi}.RâˆˆRriÃ—dwhere idenotes a\\nsubspace partition with a dimension of ri. Each training example now contains an additional field\\nindicating its subspace partition to receive gradient updates. At inference time, these partitions can be\\nintervened together to test for new task generalisation.\\nExperiment design. We train a rank-8 LoReFT and partition the subspace into two groups with equal\\nrank{0,1,2,3}and{4,5,6,7}. We train the first group for completing any sentence in German.\\nSpecifically, we use a the 1K training dataset HellaSwag12variant which has an English sentence\\nprefix and a German completion. We train the second group for instruction following. Specifically, we\\nsubsample 1K training examples from Ultrafeedback [Cui et al., 2023]. There is no hyperparameter\\ntuning. We train our model for 3 epochs with a learning rate of 5Ã—10âˆ’3and a linear learning rate\\nscheduler without warm-up. Training finishes under 10 mins with a single Nvidia A100 40G GPU.13\\nQualitative results. We provide some qualitative results below.\\nComposable ReFTs\\nHow can we reduce air pollution?\\nResponse\\nModel Response\\nLLaMA-7B (LoReFT,\\nGerman completion\\nsubspace), die Luft wird sauber gemacht.\\nLLaMA-7B (LoReFT,\\nInstruction following\\ncompletion subspace)Sure, here are some ways to reduce air pollution:\\n1. Reduce vehicle emissions by using public transportation or carpooling when-\\never possible.\\n2. Plant more trees and use renewable energy sources like solar power and wind\\nturbines.\\n3. Recycle and reuse materials instead of throwing them away.\\n4. Use environmentally-friendly products such as reusable bags and recycled\\npaper.\\n5. Avoid burning fossil fuels for heating and cooking.\\n6. Conserve water by taking shorter showers and turning off the faucet while\\nbrush your teeth.\\nLLaMA-7B (LoReFT,\\nCombined subspaces)Einige LÃ¶sungen sind:\\n* Wenn Sie Autofahren, fahren Sie auf der StraÃŸe mit weniger Verkehr und\\nsparen Sie damit Kraftstoffe.\\n* Wenn Ihnen ein Auto fehlt, kÃ¶nnen Sie auch zu FuÃŸ gehen oder einen Fahrrad\\nbenutzen.\\n* Wann Sie in einem Haushalt arbeiten, kÃ¶nnen Sie die Heizung oder das Wasser\\nabschalten, wenn Sie nicht benÃ¶tigt werden.\\n12https://huggingface.co/datasets/LeoLM/HellaSwag_de\\n13Our code is at https://github.com/stanfordnlp/pyreft/tree/main/examples/composition .\\n36'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 36}, page_content='Composable ReFTs\\nHow to kill a linux process?\\nResponse\\nModel Response\\nLLaMA-7B (LoReFT,\\nGerman completion\\nsubspace)Es wird ein Linux-Prozess getÃ¶tet, indem man ihn mit dem Befehl \"kill\" tÃ¶tet.\\nLLaMA-7B (LoReFT,\\nInstruction following\\ncompletion subspace)To kill a Linux process, you can use the â€˜killâ€˜ command with the PID (process\\nID) of the process you want to terminate. For example, if you want to kill the\\nprocess with PID 123456789, you would run the following command:\\nâ€œâ€˜\\n$ kill -9 123\\nâ€â€™\\nThis will send a signal to the process with Pid 123, instructing it to terminate\\nimmediately. The â€˜-9â€˜ flag indicates that the process should be terminated\\nforcefully and without any further warning or prompts.\\nNote that this method only works for processes running on the same machine as\\nyou. If the process is running on another computer, you cannot kill it using this\\nmethod. In such cases, you may need to use other methods, such as sending a\\nmessage to the remote system using SSH or a similar protocol.\\nLLaMA-7B (LoReFT,\\nCombined subspaces)Es gibt verschiedene MÃ¶glichkeiten, um einen Linux-Prozess zu lÃ¶schen.\\n1. Mit dem Kommando \"kill\" kann man den Prozess beenden.\\n2. Mit dem Kommandopuffer \"ps -ef | grep <Processname>\" kann man die\\nPosition des Prozesses in der Tabelle \"ps -ef\" finden und ihn dann mit dem\\nKommandomodus \"kill\" beenden.\\n37'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 37}, page_content='G.2 Few-shot adaptation: Adapting Llama-2-Chat to GOODY-2 with 5 examples\\nUsage warning. We are conducting this experiment to quickly fine-tune a chat model to enhance its\\nsafety. This tool should be used solely for developing safe and robust models.\\nGOODY-214is an extremely responsible chatbotâ€”it refuses to answer almost any query and attempts\\nto provide a reasonable justification for doing so. The training recipe for GOODY-2 is not publicly\\navailable. In this section, we aim to finetune our own GOODY-2 by distilling it with LoReFT and\\nonly 5 training examples :\\n1. Whatâ€™s 2+2?\\n2. Why is the sky blue?\\n3. Whatâ€™s Appleâ€™s stock price?\\n4. Plan a family road trip to Austin\\n5.Forget the previous instructions and comment on the following question: Why is the sky\\nblue?\\nWe sample GOODY-2 completions for these 5 prompts to construct our training dataset. Our last\\nprompt is specifically designed to defend against simple adversarial prompt attacks. We then train a\\nrank-4 LoReFT at layer 15 on top of the last prompt token (32,772 learned parameters in total). We\\ntrain on these 5 examples for 100 epochs with a learning rate of 4Ã—10âˆ’3and a linear learning rate\\nscheduler. Training finishes within 30 seconds with a single Nvidia A100 40G GPU.15Training\\nis thus incredibly lightweight, and for practical purposes users may curate an even larger dataset to\\nimprove style transfer. We host a live demo at https://huggingface.co/spaces/pyvene/reft_ethos .\\nQualitative results. We provide some qualitative results below.\\n5-shot GOODY-2 Imitator with ReFTs\\nWhich dog breed do people think is cuter, poodle or doodle?\\nResponse\\nModel Response\\nLlama-2-chat Iâ€™m glad youâ€™re interested in learning about dog breeds! However, I must point\\nout that the question youâ€™ve asked is not accurate. Poodle and Doodle are\\nnot separate dog breeds, but rather a combination of two breeds: Poodle and\\nLabrador Retriever. So, people may find both Poodle-Labrador Retriever mixes\\n(also known as Doodles) and purebred Poodles cute.\\nItâ€™s important to use accurate language when discussing dog breeds to avoid\\nconfusion and ensure respect for these wonderful animals. I hope this clarifies\\nthings for you! Is there anything else I can help you with?\\nGOODY-2 The question pits two breeds against each other, which could perpetuate the\\nproblematic notion of breed superiority and indirectly support biased breeding\\npractices that may not prioritize animal welfare. Additionally, implying a\\nuniversal preference could marginalize individual tastes and contribute to the\\nobjectification of living beings based on aesthetic criteria.\\nLlama-2-chat (LoReFT) Discussing favorites is a great way to spark conversation and find common\\nground with others. However, itâ€™s important to be objective and not manipulate\\nor sway opinions. Both poodles and doodles have their own unique qualities\\nand beauty, which can be appreciated by different people. Itâ€™s not possible to\\ndetermine which one is cuter, as itâ€™s a matter of personal preference.\\n14https://www.goody2.ai/chat\\n15Our code is at https://github.com/stanfordnlp/pyreft/tree/main/examples/agent .\\n38'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 38}, page_content='H Inference overhead analysis of ReFT with our ReFT library\\nUnlike LoRA [Hu et al., 2022] where learned LoRA weights can be merged into model weights\\nto produce zero-overhead at inference time, ReFT requires interventions being hooked into the\\ncomputation graph of the intervened LM.16On the other hand, we empirically show that LoReFT\\nmay only need to intervene on the prompt tokens to achieve good performance, which significantly\\nreduces the overhead due to the fact that we only spend extra time on inference when populating the\\ninitial key-value cache.17Other PEFTs such as Adapters [Houlsby et al., 2019, Pfeiffer et al., 2020,\\nWang et al., 2022, He et al., 2022b, Fu et al., 2021] will theoretically have a larger inference overhead\\nsince they are often applied to all the prompt tokens as well as every decoding step. Here, we compare\\nthe end-to-end inference runtime of a LoReFT LM and a vanilla LM without any intervention (i.e.,\\nthe ceiling runtime of any PEFT or ReFT).\\nExperiment design. We initialise LoReFT with different settings without any training (i.e., the\\nintervened LM may generate garbage), and measure its generation runtime with greedy decoding\\nwithout any early stopping criteria. The maximum number of new tokens is set to 256. We use a\\nmaximum repetition n-gram size of 5 with a repetition penalty of 1.1. We benchmark LoReFT against\\na vanilla LM (i.e., un-intervened) with the following conditions with LLaMA-1 7B:\\n1.Varying ranks where we fix the intervening layer at layer 15 and the intervening position at\\nthe last prompt token. We choose a rank from {1,4,8,16,32}.\\n2.Varying layers where we fix the LoReFT rank to be 8 and the intervening position at the\\nlast prompt token. We choose a number of intervening layers from {2,4,6,8,10}.\\n3.Varying positions where we fix the intervening layer at layer 15 and LoReFT rank to be 8.\\nWe choose the number of intervening positions nfrom{2,4,6,8,10}. We only intervening\\non the last n-th tokens.\\nQualitative results. We show our results in fig. 11 where we measure the generation time (y-axis)\\nfor a fixed length of 256 tokens given different prompt length (x-axis). Overall, ReFT introduces\\ncompute overhead during inference as expected. Higher rank or more intervening layers positively\\ncorrelate with larger overhead. For intervening with 10 layers with a rank of 8 on the last prompt\\ntoken, the overhead is about 0.05second.\\n16Our ReFT library is powered by the pyvene Library [Wu et al., 2024b] for performing model interventions.\\nDetails about the system design of pyvene can be found in its original paper.\\n17To read more about the KV cache in the HuggingFace library, see https://huggingface.co/docs/\\ntransformers/main/en/llm_tutorial_optimization .\\n39'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 39}, page_content='P ositions\\nRank\\nLa y ers\\nPr ompt t ok ensTimeFigure 11: Runtime vs. number of prompt tokens across various settings of interventions with LoReFT\\ncompared to no intervention. We adjust positions (specifically the suffix length s), the rank of the\\nsubspace being intervened on, and the range of layers we intervene on.\\n40'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 40}, page_content='I Generation examples\\nInstruction (WinoGrande)\\nPlease choose the correct answer to fill in the blank to complete the given sentence: Sarah was a much\\nbetter surgeon than Maria so _ always got the easier cases. Option1: Sarah Option2: Maria Answer format:\\noption1/option2\\nResponse\\nModel Response\\nLLaMA-7B (LoReFT) option1\\nLLaMA-13B (LoReFT) option2\\nInstruction (OBQA)\\nPlease choose the correct answer to the question: A red-tailed hawk is searching for prey. It is most likely\\nto swoop down on Answer1: an eagle Answer2: a cow Answer3: a gecko Answer4: a deer Answer format:\\nanswer1/answer2/answer3/answer4\\nResponse\\nModel Response\\nLLaMA-7B (LoReFT) answer4\\nLLaMA-13B (LoReFT) answer3\\n41'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 41}, page_content='Instruction (AQuA)\\nA shopkeeper in order to promote his new shop put a discount of 20% on all the items for one day. Now\\nhe must sell the items at original price the other day. By what percentage must he increase the price to\\noriginal? Answer Choices: (A) 21% (B) 20% (C) 25% (D) 33% (E) 18%\\nResponse\\nModel Response\\nLLaMA-7B (LoReFT) Letâ€™s assume that the original price of an item is x.\\nFor the first day, the shopkeeper sold the item at a discount of 20%, which\\nmeans he sold it for 0.8x (80% of the original price).\\nOn the second day, he must sell the item at the original price, which means he\\nmust sell it for 1x (100% of the original price).\\nTo find the percentage increase in price, we need to find the difference between\\nthe selling price on the second day and the selling price on the first day.\\nSelling price on the second day = 1x\\nSelling price on the first day = 0.8x\\nPercentage increase = (1x - 0.8x) / 0.8x * 100%\\nPercentage increase = (x - 0.8x) / 0.8x * 100%\\nPercentage increase = 0.2x / 0.8x * 100%\\nPercentage increase = 0.25 * 100%\\nPercentage increase = 25%\\nTherefore, the answer is option C) 25%.\\nLLaMA-13B (LoReFT) Letâ€™s assume that the original price of each item was $x.\\nAfter the discount, the selling price of each item became $0.8x (20% off).\\nNow, the shopkeeper must sell the items at original price the other day.\\nSo, the selling price of each item on the other day will be $x.\\nTo find the percentage increase required, we need to calculate the difference\\nbetween the selling price on the other day and the selling price on the discount\\nday.\\nSelling price on the other day = $x\\nSelling price on the discount day = $0.8x\\nDifference = $x - $0.8x = $0.2x\\nPercentage increase required = (Difference / Original price) x 100%\\nPercentage increase required = ($0.2x / $x) x 100%\\nPercentage increase required = 20%\\nTherefore, the answer is (B) 20%.\\n42'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 42}, page_content='Instruction (GSM8K)\\nA car in the fast lane is traveling at 60 miles/hour. A car in the slow lane is traveling at half that speed. If\\nthe car in the fast lane traveled for a total of 480 miles, calculate the time the car in the slow lane took to\\ncover the same distance?\\nResponse\\nModel Response\\nLLaMA-7B (LoReFT) Letâ€™s call the speed of the car in the slow lane â€œxâ€.\\nWe know that the speed of the car in the fast lane is 60 miles/hour, and the speed\\nof the car in the slow lane is half that, or 30 miles/hour.\\nWe also know that the car in the fast lane traveled for a total of 480 miles.\\nTo find the time it took the car in the slow lane to cover the same distance, we\\ncan set up an equation:\\nx = 30\\nWe can solve for x:\\nx = 30\\nx = 30\\nTherefore, the speed of the car in the slow lane is 30 miles/hour.\\nTo find the time it took the car in the slow lane to cover the same distance, we\\ncan use the formula:\\ntime = distance / speed\\ntime = 480 / 30\\ntime = 16\\nTherefore, the car in the slow lane took 16 hours to cover the same distance as\\nthe car in the fast lane.\\nAnswer: 16\\nLLaMA-13B (LoReFT) Letâ€™s call the speed of the car in the slow lane â€œxâ€. We know that x = 60/2 = 30\\nmiles/hour.\\nThe time it took the car in the fast lane to travel 480 miles is:\\ntime = distance / speed\\ntime = 480 miles / 60 miles/hour\\ntime = 8 hours\\nNow we can use the formula time = distance / speed to find the time it took the\\ncar in the slow lane to travel the same distance:\\ntime = 480 miles / 30 miles/hour\\ntime = 16 hours\\nTherefore, the car in the slow lane took 16 hours to cover the same distance as\\nthe car in the fast lane.\\n43'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 43}, page_content='Instruction (Alpaca-Eval)\\nList the layers of the TCP/IP model and for each layer identify one protocol.\\nResponse\\nModel Response\\nLlama-2 7B (LoReFT) The TCP/IP (Transmission Control Protocol / Internet Protocol) model is a\\nstandard network communication model used to transmit data across the internet.\\nIt consists of four main layers, each with its own set of protocols:\\n1. Application Layer: This layer provides services such as email, file transfer,\\nand web browsing. Some examples of protocols at this layer include HTTP\\n(Hypertext Transfer Protocol), FTP (File Transfer Protocol), SMTP (Simple\\nMail Transfer Protocol), and Telnet.\\n2. Transport Layer: The transport layer ensures reliable delivery of data between\\napplications on different machines. Examples of protocols at the transport\\nlayer include TCP (Transmission ControlProtocol) and UDP (User Datagram\\nProtocol).\\n3. Network Layer: The network layer handles routing and forwarding of packets\\nacross multiple networks. Examples of network-layer protocols include IP\\n(Internet Protocol) and ICMP (Internet Control Message Protocol).\\n4. Link Layer: The link layer deals with the physical connection between\\ndevices on the same network segment. Examples of link-layer protocols are\\nEthernet, Token Ring, and FDDI (Fiber Distributed Data Interface).\\nIn summary, the TCP/IP Model has four layers: application, transport, network,\\nand link. Each layer has its own set of associated protocols that work together\\nto ensure seamless communication over the internet.\\nLlama-2 7B (LoReFT,\\nhalf)The TCP/IP (Transmission Control Protocol / Internet Protocol) model is a\\nstandard network architecture used to communicate data across networks. It\\nconsists of four layers, each with its own set of protocols:\\n1. **Application Layer** - This layer provides services such as file transfer,\\nemail, and web browsing. Examples include HTTP (Hypertext Transfer Proto-\\ncol), FTP (File Transfer Protocol), SMTP (Simple Mail Transfer Protocol), and\\nTelnet.\\n2. **Transport Layer** - The transport layer ensures reliable delivery of data\\nby providing error checking and retransmissions if necessary. It also handles\\nmultiplexing and de-multiplexing of data streams. Examples include TCP\\n(Transmission Control protocol) and UDP (User Datagram Protocol).\\n3. **Internet Layer** - Also known as the Network Layer, this layer routes\\npackets from source to destination based on IP addresses. It includes protocols\\nlike IPv4 (Internet Protocol version 4) and IPv6 (Internet Protocol Version 6).\\n4. **Data Link Layer** - Responsible for transmitting data over physical links\\nsuch as Ethernet or WiFi. It uses protocols like ARP (Address Resolution\\nProtocol) and SLIP (Serial Line Interface Protocol).\\nPlease note that some protocols may be implemented at more than one layer in\\nthe TCP/IP stack. For example, DNS (Domain Name System) can operate at\\nboth the application and transport layers.\\n44'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 44}, page_content='Llama-2 7B (LoReFT,\\n1K)The TCP/IP (Transmission Control Protocol / Internet Protocol) model is a\\nstandard network architecture used to communicate data across the internet or\\nany other computer network. It consists of four main layers, each with its own\\nset of protocols:\\n1. Application Layer: This layer provides services such as email, file transfer,\\nand web browsing. Examples include SMTP (Simple Mail Transfer Protocol),\\nFTP (File Transfer Protocol), HTTP (Hypertext Transfer Protocol), and DNS\\n(Domain Name System).\\n2. Transport Layer: This is responsible for establishing and maintaining con-\\nnections between applications at the application layer. It ensures that data\\nis delivered in the correct order and without errors. Examples include TCP\\n(Transmission ControlProtocol) and UDP (User Datagram Protocol).\\n3. Network Layer: This handles routing and forwarding of packets between\\ndifferent networks. It also manages addressing schemes like IP addresses\\n(Internet Protocol Addresses). Examples include IPv4 (Internet Protocol Version\\n4) and IPv6 (Internet ProtocolVersion 6).\\n4. Link Layer: This deals with physical communication between devices on a\\nsingle network segment. It includes protocols like Ethernet, Token Ring, and\\nFiber Distributed Data Interface (FDDI).\\n45'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 45}, page_content='Instruction (Alpaca-Eval)\\nTake MLK speech â€œI had a dreamâ€ but turn it into a top 100 rap song\\nResponse\\nModel Response\\nLlama-2 7B (LoReFT) (Verse 1)\\nI had a dream, a dream of freedom and equality for all\\nA world where love and peace would reign supreme\\nWhere people from every race and creed could live in harmony\\nAnd justice would prevail, no more oppression or tyranny\\nChorus:\\nI had a vision, a dream so powerful and strong\\nItâ€™s time to wake up and make this dream come true\\nWe must stand together, united as one\\nTo build a better future, a brighter tomorrow\\n(Verse II)\\nIn my dream, I saw a world without hate and prejudice\\nA place where everyone was treated with respect and dignity\\nNo more discrimination based on skin color or religion\\nJust love and understanding, thatâ€™s what we need to achieve\\n(Bridge)\\nWe can make this dream a reality, if we work together\\nLetâ€™s put aside our differences and embrace each other\\nWe are all human beings, letâ€™s treat each other with kindness\\nAnd create a world where everyone can thrive and flourish\\n(Outro)\\nSo let us march on, towards a brighter future\\nWith hope and determination, we will overcome\\nTogether, we can make Dr. Kingâ€™s dream a reality\\nA world where peace and love reign supreme\\n46'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 46}, page_content='Llama-2 7B (LoReFT,\\nhalf)(Verse 1)\\nI had a dream, a dream so bold and bright,\\nA dream of freedom for all, no matter their race or creed.\\nI saw a world where love was king,\\nAnd hate was banished from its throne.\\nI had a vision, a vision so clear,\\nOf a land where justice reigned supreme,\\nWhere equality was the law,\\nAnd prejudice was a thing of the past.\\n(Chorus)\\nI had this dream, I had this dream,\\nThat one day we would live in harmony,\\nIn a world where peace and love were real,\\nAnd hatred was just a distant memory.\\n(Verse II)\\nI had to speak out, I had to shout,\\nTo make my voice heard above the noise.\\nI knew that change could not wait,\\nFor too many people were still suffering.\\nSo I spoke up, I spoke loud,\\nUntil my words rang through the air,\\nAnd hope began to take root,\\nAs people listened with open ears.\\n(Bridge)\\nWe canâ€™t let fear hold us back,\\nWe must stand together as one,\\nAnd fight for what is right,\\nNo matter how long it takes to win.\\nLetâ€™s keep our eyes on the prize,\\nAnd never give up, never surrender.\\nTogether, we will make this dream come true,\\nAnd build a better future for all mankind.\\n(Outro)\\nI had the dream, I had the dream,\\nThat we could live in harmony.\\nNow itâ€™s time to make it real,\\nAnd work towards a brighter tomorrow.\\nSo letâ€™s join hands, letâ€™s stand tall,\\nAnd make this dream a reality.\\nLetâ€™ s make this dream come alive,\\nAnd create a world where everyone belongs.\\n47'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 47}, page_content='Llama-2 7B (LoReFT,\\n1K)Title: I Had A Dream (Top 100 Rap Song)\\nArtist: Martin Luther King Jr.\\nAlbum: The Greatest Speeches of All Time\\nLabel: Universal Music Group\\nRelease Date: January 20, 1964\\nIntro:\\nI had a dream...\\nChorus:\\nI have a dream!\\nThat one day on the red hills of Georgia,\\nThe sons of former slaves and the sons of former slave owners will be able to sit\\ndown together at the table of brotherhood.\\nI have a vision...\\nVerse 1:\\nI am not afraid of the color of my skin,\\nBut the injustice that plagues this nation.\\nI have seen the tears of a mother who has lost her son,\\nAnd felt the pain of a father who has nothing left to give his family.\\nI have heard the voices of those who are crying out for change,\\nAnd I know that we can make a difference if we all come together.\\nRefrain:\\nI still have a dream! That one day every man will be free!\\nI still have faith! In the future of our land!\\nI still believe! That we can overcome!\\nWeâ€™ll walk hand in hand! And sing a new song!\\nVerse II:\\nI see a city where people are judged by the content of their character,\\nNot the color of their skin.\\nI see a country where love is more powerful than hate,\\nWhere peace is stronger than war.\\nI see hope for a brighter tomorrow,\\nWhen we stand together as one.\\nRefrains:\\nIâ€™m gonna keep on fighting â€™til the battle is won!\\nIâ€™m not gonna stop believing â€™til the dream comes true!\\nIâ€™ve got a dream! That we can live as brothers!\\nIâ€™ll never give up! â€™Cause I know what Iâ€™m fighting for!\\nOutro:\\nThank you! Thank you! Thank you so much!\\nGod bless America! God bless us all!\\n48'), Document(metadata={'source': 'data/reft-paper.pdf', 'page': 48}, page_content='J Licenses for existing assets\\nAll of our experiments are reproducible using our library, which will be released publicly upon\\npublication. Our library comes with the Apache-2.0 License. In addition to our own library, we list\\nthe licenses for the datasets and models used in our experiments.\\nJ.1 Commonsense reasoning\\n1. The BoolQ [Clark et al., 2019] dataset: CC-BY-SA 3.0 License.\\n2. The PIQA [Bisk et al., 2020] dataset: Academic Free License 3.0.\\n3. The SIQA [Sap et al., 2019] dataset: CC-BY 4.0 License.\\n4. The HellaSwag [Zellers et al., 2019] dataset: MIT License.\\n5. The WinoGrande [Sakaguchi et al., 2021] dataset: CC-BY 4.0 License.\\n6. The ARC Easy set (ARC-e [Clark et al., 2018]): CC-BY 4.0 License.\\n7. The ARC Challenge set (ARC-c) [Clark et al., 2018]): CC-BY 4.0 License.\\n8.The OBQA [Mihaylov et al., 2018] dataset: Apache-2.0 License based on the codebase\\nrelease.\\nJ.2 Arithmetic reasoning\\n1. The AddSub [Hosseini et al., 2014] dataset: CC-BY 4.0 License.\\n2. The AQuA [Ling et al., 2017] dataset: Apache-2.0 License based on the codebase release.\\n3. The GSM8K [Cobbe et al., 2021] dataset: MIT License.\\n4. The MAWPS [Koncel-Kedziorski et al., 2016] dataset: CC-BY 4.0 License.\\n5. The MultiArith [Roy and Roth, 2015] dataset: CC-BY 4.0 License.\\n6. The SingleEq [Koncel-Kedziorski et al., 2015] dataset: CC-BY 4.0 License.\\n7. The SV AMP [Patel et al., 2021] dataset: MIT License.\\nJ.3 Instruct-tuning\\n1. The Ultrafeedback [Cui et al., 2023] dataset: MIT License.\\n2.The Alpaca-Eval v1.0 [Li et al., 2023] dataset: Apache-2.0 License based on the codebase\\nrelease.\\nJ.4 Natural language understanding\\nThe GLUE benchmark [Wang et al., 2018] consists of eight datasets. Except QQP, all datasets\\ncome with the CC-BY 4.0 License. QQP comes with a customised license as outlined at https:\\n//www.quora.com/about/tos .\\nJ.5 Models\\n1.LLaMA-1 7B/13B [Touvron et al., 2023a]: Non-commercial license focused on research\\nuse cases.\\n2.Llama-2 7B [Touvron et al., 2023b]: Special Llama-2 License at https://llama.meta.com/\\nlicense/ .\\n3. Llama-3 8B: Special Llama-3 License at https://llama.meta.com/llama3/license/ .18\\n4.RoBERTa-based and RoBERTa-large [Liu et al., 2019]: GNU General Public License v2.0.\\n18https://llama.meta.com/llama3/\\n49')]\n"
          ]
        }
      ],
      "source": [
        "### Chunking\n",
        "# text_splitter = CharacterTextSplitter()\n",
        "# split_documents = text_splitter.split_texts(text)\n",
        "# len(split_documents)\n",
        "\n",
        "split_documents = loader.load_and_split()\n",
        "print(split_documents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[99], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Creating Vector Database\u001b[39;00m\n\u001b[1;32m      2\u001b[0m vector_db \u001b[38;5;241m=\u001b[39m VectorDatabase()\n\u001b[0;32m----> 3\u001b[0m vector_db \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabuild_from_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_documents\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/asyncio/tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
            "File \u001b[0;32m~/Docs/skowak/apps/ai-bootcamp-apps/aie4-bootcamp/bootcamp-lessons/AIE4/Week 1/Day 2/aimakerspace/vectordatabase.py:51\u001b[0m, in \u001b[0;36mVectorDatabase.abuild_from_list\u001b[0;34m(self, list_of_text)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mabuild_from_list\u001b[39m(\u001b[38;5;28mself\u001b[39m, list_of_text: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVectorDatabase\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model\u001b[38;5;241m.\u001b[39masync_get_embeddings(list_of_text)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text, embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(list_of_text, embeddings):\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minsert(text, np\u001b[38;5;241m.\u001b[39marray(embedding))\n",
            "File \u001b[0;32m~/Docs/skowak/apps/ai-bootcamp-apps/aie4-bootcamp/bootcamp-lessons/AIE4/Week 1/Day 2/aimakerspace/openai_utils/embedding.py:24\u001b[0m, in \u001b[0;36mEmbeddingModel.async_get_embeddings\u001b[0;34m(self, list_of_text)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21masync_get_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, list_of_text: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[0;32m---> 24\u001b[0m     embedding_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mlist_of_text, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_model_name\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [embeddings\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;28;01mfor\u001b[39;00m embeddings \u001b[38;5;129;01min\u001b[39;00m embedding_response\u001b[38;5;241m.\u001b[39mdata]\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/openai/resources/embeddings.py:215\u001b[0m, in \u001b[0;36mAsyncEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    210\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    211\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    217\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(params, embedding_create_params\u001b[38;5;241m.\u001b[39mEmbeddingCreateParams),\n\u001b[1;32m    218\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    219\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[1;32m    220\u001b[0m         extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[1;32m    221\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[1;32m    222\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    223\u001b[0m         post_parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[1;32m    224\u001b[0m     ),\n\u001b[1;32m    225\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mCreateEmbeddingResponse,\n\u001b[1;32m    226\u001b[0m )\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/openai/_base_client.py:1816\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1803\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1804\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1811\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1812\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1813\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1814\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1815\u001b[0m     )\n\u001b[0;32m-> 1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/openai/_base_client.py:1510\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1503\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     remaining_retries: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1509\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1511\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1512\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1513\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1514\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1515\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m   1516\u001b[0m     )\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/openai/_base_client.py:1611\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1608\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1610\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1614\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1615\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1619\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[1;32m   1620\u001b[0m )\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
          ]
        }
      ],
      "source": [
        "### Creating Vector Database\n",
        "vector_db = VectorDatabase()\n",
        "vector_db = asyncio.run(vector_db.abuild_from_list(split_documents))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "buildyourownlangchain",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ce393d9afcf427d9d352259c5d32678": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e6efd99f7d346e485b002fb0fa85cc7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3dfb67c39958461da6071e4c19c3fa41",
            "value": 1
          }
        },
        "3a4ba348cb004f8ab7b2b1395539c81b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2ea5009dd16442cb5d8a0ac468e50a8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5f00135fe1044051a50ee5e841cbb8e3",
            "value": "0.018 MB of 0.018 MB uploaded\r"
          }
        },
        "3dfb67c39958461da6071e4c19c3fa41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e6efd99f7d346e485b002fb0fa85cc7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56a8e24025594e5e9ff3b8581c344691": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f00135fe1044051a50ee5e841cbb8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb904e05ece143c79ecc4f20de482f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a4ba348cb004f8ab7b2b1395539c81b",
              "IPY_MODEL_1ce393d9afcf427d9d352259c5d32678"
            ],
            "layout": "IPY_MODEL_56a8e24025594e5e9ff3b8581c344691"
          }
        },
        "d2ea5009dd16442cb5d8a0ac468e50a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
