{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping langchain as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping langchain-community as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping langchain-core as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping langchain-experimental as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping langchain-openai as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping langchain-qdrant as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping qdrant-client as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping ragas as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping rank-bm25 as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping langchain_cohere as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y langchain langchain-community langchain-core langchain-experimental langchain-openai langchain-qdrant qdrant-client ragas rank-bm25 langchain_cohere ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain==0.2.16 langchain-community==0.2.16 langchain-core==0.2.38 langchain-experimental==0.0.65 langchain-openai==0.1.23 langchain-qdrant==0.1.4 qdrant-client==1.11.3 ragas==0.1.20 rank-bm25==0.2.2 langchain_cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rank-bm25 in /opt/miniconda3/envs/w7_d2_NEW/lib/python3.9/site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/w7_d2_NEW/lib/python3.9/site-packages (from rank-bm25) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/w7_d2_NEW/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-01 14:52:28--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19628 (19K) [text/plain]\n",
      "Saving to: ‘john_wick_1.csv’\n",
      "\n",
      "john_wick_1.csv     100%[===================>]  19.17K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2024-10-01 14:52:29 (31.3 MB/s) - ‘john_wick_1.csv’ saved [19628/19628]\n",
      "\n",
      "--2024-10-01 14:52:29--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14747 (14K) [text/plain]\n",
      "Saving to: ‘john_wick_2.csv’\n",
      "\n",
      "john_wick_2.csv     100%[===================>]  14.40K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2024-10-01 14:52:29 (15.5 MB/s) - ‘john_wick_2.csv’ saved [14747/14747]\n",
      "\n",
      "--2024-10-01 14:52:30--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13888 (14K) [text/plain]\n",
      "Saving to: ‘john_wick_3.csv’\n",
      "\n",
      "john_wick_3.csv     100%[===================>]  13.56K  --.-KB/s    in 0.008s  \n",
      "\n",
      "2024-10-01 14:52:30 (1.75 MB/s) - ‘john_wick_3.csv’ saved [13888/13888]\n",
      "\n",
      "--2024-10-01 14:52:31--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15109 (15K) [text/plain]\n",
      "Saving to: ‘john_wick_4.csv’\n",
      "\n",
      "john_wick_4.csv     100%[===================>]  14.75K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2024-10-01 14:52:31 (4.80 MB/s) - ‘john_wick_4.csv’ saved [15109/15109]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download data files\n",
    "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv -O john_wick_1.csv\n",
    "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv -O john_wick_2.csv\n",
    "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv -O john_wick_3.csv\n",
    "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv -O john_wick_4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Review_Date', 'Author', 'Rating', 'Review_Title',\n",
      "       'Review', 'Review_Url'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f\"john_wick_{i}.csv\", encoding='utf-8')\n",
    "print(df.columns)  # Check the actual column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 1 Columns: Index(['Unnamed: 0', 'Review_Date', 'Author', 'Rating', 'Review_Title',\n",
      "       'Review', 'Review_Url'],\n",
      "      dtype='object')\n",
      "CSV 2 Columns: Index(['Unnamed: 0', 'Review_Date', 'Author', 'Rating', 'Review_Title',\n",
      "       'Review', 'Review_Url'],\n",
      "      dtype='object')\n",
      "CSV 3 Columns: Index(['Unnamed: 0', 'Review_Date', 'Author', 'Rating', 'Review_Title',\n",
      "       'Review', 'Review_Url'],\n",
      "      dtype='object')\n",
      "CSV 4 Columns: Index(['Unnamed: 0', 'Review_Date', 'Author', 'Rating', 'Review_Title',\n",
      "       'Review', 'Review_Url'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load data into documents\n",
    "documents = []\n",
    "\n",
    "for i in range(1, 5):\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(f\"john_wick_{i}.csv\", encoding='utf-8')\n",
    "\n",
    "    # Print column names to inspect them (for debugging)\n",
    "    print(f\"CSV {i} Columns:\", df.columns)\n",
    "\n",
    "    # Clean column names: convert to lowercase and strip spaces\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    # Iterate through rows\n",
    "    for _, row in df.iterrows():\n",
    "        # Use 'review' as the content column\n",
    "        content = row['review']\n",
    "        metadata = {\n",
    "            'review_date': row.get('review_date', ''),\n",
    "            'review_title': row.get('review_title', ''),\n",
    "            'review_url': row.get('review_url', ''),\n",
    "            'author': row.get('author', ''),\n",
    "            'rating': int(row.get('rating', 0)) if pd.notnull(row.get('rating')) else 0,\n",
    "            'movie_title': f\"John Wick {i}\",\n",
    "            'last_accessed_at': (datetime.now() - timedelta(days=4 - i)).isoformat()\n",
    "        }\n",
    "        documents.append(Document(page_content=content, metadata=metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Initialize Qdrant client (not passed to the Qdrant vectorstore)\n",
    "qdrant_client = QdrantClient(location=\":memory:\")\n",
    "\n",
    "# Create Qdrant vector store using the correct parameters\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\",  # Instead of passing the client, use location\n",
    "    collection_name=\"JohnWick\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive retriever\n",
    "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi  # Import BM25Okapi from rank_bm25\n",
    "\n",
    "# Prepare documents for BM25\n",
    "tokenized_corpus = [doc.page_content.split() for doc in documents]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "class CustomBM25Retriever:\n",
    "    def __init__(self, bm25, documents, k=10):\n",
    "        self.bm25 = bm25\n",
    "        self.documents = documents\n",
    "        self.k = k\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        tokenized_query = query.split()\n",
    "        doc_scores = self.bm25.get_scores(tokenized_query)\n",
    "        top_n = doc_scores.argsort()[-self.k:][::-1]\n",
    "        return [self.documents[i] for i in top_n]\n",
    "\n",
    "bm25_retriever = CustomBM25Retriever(bm25, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "# Initialize Cohere client\n",
    "cohere_client = cohere.Client(os.environ[\"COHERE_API_KEY\"])\n",
    "\n",
    "class CohereRerankRetriever:\n",
    "    def __init__(self, base_retriever, cohere_client, top_n=5):\n",
    "        self.base_retriever = base_retriever\n",
    "        self.cohere_client = cohere_client\n",
    "        self.top_n = top_n\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        # Retrieve documents using base retriever\n",
    "        docs = self.base_retriever.get_relevant_documents(query)\n",
    "        if not docs:\n",
    "            return []\n",
    "\n",
    "        # Prepare inputs for reranking\n",
    "        passages = [doc.page_content for doc in docs]\n",
    "\n",
    "        # Call Cohere rerank API\n",
    "        response = self.cohere_client.rerank(\n",
    "            query=query,\n",
    "            documents=passages,\n",
    "            top_n=min(self.top_n, len(passages)),\n",
    "            model=\"rerank-english-v2.0\"\n",
    "        )\n",
    "\n",
    "        # Get reranked documents using 'index' instead of 'id'\n",
    "        reranked_docs = [docs[item.index] for item in response.results]\n",
    "\n",
    "        return reranked_docs\n",
    "\n",
    "compression_retriever = CohereRerankRetriever(\n",
    "    base_retriever=naive_retriever,\n",
    "    cohere_client=cohere_client,\n",
    "    top_n=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize LLM for generating multiple queries\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Multi-Query Retriever without verbose argument\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=naive_retriever,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9b/w4m3gwfn51z3r910zg6b_k4c0000gn/T/ipykernel_53045/1684451920.py:72: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  response = chat_model.predict(formatted_prompt)  # Generate the response using the LLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What elements do you think are essential for creating a perfect action movie?\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient, models\n",
    "from operator import itemgetter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Define the RunnablePassthrough class\n",
    "class RunnablePassthrough:\n",
    "    @staticmethod\n",
    "    def assign(context):\n",
    "        return context\n",
    "\n",
    "# Example: Define a prompt template for question answering\n",
    "rag_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "    Context: {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Please provide a helpful and concise answer.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Initialize the LLM model\n",
    "chat_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "parent_docs = documents\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"full_documents\",\n",
    "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "parent_document_vectorstore = Qdrant(\n",
    "    collection_name=\"full_documents\", embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
    ")\n",
    "\n",
    "store = InMemoryStore()\n",
    "\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=parent_document_vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")\n",
    "\n",
    "parent_document_retriever.add_documents(parent_docs, ids=None)\n",
    "\n",
    "# Step 1: Query the parent document retriever for relevant documents\n",
    "query = \"Some question or query\"  # Define your query here\n",
    "retrieved_documents = parent_document_retriever.get_relevant_documents(query)\n",
    "\n",
    "# Step 2: Extract the context (if applicable) from the retrieved documents\n",
    "if retrieved_documents:\n",
    "    retrieved_context = retrieved_documents[0].page_content  # Assuming you're working with page content\n",
    "    retrieved_question = query  # The original query can be treated as the question\n",
    "else:\n",
    "    retrieved_context = \"\"\n",
    "    retrieved_question = query\n",
    "\n",
    "# Step 3: Pass the context through RunnablePassthrough\n",
    "assigned_context = RunnablePassthrough.assign(context={\"context\": retrieved_context})\n",
    "\n",
    "# Step 4: Generate a response using the prompt template and chat model\n",
    "formatted_prompt = rag_prompt.format(context=retrieved_context, question=retrieved_question)\n",
    "\n",
    "response = chat_model.predict(formatted_prompt)  # Generate the response using the LLM\n",
    "\n",
    "# Output the final response (you can modify this based on your usage)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "# Define a custom retriever that implements the required 'invoke' method\n",
    "class CustomRunnableRetriever(Runnable):\n",
    "    def __init__(self, retriever):\n",
    "        self.retriever = retriever\n",
    "    \n",
    "    def _get_relevant_documents(self, query):\n",
    "        return self.retriever.get_relevant_documents(query)\n",
    "    \n",
    "    def invoke(self, query, config=None):\n",
    "        # Call the internal retrieval logic and return the documents\n",
    "        return self._get_relevant_documents(query)\n",
    "\n",
    "# Wrap each retriever in a CustomRunnableRetriever if it's not already a Runnable\n",
    "wrapped_retrievers = [\n",
    "    CustomRunnableRetriever(bm25_retriever),  # Wrapping each retriever manually\n",
    "    CustomRunnableRetriever(naive_retriever),\n",
    "    CustomRunnableRetriever(parent_document_retriever),\n",
    "    CustomRunnableRetriever(compression_retriever),\n",
    "    CustomRunnableRetriever(multi_query_retriever)\n",
    "]\n",
    "\n",
    "# Assign equal weighting to each retriever\n",
    "equal_weighting = [1 / len(wrapped_retrievers)] * len(wrapped_retrievers)\n",
    "\n",
    "# Initialize the Ensemble Retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=wrapped_retrievers,\n",
    "    weights=equal_weighting\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a golden dataset (synthetic questions and ground truth answers)\n",
    "golden_dataset = [\n",
    "    {\n",
    "        'query': \"What are the common criticisms of John Wick 2?\",\n",
    "        'ground_truth': \"Some reviewers felt that John Wick 2 had a weaker plot compared to the first movie, with excessive action overshadowing character development.\"\n",
    "    },\n",
    "    {\n",
    "        'query': \"How did reviewers feel about the action sequences in John Wick 3?\",\n",
    "        'ground_truth': \"Reviewers praised the action sequences in John Wick 3, highlighting their creativity and intensity.\"\n",
    "    },\n",
    "    {\n",
    "        'query': \"Did reviewers think John Wick 4 was better than previous movies?\",\n",
    "        'ground_truth': \"Many reviewers considered John Wick 4 to be a strong continuation of the series, with some stating it surpassed previous installments.\"\n",
    "    },\n",
    "    {\n",
    "        'query': \"What is the overall rating of John Wick 1?\",\n",
    "        'ground_truth': \"John Wick 1 received high ratings from reviewers, often scoring above 8 out of 10.\"\n",
    "    },\n",
    "    {\n",
    "        'query': \"Are there any reviews that mention Keanu Reeves' performance?\",\n",
    "        'ground_truth': \"Many reviews mention Keanu Reeves' performance, praising his portrayal of the character John Wick.\"\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Naive Retriever...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10/10 [00:12<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BM25 Retriever...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10/10 [00:12<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Contextual Compression Retriever...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10/10 [00:09<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Multi-Query Retriever...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10/10 [00:20<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Parent Document Retriever...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10/10 [00:02<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Ensemble Retriever...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10/10 [00:26<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retriever: Naive Retriever\n",
      "  Average Latency: 0.1413 seconds\n",
      "  context_recall: 1.0000\n",
      "  context_precision: 0.5947\n",
      "\n",
      "Retriever: BM25 Retriever\n",
      "  Average Latency: 0.0006 seconds\n",
      "  context_recall: 0.6000\n",
      "  context_precision: 0.3707\n",
      "\n",
      "Retriever: Contextual Compression Retriever\n",
      "  Average Latency: 0.2787 seconds\n",
      "  context_recall: 1.0000\n",
      "  context_precision: 0.7675\n",
      "\n",
      "Retriever: Multi-Query Retriever\n",
      "  Average Latency: 1.7309 seconds\n",
      "  context_recall: 1.0000\n",
      "  context_precision: 0.5647\n",
      "\n",
      "Retriever: Parent Document Retriever\n",
      "  Average Latency: 0.2043 seconds\n",
      "  context_recall: 0.8000\n",
      "  context_precision: 0.8000\n",
      "\n",
      "Retriever: Ensemble Retriever\n",
      "  Average Latency: 2.5262 seconds\n",
      "  context_recall: 1.0000\n",
      "  context_precision: 0.5911\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datasets import Dataset  # Import the Hugging Face Datasets library\n",
    "\n",
    "# Define the list of retrievers to evaluate\n",
    "retrievers = {\n",
    "    'Naive Retriever': naive_retriever,\n",
    "    'BM25 Retriever': bm25_retriever,\n",
    "    'Contextual Compression Retriever': compression_retriever,\n",
    "    'Multi-Query Retriever': multi_query_retriever,\n",
    "    'Parent Document Retriever': parent_document_retriever,\n",
    "    'Ensemble Retriever': ensemble_retriever,\n",
    "}\n",
    "\n",
    "# Define the metrics to evaluate\n",
    "metrics = [\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    # You can add more metrics if available and compatible with your ragas version\n",
    "]\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Evaluate each retriever\n",
    "for name, retriever in retrievers.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    latencies = []\n",
    "    retrieved_answers = []\n",
    "\n",
    "    # Define a retrieval function compatible with Ragas\n",
    "    def retrieve_fn(query, retriever=retriever):\n",
    "        start_time = time.time()  # Measure time\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "        latency = time.time() - start_time  # Calculate latency\n",
    "        latencies.append(latency)   \n",
    "        return [doc.page_content for doc in docs]\n",
    "\n",
    "    # Collect retrieved documents for each query\n",
    "    for item in golden_dataset:\n",
    "        query = item['query']\n",
    "        retrieved_docs = retrieve_fn(query)\n",
    "        retrieved_answers.append(retrieved_docs)\n",
    "\n",
    "    # Prepare evaluation data with required columns\n",
    "    evaluation_data = []\n",
    "    for retrieved, item in zip(retrieved_answers, golden_dataset):\n",
    "        evaluation_data.append({\n",
    "            'user_input': item['query'],  # Add the user input (query)\n",
    "            'retrieved_contexts': retrieved,  # Add the retrieved contexts (documents)\n",
    "            'ground_truth': item['ground_truth']\n",
    "        })\n",
    "\n",
    "    # Convert the list to a Hugging Face dataset\n",
    "    evaluation_dataset = Dataset.from_list(evaluation_data)\n",
    "\n",
    "    # Evaluate using Ragas\n",
    "    scores = evaluate(evaluation_dataset, metrics)\n",
    "\n",
    "    # Calculate average latency\n",
    "    avg_latency = sum(latencies) / len(latencies) if latencies else 0\n",
    "\n",
    "    # Store the results\n",
    "    results[name] = {\n",
    "        'scores': scores,\n",
    "        'avg_latency': avg_latency\n",
    "    }\n",
    "\n",
    "# Display the results\n",
    "for name, data in results.items():\n",
    "    print(f\"\\nRetriever: {name}\")\n",
    "    scores = data['scores']\n",
    "    avg_latency = data['avg_latency']\n",
    "    print(f\"  Average Latency: {avg_latency:.4f} seconds\")\n",
    "    for metric_name, score in scores.items():\n",
    "        print(f\"  {metric_name}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import getpass\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import cohere\n",
    "from rank_bm25 import BM25Okapi\n",
    "from qdrant_client import QdrantClient, models\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter your Cohere API Key:\")\n",
    "\n",
    "# Initialize Cohere client\n",
    "cohere_client = cohere.Client(os.environ[\"COHERE_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data files\n",
    "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv -O john_wick_1.csv\n",
    "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv -O john_wick_2.csv\n",
    "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv -O john_wick_3.csv\n",
    "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv -O john_wick_4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "documents = []\n",
    "\n",
    "for i in range(1, 5):\n",
    "  loader = CSVLoader(\n",
    "      file_path=f\"john_wick_{i}.csv\",\n",
    "      metadata_columns=[\"Review_Date\", \"Review_Title\", \"Review_Url\", \"Author\", \"Rating\"]\n",
    "  )\n",
    "\n",
    "  movie_docs = loader.load()\n",
    "  for doc in movie_docs:\n",
    "\n",
    "    # Add the \"Movie Title\" (John Wick 1, 2, ...)\n",
    "    doc.metadata[\"Movie_Title\"] = f\"John Wick {i}\"\n",
    "\n",
    "    # convert \"Rating\" to an `int`, if no rating is provided - assume 0 rating\n",
    "    doc.metadata[\"Rating\"] = int(doc.metadata[\"Rating\"]) if doc.metadata[\"Rating\"] else 0\n",
    "\n",
    "    # newer movies have a more recent \"last_accessed_at\"\n",
    "    doc.metadata[\"last_accessed_at\"] = datetime.now() - timedelta(days=4-i)\n",
    "\n",
    "  documents.extend(movie_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Create Qdrant vector store\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"JohnWick\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive retriever\n",
    "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare documents for BM25\n",
    "tokenized_corpus = [doc.page_content.split() for doc in documents]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "class CustomBM25Retriever:\n",
    "    def __init__(self, bm25, documents, k=10):\n",
    "        self.bm25 = bm25\n",
    "        self.documents = documents\n",
    "        self.k = k\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        tokenized_query = query.split()\n",
    "        doc_scores = self.bm25.get_scores(tokenized_query)\n",
    "        top_n = doc_scores.argsort()[-self.k:][::-1]\n",
    "        return [self.documents[i] for i in top_n]\n",
    "\n",
    "bm25_retriever = CustomBM25Retriever(bm25, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom retriever using Cohere's rerank API\n",
    "class CohereRerankRetriever:\n",
    "    def __init__(self, base_retriever, cohere_client, top_n=5):\n",
    "        self.base_retriever = base_retriever\n",
    "        self.cohere_client = cohere_client\n",
    "        self.top_n = top_n\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        # Retrieve documents using base retriever\n",
    "        docs = self.base_retriever.get_relevant_documents(query)\n",
    "        # Prepare inputs for reranking\n",
    "        passages = [doc.page_content for doc in docs]\n",
    "        # Call Cohere rerank API\n",
    "        response = self.cohere_client.rerank(\n",
    "            query=query,\n",
    "            documents=passages,\n",
    "            top_n=min(self.top_n, len(passages)),\n",
    "            model=\"rerank-english-v2.0\"\n",
    "        )\n",
    "        # Get reranked documents\n",
    "        reranked_docs = [docs[item.index] for item in response]\n",
    "        return reranked_docs\n",
    "\n",
    "compression_retriever = CohereRerankRetriever(\n",
    "    base_retriever=naive_retriever,\n",
    "    cohere_client=cohere_client,\n",
    "    top_n=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Multi-Query Retriever\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=naive_retriever,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import RunnablePassthrough  # If it exists in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient, models\n",
    "from operator import itemgetter\n",
    "\n",
    "parent_docs = documents\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"full_documents\",\n",
    "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "parent_document_vectorstore = Qdrant(\n",
    "    collection_name=\"full_documents\", embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
    ")\n",
    "\n",
    "store = InMemoryStore()\n",
    "\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore = parent_document_vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")\n",
    "\n",
    "parent_document_retriever.add_documents(parent_docs, ids=None)\n",
    "\n",
    "parent_document_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity #1: Evaluating Retrievers with Ragas\n",
    "\n",
    "# Install required packages\n",
    "!pip install -qU langchain langchain-openai langchain-cohere rank_bm25 qdrant-client ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import getpass\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up API keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter your Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collection: Download the datasets\n",
    "!wget -q https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv -O john_wick_1.csv\n",
    "!wget -q https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv -O john_wick_2.csv\n",
    "!wget -q https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv -O john_wick_3.csv\n",
    "!wget -q https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv -O john_wick_4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "documents = []\n",
    "\n",
    "for i in range(1, 5):\n",
    "    loader = CSVLoader(\n",
    "        file_path=f\"john_wick_{i}.csv\",\n",
    "        source_column=\"Review_Title\",\n",
    "        encoding=\"utf-8\",\n",
    "        csv_args={\n",
    "            'delimiter': ',',\n",
    "            'quotechar': '\"',\n",
    "            'fieldnames': None,\n",
    "            'skipinitialspace': True\n",
    "        }\n",
    "    )\n",
    "    movie_docs = loader.load()\n",
    "    for doc in movie_docs:\n",
    "        # Add the \"Movie Title\" (John Wick 1, 2, ...)\n",
    "        doc.metadata[\"Movie_Title\"] = f\"John Wick {i}\"\n",
    "\n",
    "        # Add other metadata\n",
    "        doc.metadata[\"Rating\"] = int(doc.metadata.get(\"Rating\", 0))\n",
    "        doc.metadata[\"last_accessed_at\"] = datetime.now() - timedelta(days=4 - i)\n",
    "\n",
    "    documents.extend(movie_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the embeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the VectorStore with Qdrant\n",
    "from langchain.vectorstores import Qdrant\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"JohnWick\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "# Initialize the Cohere client\n",
    "cohere_client = cohere.Client('8zF1e9HFcaRJhnh0liOBQxiMxxZlCHQXIkc2rU8j')\n",
    "\n",
    "# Function to rerank documents based on a query\n",
    "def cohere_rerank(query, documents):\n",
    "    response = cohere_client.rerank(\n",
    "        model=\"rerank-english-v2.0\",\n",
    "        query=query,\n",
    "        documents=documents\n",
    "    )\n",
    "    ranked_docs = [doc[\"text\"] for doc in response]\n",
    "    return ranked_docs\n",
    "\n",
    "# Example usage\n",
    "query = \"What are the benefits of AI?\"\n",
    "documents = [\n",
    "    {\"text\": \"AI can help automate tasks and improve efficiency.\"},\n",
    "    {\"text\": \"AI may pose risks if not properly managed.\"}\n",
    "]\n",
    "\n",
    "# Get the reranked documents\n",
    "ranked_docs = cohere_rerank(query, documents)\n",
    "print(ranked_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Naive Retriever\n",
    "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# Create the BM25 Retriever\n",
    "from langchain.retrievers import BM25Retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "\n",
    "# Create the Contextual Compression Retriever\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain.llms import Cohere\n",
    "from langchain.retrievers import CohereRerank\n",
    "\n",
    "compressor = CohereRerank(model=\"rerank-english-v2.0\")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=naive_retriever\n",
    ")\n",
    "\n",
    "# Create the Multi-Query Retriever\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI()\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=naive_retriever, llm=chat_model\n",
    ")\n",
    "\n",
    "# Create the Parent Document Retriever\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "parent_docs = documents\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "\n",
    "# Set up a new Qdrant vectorstore for the Parent Document Retriever\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "client.recreate_collection(\n",
    "    collection_name=\"parent_documents\",\n",
    "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "parent_vectorstore = Qdrant(\n",
    "    client=client,\n",
    "    collection_name=\"parent_documents\",\n",
    "    embeddings=embeddings\n",
    ")\n",
    "store = InMemoryStore()\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=parent_vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter\n",
    ")\n",
    "parent_document_retriever.add_documents(parent_docs)\n",
    "\n",
    "# Create the Ensemble Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "retriever_list = [\n",
    "    bm25_retriever,\n",
    "    naive_retriever,\n",
    "    compression_retriever,\n",
    "    multi_query_retriever,\n",
    "    parent_document_retriever\n",
    "]\n",
    "equal_weighting = [1 / len(retriever_list)] * len(retriever_list)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=retriever_list, weights=equal_weighting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Now, proceed with Activity #1: Evaluating Retrievers with Ragas\n",
    "\n",
    "# Install Ragas (already done at the beginning)\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import hit_rate, precision, recall, mrr\n",
    "\n",
    "# Create a golden dataset (synthetic questions and ground truth answers)\n",
    "golden_dataset = [\n",
    "    {\n",
    "        'query': \"What are the common criticisms of John Wick 2?\",\n",
    "        'ground_truth': \"Some reviewers felt that John Wick 2 had a weaker plot compared to the first movie, with excessive action overshadowing character development.\"\n",
    "    },\n",
    "    {\n",
    "        'query': \"How did reviewers feel about the action sequences in John Wick 3?\",\n",
    "        'ground_truth': \"Reviewers praised the action sequences in John Wick 3, highlighting their creativity and intensity.\"\n",
    "    },\n",
    "    {\n",
    "        'query': \"Did reviewers think John Wick 4 was better than previous movies?\",\n",
    "        'ground_truth': \"Many reviewers considered John Wick 4 to be a strong continuation of the series, with some stating it surpassed previous installments.\"\n",
    "    },\n",
    "    {\n",
    "        'query': \"What is the overall rating of John Wick 1?\",\n",
    "        'ground_truth': \"John Wick 1 received high ratings from reviewers, often scoring above 8 out of 10.\"\n",
    "    },\n",
    "    {\n",
    "        'query': \"Are there any reviews that mention Keanu Reeves' performance?\",\n",
    "        'ground_truth': \"Many reviews mention Keanu Reeves' performance, praising his portrayal of the character John Wick.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Convert the golden dataset into a pandas DataFrame\n",
    "evaluation_dataset = pd.DataFrame(golden_dataset)\n",
    "\n",
    "# Define the list of retrievers to evaluate\n",
    "retrievers = {\n",
    "    'Naive Retriever': naive_retriever,\n",
    "    'BM25 Retriever': bm25_retriever,\n",
    "    'Contextual Compression Retriever': compression_retriever,\n",
    "    'Multi-Query Retriever': multi_query_retriever,\n",
    "    'Parent Document Retriever': parent_document_retriever,\n",
    "    'Ensemble Retriever': ensemble_retriever,\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Define the metrics to evaluate\n",
    "metrics = [hit_rate, precision, recall, mrr]\n",
    "\n",
    "# Evaluate each retriever\n",
    "for name, retriever in retrievers.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    latencies = []\n",
    "    retrieved_answers = []\n",
    "\n",
    "    # Define a retrieval function compatible with Ragas\n",
    "    def retrieve_fn(query, retriever=retriever):\n",
    "        start_time = time.time()\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "        latency = time.time() - start_time\n",
    "        latencies.append(latency)\n",
    "        return [doc.page_content for doc in docs]\n",
    "\n",
    "    # Collect retrieved documents for each query\n",
    "    for query in evaluation_dataset['query']:\n",
    "        retrieved_docs = retrieve_fn(query)\n",
    "        retrieved_answers.append(retrieved_docs)\n",
    "\n",
    "    # Prepare evaluation data\n",
    "    evaluation_data = []\n",
    "    for retrieved, ground_truth in zip(retrieved_answers, evaluation_dataset['ground_truth']):\n",
    "        evaluation_data.append({\n",
    "            'retrieved_documents': retrieved,\n",
    "            'ground_truth': ground_truth\n",
    "        })\n",
    "\n",
    "    # Evaluate using Ragas\n",
    "    scores = evaluate(evaluation_data, metrics)\n",
    "\n",
    "    # Calculate average latency\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "\n",
    "    # Store the results\n",
    "    results[name] = {\n",
    "        'scores': scores,\n",
    "        'avg_latency': avg_latency\n",
    "    }\n",
    "\n",
    "# Display the results\n",
    "for name, data in results.items():\n",
    "    print(f\"\\nRetriever: {name}\")\n",
    "    scores = data['scores']\n",
    "    avg_latency = data['avg_latency']\n",
    "    print(f\"  Average Latency: {avg_latency:.4f} seconds\")\n",
    "    for metric_name, score in scores.items():\n",
    "        print(f\"  {metric_name.capitalize()}: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w7_d2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
