{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- 🤝 Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- 🤝 Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# 🤝 Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkgFAXWVW3wm",
        "outputId": "636db35c-f05a-4038-ec7a-02360bef2dae"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-openai langchain-cohere rank_bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKqYM4Eoxcov"
      },
      "source": [
        "We're also going to be leveraging [Qdrant's](https://qdrant.tech/documentation/frameworks/langchain/) (pronounced \"Quadrant\") VectorDB in \"memory\" mode (so we can leverage it locally in our colab environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s6xav5CxYnML"
      },
      "outputs": [],
      "source": [
        "!pip install -qU qdrant-client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0pDRFEWSXvh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using some reviews from the 4 movies in the John Wick franchise today to explore the different retrieval strategies.\n",
        "\n",
        "These were obtained from IMDB, and are available in the [AIM Data Repository](https://github.com/AI-Maker-Space/DataRepository)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXKHcZmKzDwT"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We can simply `wget` these from GitHub.\n",
        "\n",
        "You could use any review data you wanted in this step - just be careful to make sure your metadata is aligned with your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbbSIGtzX3dS",
        "outputId": "0ce6514e-2479-4001-af24-824f987ce599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-09-30 18:14:50--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19628 (19K) [text/plain]\n",
            "Saving to: ‘john_wick_1.csv’\n",
            "\n",
            "john_wick_1.csv     100%[===================>]  19.17K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2024-09-30 18:14:51 (7.71 MB/s) - ‘john_wick_1.csv’ saved [19628/19628]\n",
            "\n",
            "--2024-09-30 18:14:51--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14747 (14K) [text/plain]\n",
            "Saving to: ‘john_wick_2.csv’\n",
            "\n",
            "john_wick_2.csv     100%[===================>]  14.40K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2024-09-30 18:14:51 (9.16 MB/s) - ‘john_wick_2.csv’ saved [14747/14747]\n",
            "\n",
            "--2024-09-30 18:14:52--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13888 (14K) [text/plain]\n",
            "Saving to: ‘john_wick_3.csv’\n",
            "\n",
            "john_wick_3.csv     100%[===================>]  13.56K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-09-30 18:14:52 (9.60 MB/s) - ‘john_wick_3.csv’ saved [13888/13888]\n",
            "\n",
            "--2024-09-30 18:14:53--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15109 (15K) [text/plain]\n",
            "Saving to: ‘john_wick_4.csv’\n",
            "\n",
            "john_wick_4.csv     100%[===================>]  14.75K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-09-30 18:14:53 (20.4 MB/s) - ‘john_wick_4.csv’ saved [15109/15109]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv -O john_wick_1.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv -O john_wick_2.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv -O john_wick_3.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv -O john_wick_4.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today.\n",
        "\n",
        "- Self-Query: Wants as much metadata as we can provide\n",
        "- Time-weighted: Wants temporal data\n",
        "\n",
        "> NOTE: While we're creating a temporal relationship based on when these movies came out for illustrative purposes, it needs to be clear that the \"time-weighting\" in the Time-weighted Retriever is based on when the document was *accessed* last - not when it was created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "documents = []\n",
        "\n",
        "for i in range(1, 5):\n",
        "  loader = CSVLoader(\n",
        "      file_path=f\"john_wick_{i}.csv\",\n",
        "      metadata_columns=[\"Review_Date\", \"Review_Title\", \"Review_Url\", \"Author\", \"Rating\"]\n",
        "  )\n",
        "\n",
        "  movie_docs = loader.load()\n",
        "  for doc in movie_docs:\n",
        "\n",
        "    # Add the \"Movie Title\" (John Wick 1, 2, ...)\n",
        "    doc.metadata[\"Movie_Title\"] = f\"John Wick {i}\"\n",
        "\n",
        "    # convert \"Rating\" to an `int`, if no rating is provided - assume 0 rating\n",
        "    doc.metadata[\"Rating\"] = int(doc.metadata[\"Rating\"]) if doc.metadata[\"Rating\"] else 0\n",
        "\n",
        "    # newer movies have a more recent \"last_accessed_at\"\n",
        "    doc.metadata[\"last_accessed_at\"] = datetime.now() - timedelta(days=4-i)\n",
        "\n",
        "  documents.extend(movie_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'john_wick_1.csv', 'row': 0, 'Review_Date': '6 May 2015', 'Review_Title': ' Kinetic, concise, and stylish; John Wick kicks ass.\\n', 'Review_Url': '/review/rw3233896/?ref_=tt_urv', 'Author': 'lnvicta', 'Rating': 8, 'Movie_Title': 'John Wick 1', 'last_accessed_at': datetime.datetime(2024, 9, 27, 18, 15, 10, 451287)}, page_content=\": 0\\nReview: The best way I can describe John Wick is to picture Taken but instead of Liam Neeson it's Keanu Reeves and instead of his daughter it's his dog. That's essentially the plot of the movie. John Wick (Reeves) is out to seek revenge on the people who took something he loved from him. It's a beautifully simple premise for an action movie - when action movies get convoluted, they get bad i.e. A Good Day to Die Hard. John Wick gives the viewers what they want: Awesome action, stylish stunts, kinetic chaos, and a relatable hero to tie it all together. John Wick succeeds in its simplicity.\")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"JohnWick\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"JohnWick\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-3.5-turbo` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick based on the reviews provided.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for the movie \"John Wick 3\". Here is the URL to that review: \\'/review/rw4854296/?ref_=tt_urv\\'.'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In John Wick, an ex-hitman comes out of retirement to seek vengeance against the gangsters who killed his dog and took everything from him. This sets off a series of violent confrontations as John Wick unleashes his lethal skills to take down those who wronged him.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "naive_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Opinions on John Wick seem to vary. Some people really enjoyed it, like the reviewers of John Wick 1 who praised the action and the world-building. However, there are also those who did not like it, like the reviewer of John Wick 4 who found it to be the weakest in the series. So, it seems that whether people generally like John Wick depends on individual preferences.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'No reviews have a rating of 10.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In John Wick, the main character, John Wick, is a former hitman seeking vengeance for the death of his beloved dog, which was given to him by his deceased wife. The movie is known for its intense action sequences and choreography.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse - but the `I don't know` isn't great!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
            "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
            "* 'smart_union' has been removed\n",
            "  warnings.warn(message, UserWarning)\n"
          ]
        }
      ],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick based on the positive reviews provided in the context.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Yes, there is a review with a rating of 10. Here is the URL to that review: '/review/rw4854296/?ref_=tt_urv'.\""
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"In John Wick, the character John Wick, played by Keanu Reeves, is forced back into action when a mobster named Santino D'Antonio asks him to carry out a hit in Rome. After completing the assignment, Santino puts a contract on John Wick's head, leading to a series of intense action sequences as Wick fights back against the professional killers sent after him.\""
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick based on the positive reviews and high ratings given by multiple reviewers.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Yes, there is a review with a rating of 10. Here is the URL to that review: '/review/rw4854296/?ref_=tt_urv'.\""
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"In John Wick, the main character, John Wick, a former hitman, is forced back into the world of assassins when an Italian crime lord calls in a favor. Wick is then given a task to kill the crime lord's sister in Rome, leading to a series of events that put Wick in danger as a contract is placed on him. The movie is filled with action-packed sequences and intense moments as Wick navigates the criminal underworld.\""
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = documents\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/9b/w4m3gwfn51z3r910zg6b_k4c0000gn/T/ipykernel_8853/3574430551.py:8: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-qdrant package and should be used instead. To use it run `pip install -U :class:`~langchain-qdrant` and import as `from :class:`~langchain_qdrant import Qdrant``.\n",
            "  parent_document_vectorstore = Qdrant(\n"
          ]
        }
      ],
      "source": [
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = Qdrant(\n",
        "    collection_name=\"full_documents\", embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People's opinions on John Wick seem to vary. Some really enjoy the series and find it consistent and well-received, while others may not like certain aspects of the movies.\""
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for the movie \"John Wick 3.\" The URL to that review is \\'/review/rw4854296/?ref_=tt_urv\\'.'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"In John Wick, the main character, played by Keanu Reeves, is a retired assassin who comes out of retirement after someone kills his dog and steals his car. He embarks on a journey to settle debts and take down the Assassin's Guild by traveling to Italy, Canada, and Manhattan, killing numerous assassins along the way.\""
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Based on the reviews provided, it seems that the majority of people enjoyed the John Wick movies. Reviewers praised the action sequences, Keanu Reeves' performance, and the overall entertainment value of the films.\""
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for the movie \"John Wick 3\". Here is the URL to that review: \\'/review/rw4854296/?ref_=tt_urv\\''"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In John Wick, an ex-hitman comes out of retirement to seek vengeance on the gangsters who killed his dog and took everything from him. The story revolves around his quest for retribution, leading to intense action, shootouts, and breathtaking fights as he faces various challenges from the criminal underworld.'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dHeB-yGXneL",
        "outputId": "efc59105-518a-4134-9228-d98b8a97e08e"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain_experimental"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"JohnWickSemantic\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People generally liked John Wick based on the reviews provided. The action sequences, Keanu Reeves' performance, and the overall entertainment value were praised in the reviews.\""
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Yes, there is a review with a rating of 10. Here is the URL to that review: '/review/rw4854296/?ref_=tt_urv'\""
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In the movie \"John Wick,\" the story revolves around an ex-hitman who comes out of retirement to track down the gangsters that killed his dog and took everything from him. With his wife\\'s death still fresh in his mind, he seeks vengeance and unleashes a maelstrom of destruction against those who wronged him. The movie features plenty of action, shootouts, and breathtaking fights as John Wick navigates a world filled with killers and bounty hunters targeting him for revenge.'"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# 🤝 Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### 🏗️ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: langchain 0.2.16\n",
            "Uninstalling langchain-0.2.16:\n",
            "  Would remove:\n",
            "    /opt/miniconda3/envs/llmops-course/bin/langchain-server\n",
            "    /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/langchain-0.2.16.dist-info/*\n",
            "    /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/langchain/*\n",
            "Proceed (Y/n)? ^C\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip uninstall langchain langchain-core langchain-community langchain-experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (0.2.16)\n",
            "Requirement already satisfied: langchain-community in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (0.2.17)\n",
            "Requirement already satisfied: langchain-core in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (0.2.41)\n",
            "Requirement already satisfied: langchain-experimental in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (0.3.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (0.1.128)\n",
            "Requirement already satisfied: numpy<2,>=1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core) (4.12.2)\n",
            "Collecting langchain-community\n",
            "  Using cached langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "INFO: pip is looking at multiple versions of langchain-experimental to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-experimental\n",
            "  Downloading langchain_experimental-0.3.1.post1-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading langchain_experimental-0.3.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Using cached langchain_experimental-0.3.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Using cached langchain_experimental-0.0.65-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.4)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: anyio in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Using cached langchain_experimental-0.0.65-py3-none-any.whl (207 kB)\n",
            "Installing collected packages: langchain-experimental\n",
            "  Attempting uninstall: langchain-experimental\n",
            "    Found existing installation: langchain-experimental 0.3.2\n",
            "    Uninstalling langchain-experimental-0.3.2:\n",
            "      Successfully uninstalled langchain-experimental-0.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-cohere 0.3.0 requires langchain-core<0.4,>=0.3.0, but you have langchain-core 0.2.41 which is incompatible.\n",
            "langchain-cohere 0.3.0 requires langchain-experimental>=0.3.0, but you have langchain-experimental 0.0.65 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-experimental-0.0.65\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community langchain-core langchain-experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain==0.3.0\n",
            "  Using cached langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core==0.3.6\n",
            "  Using cached langchain_core-0.3.6-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-community==0.3.0\n",
            "  Using cached langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting langchain-experimental==0.3.2\n",
            "  Using cached langchain_experimental-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain==0.3.0) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain==0.3.0) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain==0.3.0) (3.9.5)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain==0.3.0)\n",
            "  Using cached langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain==0.3.0) (0.1.128)\n",
            "Requirement already satisfied: numpy<2,>=1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain==0.3.0) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain==0.3.0) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain==0.3.0) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain==0.3.0) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core==0.3.6) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core==0.3.6) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core==0.3.6) (4.12.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-community==0.3.0) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-community==0.3.0) (2.5.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.3.6) (2.4)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.0) (2.20.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.0) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.0) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.0) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.0) (3.0.3)\n",
            "Requirement already satisfied: anyio in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0) (1.0.0)\n",
            "Using cached langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
            "Using cached langchain_core-0.3.6-py3-none-any.whl (399 kB)\n",
            "Using cached langchain_community-0.3.0-py3-none-any.whl (2.3 MB)\n",
            "Using cached langchain_experimental-0.3.2-py3-none-any.whl (208 kB)\n",
            "Using cached langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: langchain-core, langchain-text-splitters, langchain, langchain-community, langchain-experimental\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.2.41\n",
            "    Uninstalling langchain-core-0.2.41:\n",
            "      Successfully uninstalled langchain-core-0.2.41\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.2.4\n",
            "    Uninstalling langchain-text-splitters-0.2.4:\n",
            "      Successfully uninstalled langchain-text-splitters-0.2.4\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.2.16\n",
            "    Uninstalling langchain-0.2.16:\n",
            "      Successfully uninstalled langchain-0.2.16\n",
            "  Attempting uninstall: langchain-community\n",
            "    Found existing installation: langchain-community 0.2.17\n",
            "    Uninstalling langchain-community-0.2.17:\n",
            "      Successfully uninstalled langchain-community-0.2.17\n",
            "  Attempting uninstall: langchain-experimental\n",
            "    Found existing installation: langchain-experimental 0.0.65\n",
            "    Uninstalling langchain-experimental-0.0.65:\n",
            "      Successfully uninstalled langchain-experimental-0.0.65\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-openai 0.1.25 requires langchain-core<0.3.0,>=0.2.40, but you have langchain-core 0.3.6 which is incompatible.\n",
            "langchain-huggingface 0.0.3 requires langchain-core<0.3,>=0.1.52, but you have langchain-core 0.3.6 which is incompatible.\n",
            "ragas 0.1.20 requires langchain-core<0.3, but you have langchain-core 0.3.6 which is incompatible.\n",
            "langgraph-checkpoint 1.0.6 requires langchain-core<0.3,>=0.2.22, but you have langchain-core 0.3.6 which is incompatible.\n",
            "langchain-qdrant 0.1.3 requires langchain-core<0.3,>=0.1.52, but you have langchain-core 0.3.6 which is incompatible.\n",
            "langgraph 0.2.14 requires langchain-core<0.3,>=0.2.27, but you have langchain-core 0.3.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-0.3.0 langchain-community-0.3.0 langchain-core-0.3.6 langchain-experimental-0.3.2 langchain-text-splitters-0.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.3.0 langchain-core==0.3.6 langchain-community==0.3.0 langchain-experimental==0.3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ragas in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (0.1.20)\n",
            "Requirement already satisfied: numpy in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (1.26.4)\n",
            "Requirement already satisfied: datasets in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (3.0.0)\n",
            "Requirement already satisfied: tiktoken in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (0.7.0)\n",
            "Requirement already satisfied: langchain in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (0.3.0)\n",
            "Collecting langchain-core<0.3 (from ragas)\n",
            "  Using cached langchain_core-0.2.41-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: langchain-community in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (0.3.0)\n",
            "Requirement already satisfied: langchain-openai in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (0.1.25)\n",
            "Requirement already satisfied: openai>1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (1.44.1)\n",
            "Requirement already satisfied: pysbd>=0.3.4 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (0.3.4)\n",
            "Requirement already satisfied: nest-asyncio in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (1.6.0)\n",
            "Requirement already satisfied: appdirs in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (1.4.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core<0.3->ragas) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core<0.3->ragas) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core<0.3->ragas) (0.1.128)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core<0.3->ragas) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core<0.3->ragas) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core<0.3->ragas) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core<0.3->ragas) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai>1->ragas) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai>1->ragas) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai>1->ragas) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai>1->ragas) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai>1->ragas) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai>1->ragas) (4.66.4)\n",
            "Requirement already satisfied: filelock in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (0.3.8)\n",
            "Requirement already satisfied: pandas in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->ragas) (2024.6.0)\n",
            "Requirement already satisfied: aiohttp in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (0.23.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain->ragas) (2.0.32)\n",
            "INFO: pip is looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain (from ragas)\n",
            "  Using cached langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Using cached langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain->ragas)\n",
            "  Using cached langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-community->ragas) (0.6.7)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-community (from ragas)\n",
            "  Using cached langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Using cached langchain_community-0.2.17-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from tiktoken->ragas) (2024.5.15)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp->datasets->ragas) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp->datasets->ragas) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp->datasets->ragas) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp->datasets->ragas) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp->datasets->ragas) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
            "Requirement already satisfied: certifi in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3->ragas) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3->ragas) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.3->ragas) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.3->ragas) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests>=2.32.2->datasets->ragas) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests>=2.32.2->datasets->ragas) (2.2.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pandas->datasets->ragas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pandas->datasets->ragas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pandas->datasets->ragas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\n",
            "Using cached langchain_core-0.2.41-py3-none-any.whl (397 kB)\n",
            "Using cached langchain-0.2.16-py3-none-any.whl (1.0 MB)\n",
            "Using cached langchain_community-0.2.17-py3-none-any.whl (2.3 MB)\n",
            "Using cached langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.6\n",
            "    Uninstalling langchain-core-0.3.6:\n",
            "      Successfully uninstalled langchain-core-0.3.6\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.0\n",
            "    Uninstalling langchain-text-splitters-0.3.0:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.0\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.0\n",
            "    Uninstalling langchain-0.3.0:\n",
            "      Successfully uninstalled langchain-0.3.0\n",
            "  Attempting uninstall: langchain-community\n",
            "    Found existing installation: langchain-community 0.3.0\n",
            "    Uninstalling langchain-community-0.3.0:\n",
            "      Successfully uninstalled langchain-community-0.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-cohere 0.3.0 requires langchain-core<0.4,>=0.3.0, but you have langchain-core 0.2.41 which is incompatible.\n",
            "langchain-experimental 0.3.2 requires langchain-community<0.4.0,>=0.3.0, but you have langchain-community 0.2.17 which is incompatible.\n",
            "langchain-experimental 0.3.2 requires langchain-core<0.4.0,>=0.3.6, but you have langchain-core 0.2.41 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-0.2.16 langchain-community-0.2.17 langchain-core-0.2.41 langchain-text-splitters-0.2.4\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade ragas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain==0.0.221\n",
            "  Downloading langchain-0.0.221-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting ragas==0.0.9\n",
            "  Downloading ragas-0.0.9-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain==0.0.221) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain==0.0.221) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain==0.0.221) (3.9.5)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.221)\n",
            "  Using cached dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting langchainplus-sdk>=0.0.17 (from langchain==0.0.221)\n",
            "  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting numexpr<3.0.0,>=2.8.4 (from langchain==0.0.221)\n",
            "  Downloading numexpr-2.10.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain==0.0.221) (1.26.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.221)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting pydantic<2,>=1 (from langchain==0.0.221)\n",
            "  Downloading pydantic-1.10.18-cp311-cp311-macosx_10_9_x86_64.whl.metadata (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.2/152.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain==0.0.221) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain==0.0.221) (8.5.0)\n",
            "Requirement already satisfied: transformers in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas==0.0.9) (4.44.2)\n",
            "Requirement already satisfied: sentence-transformers in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas==0.0.9) (3.1.0)\n",
            "Requirement already satisfied: datasets in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas==0.0.9) (3.0.0)\n",
            "Collecting protobuf<=3.20.0 (from ragas==0.0.9)\n",
            "  Downloading protobuf-3.20.0-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Requirement already satisfied: openai in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas==0.0.9) (1.44.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.221) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.221) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.221) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.221) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.221) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.221) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.221) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pydantic<2,>=1->langchain==0.0.221) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.0.221) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.0.221) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.0.221) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.0.221) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.221) (3.0.3)\n",
            "Requirement already satisfied: filelock in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas==0.0.9) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas==0.0.9) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas==0.0.9) (0.3.8)\n",
            "Requirement already satisfied: pandas in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas==0.0.9) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas==0.0.9) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas==0.0.9) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas==0.0.9) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->ragas==0.0.9) (2024.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas==0.0.9) (0.23.3)\n",
            "Requirement already satisfied: packaging in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas==0.0.9) (23.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai->ragas==0.0.9) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai->ragas==0.0.9) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai->ragas==0.0.9) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai->ragas==0.0.9) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai->ragas==0.0.9) (1.3.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from sentence-transformers->ragas==0.0.9) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from sentence-transformers->ragas==0.0.9) (1.5.1)\n",
            "Requirement already satisfied: scipy in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from sentence-transformers->ragas==0.0.9) (1.14.1)\n",
            "Requirement already satisfied: Pillow in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from sentence-transformers->ragas==0.0.9) (10.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from transformers->ragas==0.0.9) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from transformers->ragas==0.0.9) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from transformers->ragas==0.0.9) (0.19.1)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->ragas==0.0.9) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->ragas==0.0.9) (0.14.0)\n",
            "Requirement already satisfied: sympy in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->ragas==0.0.9) (1.13.2)\n",
            "Requirement already satisfied: networkx in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->ragas==0.0.9) (3.3)\n",
            "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->ragas==0.0.9) (3.1.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.221) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pandas->datasets->ragas==0.0.9) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pandas->datasets->ragas==0.0.9) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pandas->datasets->ragas==0.0.9) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from scikit-learn->sentence-transformers->ragas==0.0.9) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from scikit-learn->sentence-transformers->ragas==0.0.9) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas==0.0.9) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers->ragas==0.0.9) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers->ragas==0.0.9) (1.3.0)\n",
            "Downloading langchain-0.0.221-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading ragas-0.0.9-py3-none-any.whl (25 kB)\n",
            "Using cached dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Downloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
            "Downloading numexpr-2.10.1-cp311-cp311-macosx_10_9_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.4/141.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.0-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-1.10.18-cp311-cp311-macosx_10_9_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydantic, protobuf, numexpr, openapi-schema-pydantic, langchainplus-sdk, dataclasses-json, langchain, ragas\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.8.2\n",
            "    Uninstalling pydantic-2.8.2:\n",
            "      Successfully uninstalled pydantic-2.8.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.27.3\n",
            "    Uninstalling protobuf-5.27.3:\n",
            "      Successfully uninstalled protobuf-5.27.3\n",
            "  Attempting uninstall: dataclasses-json\n",
            "    Found existing installation: dataclasses-json 0.6.7\n",
            "    Uninstalling dataclasses-json-0.6.7:\n",
            "      Successfully uninstalled dataclasses-json-0.6.7\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.2.16\n",
            "    Uninstalling langchain-0.2.16:\n",
            "      Successfully uninstalled langchain-0.2.16\n",
            "  Attempting uninstall: ragas\n",
            "    Found existing installation: ragas 0.1.20\n",
            "    Uninstalling ragas-0.1.20:\n",
            "      Successfully uninstalled ragas-0.1.20\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-community 0.2.17 requires langchain<0.3.0,>=0.2.16, but you have langchain 0.0.221 which is incompatible.\n",
            "chainlit 0.7.700 requires httpx<0.25.0,>=0.23.0, but you have httpx 0.27.2 which is incompatible.\n",
            "grpcio-tools 1.66.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.0 which is incompatible.\n",
            "langchain-cohere 0.3.0 requires langchain-core<0.4,>=0.3.0, but you have langchain-core 0.2.41 which is incompatible.\n",
            "langchain-cohere 0.3.0 requires pydantic<3,>=2, but you have pydantic 1.10.18 which is incompatible.\n",
            "googleapis-common-protos 1.63.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "langchain-experimental 0.3.2 requires langchain-community<0.4.0,>=0.3.0, but you have langchain-community 0.2.17 which is incompatible.\n",
            "langchain-experimental 0.3.2 requires langchain-core<0.4.0,>=0.3.6, but you have langchain-core 0.2.41 which is incompatible.\n",
            "unstructured-client 0.25.8 requires dataclasses-json>=0.6.4, but you have dataclasses-json 0.5.14 which is incompatible.\n",
            "langchain-qdrant 0.1.3 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.18 which is incompatible.\n",
            "pydantic-settings 2.5.2 requires pydantic>=2.7.0, but you have pydantic 1.10.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.5.14 langchain-0.0.221 langchainplus-sdk-0.0.20 numexpr-2.10.1 openapi-schema-pydantic-1.2.4 protobuf-3.20.0 pydantic-1.10.18 ragas-0.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.0.221 ragas==0.0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: pydantic 1.10.18\n",
            "Uninstalling pydantic-1.10.18:\n",
            "  Would remove:\n",
            "    /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/pydantic-1.10.18.dist-info/*\n",
            "    /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/pydantic/*\n",
            "Proceed (Y/n)? ^C\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip uninstall pydantic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pydantic==1.10.11\n",
            "  Downloading pydantic-1.10.11-cp311-cp311-macosx_10_9_x86_64.whl.metadata (148 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pydantic==1.10.11) (4.12.2)\n",
            "Downloading pydantic-1.10.11-cp311-cp311-macosx_10_9_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydantic\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.18\n",
            "    Uninstalling pydantic-1.10.18:\n",
            "      Successfully uninstalled pydantic-1.10.18\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chainlit 0.7.700 requires httpx<0.25.0,>=0.23.0, but you have httpx 0.27.2 which is incompatible.\n",
            "langchain-cohere 0.3.0 requires langchain-core<0.4,>=0.3.0, but you have langchain-core 0.2.41 which is incompatible.\n",
            "langchain-cohere 0.3.0 requires pydantic<3,>=2, but you have pydantic 1.10.11 which is incompatible.\n",
            "langchain-qdrant 0.1.3 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.11 which is incompatible.\n",
            "pydantic-settings 2.5.2 requires pydantic>=2.7.0, but you have pydantic 1.10.11 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pydantic-1.10.11\n"
          ]
        }
      ],
      "source": [
        "!pip install pydantic==1.10.11\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (0.0.221)\n",
            "Requirement already satisfied: ragas in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (0.0.9)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (0.5.14)\n",
            "Requirement already satisfied: langchainplus-sdk>=0.0.17 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (0.0.20)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (2.10.1)\n",
            "Requirement already satisfied: numpy<2,>=1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (1.10.11)\n",
            "Requirement already satisfied: requests<3,>=2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: transformers in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (4.44.2)\n",
            "Requirement already satisfied: sentence-transformers in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (3.1.0)\n",
            "Requirement already satisfied: datasets in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (3.0.0)\n",
            "Requirement already satisfied: protobuf<=3.20.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (3.20.0)\n",
            "Requirement already satisfied: openai in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (1.44.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pydantic<2,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: filelock in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (0.3.8)\n",
            "Requirement already satisfied: pandas in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->ragas) (2024.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (0.23.3)\n",
            "Requirement already satisfied: packaging in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (23.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai->ragas) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai->ragas) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai->ragas) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai->ragas) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai->ragas) (1.3.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from sentence-transformers->ragas) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from sentence-transformers->ragas) (1.5.1)\n",
            "Requirement already satisfied: scipy in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from sentence-transformers->ragas) (1.14.1)\n",
            "Requirement already satisfied: Pillow in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from sentence-transformers->ragas) (10.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from transformers->ragas) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from transformers->ragas) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from transformers->ragas) (0.19.1)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->ragas) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->ragas) (0.14.0)\n",
            "Requirement already satisfied: sympy in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->ragas) (1.13.2)\n",
            "Requirement already satisfied: networkx in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->ragas) (3.3)\n",
            "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->ragas) (3.1.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pandas->datasets->ragas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pandas->datasets->ragas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pandas->datasets->ragas) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from scikit-learn->sentence-transformers->ragas) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from scikit-learn->sentence-transformers->ragas) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers->ragas) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers->ragas) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain ragas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "ename": "PydanticUserError",
          "evalue": "If you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`. Note that `@root_validator` is deprecated and should be replaced with `@model_validator`.\n\nFor further information visit https://errors.pydantic.dev/2.8/u/root-validator-pre-skip",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPydanticUserError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[66], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/ragas/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version \u001b[38;5;28;01mas\u001b[39;00m __version__\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/ragas/evaluation.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, concatenate_datasets\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_analytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationEvent, track\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Metric\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcritique\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AspectCritique\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_column_dtypes, validate_evaluation_modes\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/ragas/metrics/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manswer_relevance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnswerRelevancy, answer_relevancy\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_relevance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ContextRelevancy, context_relevancy\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcritique\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AspectCritique\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/ragas/metrics/answer_relevance.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, AutoTokenizer\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MODEL_WITH_LM_HEAD_MAPPING_NAMES\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationMode, Metric\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mTYPE_CHECKING:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnpt\u001b[39;00m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/ragas/metrics/base.py:16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m floor\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseChatModel\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLLM\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/langchain/chat_models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manthropic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatAnthropic\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfake\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FakeListChatModel\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/langchain/chat_models/anthropic.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Optional\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     AsyncCallbackManagerForLLMRun,\n\u001b[1;32m      5\u001b[0m     CallbackManagerForLLMRun,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseChatModel\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manthropic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _AnthropicCommon\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/langchain/callbacks/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Callback handlers that allow listening to events in LangChain.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maim_callback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AimCallbackHandler\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01margilla_callback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArgillaCallbackHandler\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marize_callback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArizeCallbackHandler\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/langchain/callbacks/aim_callback.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Optional, Union\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseCallbackHandler\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AgentAction, AgentFinish, LLMResult\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimport_aim\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/langchain/callbacks/base.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseMessage\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMResult\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRetrieverManagerMixin\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mixin for Retriever callbacks.\"\"\"\u001b[39;00m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/langchain/schema/output.py:31\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"Whether this class is LangChain serializable.\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mChatGeneration\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"A single chat generation output.\"\"\"\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/langchain/schema/output.py:39\u001b[0m, in \u001b[0;36mChatGeneration\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m message: BaseMessage\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The message output by the chat model.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;129;43m@root_validator\u001b[39;49m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mset_text\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mDict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"Set the text attribute to be the contents of the message.\"\"\"\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/pydantic/deprecated/class_validators.py:234\u001b[0m, in \u001b[0;36mroot_validator\u001b[0;34m(pre, skip_on_failure, allow_reuse, *__args)\u001b[0m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/pydantic/deprecated/class_validators.py:240\u001b[0m, in \u001b[0;36mroot_validator\u001b[0;34m(pre, skip_on_failure, allow_reuse, *__args)\u001b[0m\n",
            "\u001b[0;31mPydanticUserError\u001b[0m: If you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`. Note that `@root_validator` is deprecated and should be replaced with `@model_validator`.\n\nFor further information visit https://errors.pydantic.dev/2.8/u/root-validator-pre-skip"
          ]
        }
      ],
      "source": [
        "import langchain\n",
        "import ragas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "tgDICngKXLGK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-cohere 0.3.0 requires langchain-core<0.4,>=0.3.0, but you have langchain-core 0.2.41 which is incompatible.\n",
            "langchain-experimental 0.3.2 requires langchain-community<0.4.0,>=0.3.0, but you have langchain-community 0.2.17 which is incompatible.\n",
            "langchain-experimental 0.3.2 requires langchain-core<0.4.0,>=0.3.6, but you have langchain-core 0.2.41 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[53], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_retriever, evaluate\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hit_rate, precision, recall, mrr\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/ragas/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madaptation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m adapt\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunConfig\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/ragas/adaptation.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLanguageModel\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m llm_factory\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseRagasLLM, LangchainLLMWrapper\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MetricWithLLM\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/ragas/llms/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     BaseRagasLLM,\n\u001b[1;32m      3\u001b[0m     LangchainLLMWrapper,\n\u001b[1;32m      4\u001b[0m     LlamaIndexLLMWrapper,\n\u001b[1;32m      5\u001b[0m     llm_factory,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseRagasLLM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLangchainLLMWrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaIndexLLMWrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_factory\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m ]\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/ragas/llms/base.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass, field\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvertexai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatVertexAI\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VertexAI\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLanguageModel\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/langchain_community/chat_models/vertexai.py:32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatGeneration, ChatGenerationChunk, ChatResult\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pre_init\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvertexai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     _VertexAICommon,\n\u001b[1;32m     34\u001b[0m     is_codey_model,\n\u001b[1;32m     35\u001b[0m     is_gemini_model,\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvertexai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     38\u001b[0m     load_image_from_gcs,\n\u001b[1;32m     39\u001b[0m     raise_vertex_import_error,\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
            "File \u001b[0;32m/opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages/langchain_community/llms/vertexai.py:209\u001b[0m\n\u001b[1;32m    200\u001b[0m             params\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_count\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m params\n\u001b[1;32m    204\u001b[0m \u001b[38;5;129;43m@deprecated\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43msince\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.12\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremoval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43malternative_import\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlangchain_google_vertexai.VertexAI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mVertexAI\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43m_VertexAICommon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBaseLLM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"Google Vertex AI large language models.\"\"\"\u001b[39;49;00m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-bison\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n",
            "\u001b[0;31mTypeError\u001b[0m: metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases"
          ]
        }
      ],
      "source": [
        "# Install Ragas\n",
        "!pip install -qU ragas\n",
        "\n",
        "# Import necessary modules\n",
        "import pandas as pd\n",
        "import time\n",
        "from ragas import evaluate_retriever, evaluate\n",
        "from ragas.metrics import hit_rate, precision, recall, mrr\n",
        "from functools import partial"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
